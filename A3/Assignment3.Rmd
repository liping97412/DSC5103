---
title: "DSC5103 Assignment 3"
subtitle: 'Regularization Methods in Classification'
author: "Tong Wang"
date: "Sep 2018"
output:
  html_document:
    highlight: tango
    theme: yeti
---
<!--
comments must be put in an HTML comment form
-->

```{r set-options, echo=FALSE, cache=FALSE}
options(width = 100)  # set output width, turn off scientific notation for big numbers
```

## NOTE:
This assignment is **due at 23:59 of Oct 4, Thursday**. You can work on this file directly and fill in your answers/code below. Please submit the output HTML file (name your file like G1Group02.html if you are from Group 02 of Section G1) onto IVLE/Files/Student Submission/Assignment3 folder.

Also, put the Section/Group and member info below.
```{r}
# Section G?
# Group ??
# Members: YOUR NAMES HERE
```



### Introduction
In this assignment, we will apply regularization methods (Ridge Regression, LASSO, and Elastic Net) to a classification problem, and compare them with traditional Logistic Regression.

Before we start, it is necessary to read through the documentation of the functions **glmnet()** (specifically, the *family* option), **cv.glmnet()** (the *family* and *type.measure* options), and **predict.glmnet()** (the *type* option) in the **glmnet** package, and find out how to run generalized linear models with regularization.

### Data Preparation
First, let us get the data. We will use the **Heart** data from the textbook, available at http://www-bcf.usc.edu/~gareth/ISL/Heart.csv.
```{r}
heart <- read.csv(file="http://www-bcf.usc.edu/~gareth/ISL/Heart.csv", row.names=1)
summary(heart)
```
The task is to use the features to predict **AHD**, binary outcome related to some heart disease. 

Some cleaning is necessary because there are NA's and also several categorical variables stored as numerical.
```{r}
# clean the NA's
heart <- na.omit(heart)
# convert to factors
heart$Sex <- as.factor(heart$Sex)
heart$Fbs <- as.factor(heart$Fbs)
heart$RestECG <- as.factor(heart$RestECG)
heart$ExAng <- as.factor(heart$ExAng)
heart$Slope <- as.factor(heart$Slope)
heart$Ca <- as.factor(heart$Ca)
summary(heart)
```

Next, we will prepare the training and test dataset for later model comparison.
```{r}
# split training and test data 50/50
N <- nrow(heart)
N.train <- round(N/2)
set.seed(456)
train.index <- sample(1:N, N.train)
data.train <- heart[train.index, ]
data.test <- heart[- train.index,]
```


### Questions and Answers
#### 1. [Logistic Regression as a benchmark] Find the optimal Logistic Regression model with **stepAIC()** using the training data, and use the model to predict using the test data.  (1 Mark)

Answer: 

```{r}
# put your R code here inside the blocks
library(MASS)
logi.mod <- glm(AHD~., data = data.train, family = binomial())
step_logi <- stepAIC(logi.mod, direction = 'both')
summary(step_logi)
pred1 <- predict(step_logi, newdata = data.test, type = "response")
logi.mod
```



#### 2. [Ridge Regression] Fit a Ridge Regression model on the training data, use cross-validtion to find the optimal $\lambda$, and use the optimal model to predict on the test data.  (1 Mark)

Answer: 

```{r}
# put your R code here inside the blocks

# Build a ridge regression model with cross validation
library(glmnetUtils)
ridge.mod <- cv.glmnet(AHD ~., data.train, alpha = 0, use.model.frame = TRUE,                           family="binomial")
plot(ridge.mod)

# Find the best lambda
ridge.lam <- ridge.mod$lambda.min
points(log(ridge.lam), min(ridge.mod$cvm), cex=3)
pred2 <- predict(ridge.mod, s = ridge.lam, newdata = data.test,type= "response")

```


#### 3. [LASSO] Fit a LASSO model on the training data, use cross-validtion to find the optimal $\lambda$, and use the optimal model to predict on the test data.  (1 Mark)

Answer: 

```{r}
# put your R code here inside the blocks
lasso.mod <- cv.glmnet(AHD ~ ., data.train, alpha=1, use.model.frame=TRUE, family             = "binomial")
plot(lasso.mod)

# Find the best lambda
lasso.lam <- lasso.mod$lambda.min
points(log(lasso.lam), min(lasso.mod$cvm), cex=3)
pred3 <- predict(lasso.mod, s = lasso.lam, newdata = data.test, type= "response")

```


#### 4. [Elastic Net] Fit an Elastic Net model on the training data, use cross-validtion to find the optimal $\alpha$ and $\lambda$, and use the optimal model to predict on the test data.  (1 Mark)

Answer: 

```{r}
# put your R code here inside the blocks
set.seed(1)
# CV
en.cva <- cva.glmnet(AHD ~ ., data.train, use.model.frame=TRUE, family = "binomial")
plot(en.cva)


### choose optimal alpha manually and find corresponding alpha&lamda
minlossplot(en.cva, cv.type="min")

# choose the 9th model
plot(en.cva$modlist[[9]])
alpha_opt <- en.cva$alpha[9]
lambda_opt <- en.cva$modlist[[9]]$lambda.min

# plot optimal lambda
plot(en.cva$modlist[[9]]$glmnet.fit, xvar="lambda", label = TRUE)
abline(v=log(lambda_opt), lty=2)

# make the prediction on test set
pred4 <- predict(en.cva, alpha = alpha_opt, newdata = data.test,type= "response")

```



#### 5. Compare the above-studied model predictions in terms of misclassification rate, ROC, and AUC.  (1 Mark)

Answer: 

```{r}
# put your R code here inside the blocks

# create prediction object
library(ROCR)
prediction_logical <- prediction(pred1, data.test$AHD)
prediction_ridge <- prediction(pred2, data.test$AHD)
prediction_lasso <- prediction(pred3, data.test$AHD)
prediction_elastic <- prediction(pred4, data.test$AHD)


# Compare misclassification rate

err_logical <- mean(as.numeric(pred1>0.5)!= (as.numeric(data.test$AHD)-1))
err_ridge <- mean(predict(ridge.mod, s = ridge.lam, newdata = data.test,
                            type= "class") != data.test$AHD)
err_lasso <- mean(predict(lasso.mod, s = lasso.lam, newdata = data.test,
                            type= "class") != data.test$AHD)
err_elastic <- mean(predict(en.cva, alpha = alpha_opt, newdata = data.test,
                            type= "class") != data.test$AHD)
err_logical
err_ridge
err_lasso
err_elastic

# Compare ROC
ROC_logical <- performance(prediction_logical, measure= "tpr", x.measure = "fpr")
plot(ROC_logical)
ROC_ridge <- performance(prediction_ridge, measure= "tpr", x.measure = "fpr")
plot(ROC_ridge)
ROC_lasso <- performance(prediction_lasso, measure= "tpr", x.measure = "fpr")
plot(ROC_lasso)
ROC_elastic<- performance(prediction_elastic, measure= "tpr", x.measure = "fpr")
plot(ROC_elastic)

# compare AUC
AUC_logical <- performance(prediction_logical, measure = "auc")
AUC_ridge <- performance(prediction_ridge, measure = "auc")
AUC_lasso <- performance(prediction_lasso, measure = "auc")
AUC_elastic <- performance(prediction_elastic, measure = "auc")
AUC_logical@y.values
AUC_ridge@y.values
AUC_lasso@y.values
AUC_elastic@y.values
```
1. As for accuracy, logistic model has the highest misclassification rate while the Ridge and Lasso model provides equally better accuracies.
2. As for ROC and AUC, the ridge model produced the largest area under ROC curve while the Lasso model gives slightly smaller AUC. 
