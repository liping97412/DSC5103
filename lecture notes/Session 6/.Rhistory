pred_ridge <- predict(model_ridge_optimal,data.test_cut, type="response",exact=TRUE)
pred_ridge <- predict(ridge.cv,s=ridge.lam2,newdata = data.test_cut, type="response",exact=TRUE)
head(pred_ridge)
# put your R code here inside the blocks
### The Lasso
# glmnet with alpha=1 means LASSO
lasso <- glmnet(AHD ~ .,data.train, family="binomial",alpha=1, use.model.frame=TRUE)
plot(lasso, xvar="lambda", label=TRUE)
# CV for optimal lambda
set.seed(1)
lasso.cv <- cv.glmnet(AHD ~ .,data.train, family="binomial",alpha=1, use.model.frame=TRUE)
plot(lasso.cv)
# optimal lambda
lasso.lam <- lasso.cv$lambda.min
log(lasso.lam)
points(log(lasso.lam), min(lasso.cv$cvm), cex=3)
#### alternatively, set optimal lambda to lambda.1se for a more parsimonious model ####
lasso.lam2 <- lasso.cv$lambda.1se
log(lasso.lam2)
min(lasso.cv$cvm) + lasso.cv$cvsd[which.min(lasso.cv$cvm)]
points(log(lasso.lam2), min(lasso.cv$cvm) + lasso.cv$cvsd[which.min(lasso.cv$cvm)], cex=3)
# plot optimal lambda
plot(lasso.cv$glmnet.fit, xvar="lambda", label = TRUE)
abline(v=log(lasso.lam), lty=2)
abline(v=log(lasso.lam2), lty=2)
# final model
coef(lasso.cv, s=lasso.lam2)
# prediciton using optimal lambda
pred_lasso <- predict(lasso.cv, s=lasso.lam2, newdata=data.test_cut,type="response", exact=TRUE)
# put your R code here inside the blocks
### The Elastic Net with a given alpha
# when 0 < alpha < 1, it becomes elastic net
en <- glmnet(AHD ~ ., data.train,family=binomial, alpha=0.5, use.model.frame=TRUE)
# put your R code here inside the blocks
### The Elastic Net with a given alpha
# when 0 < alpha < 1, it becomes elastic net
en <- glmnet(AHD ~ ., data.train,family="binomial", alpha=0.5, use.model.frame=TRUE)
plot(en, xvar="lambda", label=TRUE)
# CV
set.seed(1)
en.cv <- cv.glmnet(AHD ~ ., data.train,family=binomial, alpha=0.5, use.model.frame=TRUE)
# put your R code here inside the blocks
### The Elastic Net with a given alpha
# when 0 < alpha < 1, it becomes elastic net
en <- glmnet(AHD ~ ., data.train,family="binomial", alpha=0.5, use.model.frame=TRUE)
plot(en, xvar="lambda", label=TRUE)
# CV
set.seed(1)
en.cv <- cv.glmnet(AHD ~ ., data.train,family="binomial", alpha=0.5, use.model.frame=TRUE)
plot(en.cv)
# optimal lambda
en.lam <- en.cv$lambda.min
log(en.lam)
points(log(en.lam), min(en.cv$cvm), cex=3)
#### alternatively, set optimal lambda to lambda.1se for a more parsimonious model ####
en.lam2 <- en.cv$lambda.1se
log(en.lam2)
min(en.cv$cvm) + en.cv$cvsd[which.min(en.cv$cvm)]
points(log(en.lam2), min(en.cv$cvm) + en.cv$cvsd[which.min(en.cv$cvm)], cex=3)
# plot optimal lambda
plot(en.cv$glmnet.fit, xvar="lambda", label = TRUE)
abline(v=log(en.lam), lty=2)
abline(v=log(en.lam2), lty=2)
# final model
coef(en.cv, s=en.lam2)
# prediciton using optimal lambda
pred_en <- predict(en.cv, s=en.lam2, newdata=data.test_cut,type="response", exact=TRUE)
# put your R code here inside the blocks
library(ROCR)
pred.glm <- prediction(pred_glm,data.test$AHD)
pred.ridge <- prediction(pred_ridge,data.test$AHD)
pred.lasso <- prediction(pred_lasso,data.test$AHD)
pred.en <- prediction(pred_en,data.test$AHD)
#misclassification rate
err_glm <- performance(pred.glm,"err")
err_ridge <- performance(pred.ridge,"err")
err_lasso <- performance(pred.lasso,"err")
err_en <- performance(pred.en,"err")
#plot error
plot(err_glm,col="black")
plot(err_ridge,col="green",add=TRUE)
plot(err_lasso,col="red",add=TRUE)
plot(err_en,col="blue",add=TRUE)
legend(x=0.75,y=0.32,legend = c("err_glm","err_ridge","err_lasso","err_en"),lty = c(1,1,1,1),lwd = c(1,1,1,1),col = c("black","green","red","blue"),cex=0.6)
#roc
roc_glm <- performance(pred.glm,"tpr","fpr")
roc_ridge <- performance(pred.ridge,"tpr","fpr")
roc_lasso <- performance(pred.lasso,"tpr","fpr")
roc_en <- performance(pred.en,"tpr","fpr")
#plot roc
plot(roc_glm,col="black")
plot(roc_ridge,col="green",add=TRUE)
plot(roc_lasso,col="red",add=TRUE)
plot(roc_en,col="blue",add=TRUE)
legend(x=0.75,y=0.32,legend = c("roc_glm","roc_ridge","roc_lasso","roc_en"),lty = c(1,1,1,1),lwd = c(1,1,1,1),col = c("black","green","red","blue"),cex=0.6)
#auc
auc_glm <- performance(pred.glm,"auc")@y.values[[1]]
auc_ridge <- performance(pred.ridge,"auc")@y.values[[1]]
auc_lasso <- performance(pred.lasso,"auc")@y.values[[1]]
auc_en <- performance(pred.en,"auc")@y.values[[1]]
legend(x=0,y=0.32,legend = c("err_glm","err_ridge","err_lasso","err_en"),lty = c(1,1,1,1),lwd = c(1,1,1,1),col = c("black","green","red","blue"),cex=0.6)
# put your R code here inside the blocks
library(ROCR)
pred.glm <- prediction(pred_glm,data.test$AHD)
pred.ridge <- prediction(pred_ridge,data.test$AHD)
pred.lasso <- prediction(pred_lasso,data.test$AHD)
pred.en <- prediction(pred_en,data.test$AHD)
#misclassification rate
err_glm <- performance(pred.glm,"err")
err_ridge <- performance(pred.ridge,"err")
err_lasso <- performance(pred.lasso,"err")
err_en <- performance(pred.en,"err")
#plot error
plot(err_glm,col="black")
plot(err_ridge,col="green",add=TRUE)
plot(err_lasso,col="red",add=TRUE)
plot(err_en,col="blue",add=TRUE)
legend(x=0,y=0.32,legend = c("err_glm","err_ridge","err_lasso","err_en"),lty = c(1,1,1,1),lwd = c(1,1,1,1),col = c("black","green","red","blue"),cex=0.6)
#roc
roc_glm <- performance(pred.glm,"tpr","fpr")
roc_ridge <- performance(pred.ridge,"tpr","fpr")
roc_lasso <- performance(pred.lasso,"tpr","fpr")
roc_en <- performance(pred.en,"tpr","fpr")
#plot roc
plot(roc_glm,col="black")
plot(roc_ridge,col="green",add=TRUE)
plot(roc_lasso,col="red",add=TRUE)
plot(roc_en,col="blue",add=TRUE)
legend(x=0.75,y=0.32,legend = c("roc_glm","roc_ridge","roc_lasso","roc_en"),lty = c(1,1,1,1),lwd = c(1,1,1,1),col = c("black","green","red","blue"),cex=0.6)
#auc
auc_glm <- performance(pred.glm,"auc")@y.values[[1]]
auc_ridge <- performance(pred.ridge,"auc")@y.values[[1]]
auc_lasso <- performance(pred.lasso,"auc")@y.values[[1]]
auc_en <- performance(pred.en,"auc")@y.values[[1]]
# put your R code here inside the blocks
library(ROCR)
pred.glm <- prediction(pred_glm,data.test$AHD)
pred.ridge <- prediction(pred_ridge,data.test$AHD)
pred.lasso <- prediction(pred_lasso,data.test$AHD)
pred.en <- prediction(pred_en,data.test$AHD)
#misclassification rate
err_glm <- performance(pred.glm,"err")
err_ridge <- performance(pred.ridge,"err")
err_lasso <- performance(pred.lasso,"err")
err_en <- performance(pred.en,"err")
#plot error
plot(err_glm,col="black")
plot(err_ridge,col="green",add=TRUE)
plot(err_lasso,col="red",add=TRUE)
plot(err_en,col="blue",add=TRUE)
legend(x=1,y=0.5,legend = c("err_glm","err_ridge","err_lasso","err_en"),lty = c(1,1,1,1),lwd = c(1,1,1,1),col = c("black","green","red","blue"),cex=0.6)
#roc
roc_glm <- performance(pred.glm,"tpr","fpr")
roc_ridge <- performance(pred.ridge,"tpr","fpr")
roc_lasso <- performance(pred.lasso,"tpr","fpr")
roc_en <- performance(pred.en,"tpr","fpr")
#plot roc
plot(roc_glm,col="black")
plot(roc_ridge,col="green",add=TRUE)
plot(roc_lasso,col="red",add=TRUE)
plot(roc_en,col="blue",add=TRUE)
legend(x=0.75,y=0.32,legend = c("roc_glm","roc_ridge","roc_lasso","roc_en"),lty = c(1,1,1,1),lwd = c(1,1,1,1),col = c("black","green","red","blue"),cex=0.6)
#auc
auc_glm <- performance(pred.glm,"auc")@y.values[[1]]
auc_ridge <- performance(pred.ridge,"auc")@y.values[[1]]
auc_lasso <- performance(pred.lasso,"auc")@y.values[[1]]
auc_en <- performance(pred.en,"auc")@y.values[[1]]
# put your R code here inside the blocks
library(ROCR)
pred.glm <- prediction(pred_glm,data.test$AHD)
pred.ridge <- prediction(pred_ridge,data.test$AHD)
pred.lasso <- prediction(pred_lasso,data.test$AHD)
pred.en <- prediction(pred_en,data.test$AHD)
#misclassification rate
err_glm <- performance(pred.glm,"err")
err_ridge <- performance(pred.ridge,"err")
err_lasso <- performance(pred.lasso,"err")
err_en <- performance(pred.en,"err")
#plot error
plot(err_glm,col="black")
plot(err_ridge,col="green",add=TRUE)
plot(err_lasso,col="red",add=TRUE)
plot(err_en,col="blue",add=TRUE)
legend(x=0.8,y=0.55,legend = c("err_glm","err_ridge","err_lasso","err_en"),lty = c(1,1,1,1),lwd = c(1,1,1,1),col = c("black","green","red","blue"),cex=0.6)
#roc
roc_glm <- performance(pred.glm,"tpr","fpr")
roc_ridge <- performance(pred.ridge,"tpr","fpr")
roc_lasso <- performance(pred.lasso,"tpr","fpr")
roc_en <- performance(pred.en,"tpr","fpr")
#plot roc
plot(roc_glm,col="black")
plot(roc_ridge,col="green",add=TRUE)
plot(roc_lasso,col="red",add=TRUE)
plot(roc_en,col="blue",add=TRUE)
legend(x=0.75,y=0.32,legend = c("roc_glm","roc_ridge","roc_lasso","roc_en"),lty = c(1,1,1,1),lwd = c(1,1,1,1),col = c("black","green","red","blue"),cex=0.6)
#auc
auc_glm <- performance(pred.glm,"auc")@y.values[[1]]
auc_ridge <- performance(pred.ridge,"auc")@y.values[[1]]
auc_lasso <- performance(pred.lasso,"auc")@y.values[[1]]
auc_en <- performance(pred.en,"auc")@y.values[[1]]
# put your R code here inside the blocks
library(ROCR)
pred.glm <- prediction(pred_glm,data.test$AHD)
pred.ridge <- prediction(pred_ridge,data.test$AHD)
pred.lasso <- prediction(pred_lasso,data.test$AHD)
pred.en <- prediction(pred_en,data.test$AHD)
#misclassification rate
err_glm <- performance(pred.glm,"err")
err_ridge <- performance(pred.ridge,"err")
err_lasso <- performance(pred.lasso,"err")
err_en <- performance(pred.en,"err")
#plot error
plot(err_glm,col="black")
plot(err_ridge,col="green",add=TRUE)
plot(err_lasso,col="red",add=TRUE)
plot(err_en,col="blue",add=TRUE)
legend(x=0.8,y=0.55,legend = c("err_glm","err_ridge","err_lasso","err_en"),lty = c(1,1,1,1),lwd = c(1,1,1,1),col = c("black","green","red","blue"),cex=0.8)
#roc
roc_glm <- performance(pred.glm,"tpr","fpr")
roc_ridge <- performance(pred.ridge,"tpr","fpr")
roc_lasso <- performance(pred.lasso,"tpr","fpr")
roc_en <- performance(pred.en,"tpr","fpr")
#plot roc
plot(roc_glm,col="black")
plot(roc_ridge,col="green",add=TRUE)
plot(roc_lasso,col="red",add=TRUE)
plot(roc_en,col="blue",add=TRUE)
legend(x=0.75,y=0.32,legend = c("roc_glm","roc_ridge","roc_lasso","roc_en"),lty = c(1,1,1,1),lwd = c(1,1,1,1),col = c("black","green","red","blue"),cex=0.6)
#auc
auc_glm <- performance(pred.glm,"auc")@y.values[[1]]
auc_ridge <- performance(pred.ridge,"auc")@y.values[[1]]
auc_lasso <- performance(pred.lasso,"auc")@y.values[[1]]
auc_en <- performance(pred.en,"auc")@y.values[[1]]
# put your R code here inside the blocks
library(ROCR)
pred.glm <- prediction(pred_glm,data.test$AHD)
pred.ridge <- prediction(pred_ridge,data.test$AHD)
pred.lasso <- prediction(pred_lasso,data.test$AHD)
pred.en <- prediction(pred_en,data.test$AHD)
#misclassification rate
err_glm <- performance(pred.glm,"err")
err_ridge <- performance(pred.ridge,"err")
err_lasso <- performance(pred.lasso,"err")
err_en <- performance(pred.en,"err")
#plot error
plot(err_glm,col="black")
plot(err_ridge,col="green",add=TRUE)
plot(err_lasso,col="red",add=TRUE)
plot(err_en,col="blue",add=TRUE)
legend(x=0.8,y=0.6,legend = c("err_glm","err_ridge","err_lasso","err_en"),lty = c(1,1,1,1),lwd = c(1,1,1,1),col = c("black","green","red","blue"),cex=0.8)
#roc
roc_glm <- performance(pred.glm,"tpr","fpr")
roc_ridge <- performance(pred.ridge,"tpr","fpr")
roc_lasso <- performance(pred.lasso,"tpr","fpr")
roc_en <- performance(pred.en,"tpr","fpr")
#plot roc
plot(roc_glm,col="black")
plot(roc_ridge,col="green",add=TRUE)
plot(roc_lasso,col="red",add=TRUE)
plot(roc_en,col="blue",add=TRUE)
legend(x=0.75,y=0.32,legend = c("roc_glm","roc_ridge","roc_lasso","roc_en"),lty = c(1,1,1,1),lwd = c(1,1,1,1),col = c("black","green","red","blue"),cex=0.6)
#auc
auc_glm <- performance(pred.glm,"auc")@y.values[[1]]
auc_ridge <- performance(pred.ridge,"auc")@y.values[[1]]
auc_lasso <- performance(pred.lasso,"auc")@y.values[[1]]
auc_en <- performance(pred.en,"auc")@y.values[[1]]
# put your R code here inside the blocks
library(ROCR)
pred.glm <- prediction(pred_glm,data.test$AHD)
pred.ridge <- prediction(pred_ridge,data.test$AHD)
pred.lasso <- prediction(pred_lasso,data.test$AHD)
pred.en <- prediction(pred_en,data.test$AHD)
#misclassification rate
err_glm <- performance(pred.glm,"err")
err_ridge <- performance(pred.ridge,"err")
err_lasso <- performance(pred.lasso,"err")
err_en <- performance(pred.en,"err")
#plot error
plot(err_glm,col="black")
plot(err_ridge,col="green",add=TRUE)
plot(err_lasso,col="red",add=TRUE)
plot(err_en,col="blue",add=TRUE)
legend(x=0.8,y=0.58,legend = c("err_glm","err_ridge","err_lasso","err_en"),lty = c(1,1,1,1),lwd = c(1,1,1,1),col = c("black","green","red","blue"),cex=0.8)
#roc
roc_glm <- performance(pred.glm,"tpr","fpr")
roc_ridge <- performance(pred.ridge,"tpr","fpr")
roc_lasso <- performance(pred.lasso,"tpr","fpr")
roc_en <- performance(pred.en,"tpr","fpr")
#plot roc
plot(roc_glm,col="black")
plot(roc_ridge,col="green",add=TRUE)
plot(roc_lasso,col="red",add=TRUE)
plot(roc_en,col="blue",add=TRUE)
legend(x=0.75,y=0.32,legend = c("roc_glm","roc_ridge","roc_lasso","roc_en"),lty = c(1,1,1,1),lwd = c(1,1,1,1),col = c("black","green","red","blue"),cex=0.6)
#auc
auc_glm <- performance(pred.glm,"auc")@y.values[[1]]
auc_ridge <- performance(pred.ridge,"auc")@y.values[[1]]
auc_lasso <- performance(pred.lasso,"auc")@y.values[[1]]
auc_en <- performance(pred.en,"auc")@y.values[[1]]
# put your R code here inside the blocks
library(ROCR)
pred.glm <- prediction(pred_glm,data.test$AHD)
pred.ridge <- prediction(pred_ridge,data.test$AHD)
pred.lasso <- prediction(pred_lasso,data.test$AHD)
pred.en <- prediction(pred_en,data.test$AHD)
#misclassification rate
err_glm <- performance(pred.glm,"err")
err_ridge <- performance(pred.ridge,"err")
err_lasso <- performance(pred.lasso,"err")
err_en <- performance(pred.en,"err")
#plot error
plot(err_glm,col="black")
plot(err_ridge,col="green",add=TRUE)
plot(err_lasso,col="red",add=TRUE)
plot(err_en,col="blue",add=TRUE)
legend(x=0.8,y=0.58,legend = c("err_glm","err_ridge","err_lasso","err_en"),lty = c(1,1,1,1),lwd = c(1,1,1,1),col = c("black","green","red","blue"),cex=0.8)
#roc
roc_glm <- performance(pred.glm,"tpr","fpr")
roc_ridge <- performance(pred.ridge,"tpr","fpr")
roc_lasso <- performance(pred.lasso,"tpr","fpr")
roc_en <- performance(pred.en,"tpr","fpr")
#plot roc
plot(roc_glm,col="black")
plot(roc_ridge,col="green",add=TRUE)
plot(roc_lasso,col="red",add=TRUE)
plot(roc_en,col="blue",add=TRUE)
legend(x=0.8,y=0.3,legend = c("roc_glm","roc_ridge","roc_lasso","roc_en"),lty = c(1,1,1,1),lwd = c(1,1,1,1),col = c("black","green","red","blue"),cex=0.8)
#auc
auc_glm <- performance(pred.glm,"auc")@y.values[[1]]
auc_ridge <- performance(pred.ridge,"auc")@y.values[[1]]
auc_lasso <- performance(pred.lasso,"auc")@y.values[[1]]
auc_en <- performance(pred.en,"auc")@y.values[[1]]
library(swirl)
swirl()
head(training)
model_rpart <- rpart(Class~.,training,family="binomial")
model_rpart <- rpart(Class~.,training)
mean(training$Class==predict(model_rpart,training,type="class"))
rpart_accuracy_train <- mean(training$Class==predict(model_rpart,training,type="class"))
rpart_accuracy_test <- mean(test$Class==predict(model_rpart,test,type="class"))
rpart_accuracy_train-rpart_accuracy_test
options(width = 100)  # set output width
student.name <- "LI LIPING"  # put your name here
student.id <- 0320278  # put only the numeric digits of your NUS user id here
load("redcards.rdata")  # load the data (make sure the .rdata file is in your working directory!)
ls(all.names=T)  # list all the objects in the data file
dim(data.player)  # show data.I dimensions
summary(data.player)  # summary
glm1a <- glm(data=data.player, formula= redCards ~ leagueCountry, family=poisson)
summary(glm1a)
data.player$leagueCountry2 <- relevel(data.player$leagueCountry, ref="Germany")
glm1b <- glm(data=data.player, formula= redCards ~ leagueCountry2, family=poisson)
summary(glm1b)
data.player$redCards_binary <- ifelse(data.player$redCards>0,1,0)
glm1c <- glm(data=data.player, formula= redCards_binary ~ leagueCountry, family=binomial)
summary(glm1c)
glm2a <- glm(data=data.player, formula=redCards ~ rater1 + rater2, family=poisson)
summary(glm2a)
plot(data.player$rater1, data.player$rater2)
glm2b_1 <- glm(data=data.player, formula=redCards ~ rater1, family=poisson)
summary(glm2b_1)
glm2b_2 <- glm(data=data.player, formula=redCards ~ rater2, family=poisson)
summary(glm2b_2)
library("leaps")
regfit_2b <- regsubsets(redCards ~ ., data.player)
lm_3a <- lm(redCards~.,data = data.player)
library(MASS)
step3a <- stepAIC(lm3a,trace=FALSE)
step3a <- stepAIC(lm_3a,trace=FALSE)
glm_3a <- lm(redCards~.,data = data.player,family="poisson")
library(MASS)
step3a <- stepAIC(lm_3a,trace=FALSE)
glm_3a <- lm(redCards_binary~.,data = data.player,family="binomial")
library(MASS)
step3a <- stepAIC(lm_3a,trace=FALSE)
set.seed(student.id)
N <- 100
D <- 20
data <- data.frame(y=runif(n=N, min=-1, max=1))
for (d in 1:D) {
data[, paste0("x", d)] <- runif(n=N, min=-1, max=1)
}
summary(data)
lm4a <- lm(y ~ ., data)
summary(lm4a)
library(MASS)
step <- stepAIC(lm4a,trace=FALSE)
step$anova
lm4b <- lm(y ~ x4 + x9 + x13 + x16,data)
summary(lm4b)
plot(model_nn_reg)
model_nn_reg <- neuralnet(formula_reg,train_norm,hidden=c(3,2),act.fct="tanh", linear.output = TRUE,stepmax = 5e05)
library(caret)
library(C50)
credit <- read.csv("credit.csv")
credit$default <- factor(credit$default)
# if you like to shuffle dataset, the following are the sample codes
s <- sample(1:nrow(credit))
credit <- credit[s,]
train_data <- credit[1:900,]
test_data <- credit[901:1000,]
head(train_data)
head(test_data)
# fill in the following function for grid search, set .model = "tree", set .winnow = FALSE, tune ".trials" from 5 to 35
tr_grid <- expand.grid(.winnow=FALSE,.trials=c(5:35),.model='tree')
# fill in the following function for cross validation
# first, read the manual by help(trainControl) to find out how to use different kinds of cross validation
help(trainControl)
# second, you are required to use
# 2-fold cross validation for model_cv1
model_cv1 <-  train(default ~ ., data=train_data, method='C5.0', trControl=trainControl(method = 'cv',number = 2), tuneGrid = tr_grid)
# 10-fold cross validation for model_cv2
model_cv2 <- train(default ~ ., data=train_data, method='C5.0', trControl=trainControl(method = 'cv',number = 10), tuneGrid = tr_grid)
# 10-fold cross validation with repeats=5 for model_cv3
model_cv3 <- train(default ~ ., data=train_data, method='C5.0', trControl=trainControl(method = 'repeatedcv',number = 10,repeats = 5), tuneGrid = tr_grid)
# 10-fold cross validation with selectionFunction = "oneSE" for model_cv4, read the manual to understand the meaning of oneSE method
model_cv4 <- train(default ~ ., data=train_data, method='C5.0', trControl=trainControl(method = 'cv',number = 10,selectionFunction = "oneSE"), tuneGrid = tr_grid)
# 10-fold cross validation with selectionFunction = "tolerance" for model_cv5, read the manual to understand the meaning of tolerance method
model_cv5 <- train(default ~ ., data=train_data, method='C5.0', trControl=trainControl(method = 'cv',number = 10,selectionFunction = "tolerance"), tuneGrid = tr_grid)
# after you build the model, you can use run "model_cv1" for example to find the optimal parameter
model_cv1
model_cv2
model_cv3
model_cv4
model_cv5
# pay attention to the optimal parameter of .trials of 5 cases
# also check the accuracy on the test set by completing the following commands
print(confusionMatrix(predict(model_cv1,test_data),test_data$default))
print(confusionMatrix(predict(model_cv2,test_data),test_data$default))
print(confusionMatrix(predict(model_cv3,test_data),test_data$default))
print(confusionMatrix(predict(model_cv4,test_data),test_data$default))
print(confusionMatrix(predict(model_cv5,test_data),test_data$default))
# The following is not graded in A2, it is for you to explore further.
# After you complete this exercise once, you can repeat line #28-32 again several times,
# check the optimal parameters and the number of trials. What is the optimal value of .trials? Which cross-validation method you feel is the best?
# If you like, you can also practice writing a loop to repeat 10 or more times, calculate the mean and variance of accuracy to find out which method performs the best on this dataset.
set.seed(42)
library(neuralnet)
credit <- read.csv("credit.csv")
credit$default <- as.numeric(credit$default)-1
library(dplyr)
credit_cut <- select(credit,-default)
#onehot encoding
library(onehot)
encoder1 <- onehot(credit_cut, max_levels = 100)
credit_cut <- as.data.frame(predict(encoder1, credit_cut))
#replace the special character
names(credit_cut)<-gsub(" ","_",names(credit_cut))
names(credit_cut)<-gsub("-","_",names(credit_cut))
#normalize
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
credit_cut <- as.data.frame(lapply(credit_cut, normalize))
#build a formula
col_names <- colnames(credit_cut)
formula <- paste('default~',paste(col_names,collapse = '+'))
credit_cut[,paste0("default")] <- credit$default
#seperate the train and test dataset
train_nn <- credit_cut[1:900,]
test_nn <- credit_cut[901:1000,]
model_nn <- neuralnet(formula,train_nn,act.fct="logistic", linear.output = FALSE)
pred_nn <- neuralnet::compute(model_nn,select(test_nn,-default))
library(Metrics)
mae(test_nn$default,pred_nn$net.result)
library(ROCR)
pred <- prediction(pred_nn$net.result,test_nn$default)
roc_nn <- performance(pred,"tpr","fpr")
plot(roc_nn,col='red')
auc_nn <- performance(pred,"auc")@y.values
auc_nn
set.seed(42)
library(dplyr)
library(neuralnet)
diamonds_train <- select(read.csv("diamonds_train.csv"),-X)
diamonds_test <- select(read.csv("diamonds_test_no_label.csv"),-X)
#nomalize to [-1,1]
normalize <- function(x) {
return (2*(x - min(x)) / (max(x) - min(x))-1)
}
diamonds_train[,c(1,5,6,7,8,9,10)] <- as.data.frame(lapply(diamonds_train[,c(1,5,6,7,8,9,10)], normalize))
diamonds_test[,c(1,5,6,7,8,9)] <- as.data.frame(lapply(diamonds_test[,c(1,5,6,7,8,9)], normalize))
train_norm <- diamonds_train
test_norm <- diamonds_test
#onehot encoding
library(onehot)
encoder2 <- onehot(train_norm, max_levels = 100)
train_norm <- as.data.frame(predict(encoder2, diamonds_train))
encoder3 <- onehot(test_norm, max_levels = 100)
test_norm <- as.data.frame(predict(encoder3, diamonds_test))
#to exclude the special character
names(train_norm)<-gsub(" ","_",names(train_norm))
names(train_norm)<-gsub("=","_",names(train_norm))
names(test_norm)<-gsub(" ","_",names(test_norm))
names(test_norm)<-gsub("=","_",names(test_norm))
#to build a formula
n <- colnames(train_norm)
formula_reg <- as.formula(paste("price~", paste(n[!(n %in% "price")], collapse = " + ")))
#model baseline
model_nn_reg <- neuralnet(formula_reg,train_norm,hidden=c(3,2),act.fct="tanh", linear.output = TRUE,stepmax = 5e05)
plot(model_nn_reg)
summary(model_nn_reg)
#use caret to tune the model
library(caret)
ctrl <- trainControl(method = "cv",number=5)
grid <- expand.grid(.layer1 = c(1,2,3,4,5,6,7,8), .layer2 = c(4,5,6), .layer3 = c(0))
model_tune <- train(price~., data = train_norm, method = "neuralnet", trControl=ctrl,tuneGrid = grid,threshold=0.02,stepmax=2e05,learningrate=0.3)
model_tune
#according to the RMSE, the best model has 4 nodes in the first layer, has 5 nodes in the second layer
ctrl2 <- trainControl(method = "cv",number = 10)
grid2 <- expand.grid(.layer1 = 4, .layer2 = 5, .layer3 = 0)
model_tune2 <- train(price~., data = train_norm, method = "neuralnet", trControl=ctrl2,tuneGrid = grid2,threshold=0.02,stepmax=2e05,learningrate=0.3)
