centers <- c(sample(1:10, 5000, replace=TRUE),
sample(11:20, 5000, replace=TRUE))
means <- mixture.example$means
means <- means[centers, ]
x.test <- rmvnorm(10000, c(0, 0), 0.2 * diag(2))
x.test <- x.test + means
y.test <- c(rep(0, 5000), rep(1, 5000))
bayes.error <- sum(mixture.example$marginal * (prob * I(prob < 0.5) + (1-prob) * I(prob >= 0.5)))
library("FNN")
?knn.cv  # LOOCV for knn
# test knn.cv
mod15cv <- knn.cv(x, y, k=15)
str(mod15cv)
mod15cv
# enumerate many many k values and measure misclassification rate
ks <- c(1, 3, 5, 7, 9, 11, 15, 17, 23, 25, 35, 45, 55, 75, 99)
misclass.train <- numeric(length=length(ks))
misclass.test  <- numeric(length=length(ks))
misclass.cv  <- numeric(length=length(ks))
for (i in seq(along=ks)) {
mod.train <- knn(x, x, y, k=ks[i])
mod.test  <- knn(x, x.test, y, k=ks[i])
mod.cv <- knn.cv(x, y, k=ks[i])
misclass.train[i] <- sum(mod.train != y) / length(y)
misclass.test[i] <- sum(mod.test != y.test) / length(y.test)
misclass.cv[i] <- sum(mod.cv != y) / length(y)
}
misclass <- data.frame(k=ks, train=misclass.train, test=misclass.test, cv=misclass.cv)
misclass
# plot misclassification rate on Training, Test, and Cross-Validation
plot.mse <- ggplot(data=misclass) + geom_line(aes(x=k, y=train), color="red") + geom_point(aes(x=k, y=train)) +
geom_line(aes(x=k, y=test), color="blue") + geom_point(aes(x=k, y=test)) +
geom_line(aes(x=k, y=cv), color="green") + geom_point(aes(x=k, y=cv)) +
geom_hline(aes(yintercept = bayes.error), linetype="dashed") + scale_x_reverse(lim=c(100, 1))  + theme_bw()
plot.mse
plot.mse
library("ElemStatLearn")  # run install.packages("ElemStatLearn") if you haven't
x <- mixture.example$x
y <- mixture.example$y
prob <- mixture.example$prob
xnew <- mixture.example$xnew
px1 <- mixture.example$px1
px2 <- mixture.example$px2
df.training <- data.frame(x1=x[ , 1], x2=x[ , 2], y=y)
summary(df.training)
df.training$y <- as.factor(df.training$y)
library("ISLR")
names(Hitters)
dim(Hitters)
summary(Hitters)
sum(is.na(Hitters$Salary))
Hitters <- na.omit(Hitters)
ï¼ŸHitters
?Hitters
# remove NA's in Salary
Hitters <- na.omit(Hitters)
sum(is.na(Hitters))
dim(Hitters)
n <- nrow(Hitters)  # number of data points in the sample
p <- ncol(Hitters) - 1  # number of predictors in the sample
summary(Hitters)
## full subsets selection
library("leaps")
?regsubsets
regfit.full <- regsubsets(Salary ~ ., data=Hitters)
summary(regfit.full)  # by default, regsubsets() evaluate models up to size 8
plot(regfit.full)
regfit.full <- regsubsets(Salary ~ ., data=Hitters, nvmax=19)
summary(regfit.full)
plot(regfit.full)
coef(regfit.full, 10)
reg.summary <- summary(regfit.full)
names(reg.summary)
reg.summary$rsq
plot(reg.summary$rsq, xlab="Number of Variables", ylab="rsq", type="o")
reg.summary$adjr2
plot(reg.summary$adjr2, xlab="Number of Variables", ylab="Adjusted RSq", type="o")
which.max(reg.summary$adjr2)
points(11, reg.summary$adjr2[11], col="red", cex=2, pch=20)
reg.summary$cp
plot(reg.summary$cp, xlab="Number of Variables", ylab="Cp", type='o')
which.min(reg.summary$cp)
points(10, reg.summary$cp[10], col="red", cex=2, pch=20)
reg.summary$bic
plot(reg.summary$bic, xlab="Number of Variables", ylab="BIC", type='o')
which.min(reg.summary$bic)
points(6, reg.summary$bic[6], col="red", cex=2, pch=20)
regfit.fwd <- regsubsets(Salary ~ ., data=Hitters, nvmax=19, method="forward")
summary(regfit.fwd)
regfit.bwd <- regsubsets(Salary ~ ., data=Hitters, nvmax=19, method="backward")
summary(regfit.bwd)
library("ISLR")
names(Hitters)
dim(Hitters)
sum(is.na(Hitters$Salary))  # number of NA
Hitters <- na.omit(Hitters)
sum(is.na(Hitters))  # number of NA
dim(Hitters)
summary(Hitters)
library("glmnetUtils")
install.packages("glmnetutils")
install.packages("glmnetUtils")
library("ISLR")
names(Hitters)
dim(Hitters)
# the Salary column has NA's
sum(is.na(Hitters$Salary))  # number of NA
# remove NA's in Salary
Hitters <- na.omit(Hitters)
sum(is.na(Hitters))  # number of NA
dim(Hitters)
summary(Hitters)
### Ridge Regression
# run Ridge Regression using glmnet() (alpha=0 means Ridge Regression)
library("glmnetUtils")
?glmnet
ridge.mod <- glmnet(Salary ~ ., Hitters, alpha=0, use.model.frame=TRUE)  # one can also specify a vector of lambdas to try by adding "lambda=c(1, 10, 100, 1000)"
# check the fitted model
str(ridge.mod)
# plot the fitted model
?plot.glmnet
plot(ridge.mod, xvar="lambda", label=TRUE)
dim(coef(ridge.mod))  # 20 coefficients for the 19 variables, for each lambda
coef(ridge.mod)
ridge.mod$lambda[50]
coef(ridge.mod)[,50]
# the penalty term for the 50-th lambda
sum(coef(ridge.mod)[-1,50]^2)
ridge.mod$lambda[60]
coef(ridge.mod)[,60]
# the penalty term for the 60-th lambda
sum(coef(ridge.mod)[-1,60]^2)
coef(ridge.mod, s=1000)
?cv.glmnet
set.seed(1)
ridge.cv <- cv.glmnet(Salary ~ ., Hitters, alpha=0, use.model.frame=TRUE)  # cv.glmnet() can set lambdas automatically
plot(ridge.cv)
str(ridge.cv)
ridge.lam <- ridge.cv$lambda.min
log(ridge.lam)
min(ridge.cv$cvm)
points(log(ridge.lam), min(ridge.cv$cvm), cex=3)
ridge.lam2 <- ridge.cv$lambda.1se
log(ridge.lam2)
min(ridge.cv$cvm) + ridge.cv$cvsd[which.min(ridge.cv$cvm)]
points(log(ridge.lam2), min(ridge.cv$cvm) + ridge.cv$cvsd[which.min(ridge.cv$cvm)], cex=3)
plot(ridge.cv$glmnet.fit, xvar="lambda", label = TRUE)
abline(v=log(ridge.lam), lty=2)
abline(v=log(ridge.lam2), lty=2)
coef(ridge.cv, s=ridge.lam2)
coef(ridge.mod, s=ridge.lam2)
lasso.mod <- glmnet(Salary ~ ., Hitters, alpha=1, use.model.frame=TRUE)
plot(lasso.mod, xvar="lambda", label=TRUE)
set.seed(1)
lasso.cv <- cv.glmnet(Salary ~ ., Hitters, alpha=1, use.model.frame=TRUE)
plot(lasso.cv)
lasso.lam <- lasso.cv$lambda.min
log(lasso.lam)
points(log(lasso.lam), min(lasso.cv$cvm), cex=3)
#### alternatively, set optimal lambda to lambda.1se for a more parsimonious model ####
lasso.lam2 <- lasso.cv$lambda.1se
log(lasso.lam2)
min(lasso.cv$cvm) + lasso.cv$cvsd[which.min(lasso.cv$cvm)]
points(log(lasso.lam2), min(lasso.cv$cvm) + lasso.cv$cvsd[which.min(lasso.cv$cvm)], cex=3)
# plot optimal lambda
plot(lasso.cv$glmnet.fit, xvar="lambda", label = TRUE)
abline(v=log(lasso.lam), lty=2)
abline(v=log(lasso.lam2), lty=2)
set.seed(1)
# CV
en.cva <- cva.glmnet(Salary ~ ., Hitters, use.model.frame=TRUE)
plot(en.cva)
minlossplot(en.cva, cv.type="min")
plot(en.cva$modlist[[4]])
minlossplot(en.cva, cv.type="min")
str(lasso.cv)
library("ElemStatLearn")  # run install.packages("ElemStatLearn") if you haven't
x <- mixture.example$x
y <- mixture.example$y
prob <- mixture.example$prob
xnew <- mixture.example$xnew
px1 <- mixture.example$px1
px2 <- mixture.example$px2
df.training <- data.frame(x1=x[ , 1], x2=x[ , 2], y=y)
summary(df.training)
df.training$y <- as.factor(df.training$y)
df.grid <- data.frame(x1=xnew[ , 1], x2=xnew[ , 2])
df.grid$prob <- prob
summary(df.grid)
library("ggplot2")
p0 <- ggplot() + geom_point(data=df.training, aes(x=x1, y=x2, color=y), size=4) + scale_color_manual(values=c("green", "red")) + theme_bw()
p0
p.true <- p0 + stat_contour(data=df.grid, aes(x=x1, y=x2, z=prob), breaks=c(0.5))
p.true
library("mvtnorm")
set.seed(123)
centers <- c(sample(1:10, 5000, replace=TRUE),
sample(11:20, 5000, replace=TRUE))
means <- mixture.example$means
means <- means[centers, ]
x.test <- rmvnorm(10000, c(0, 0), 0.2 * diag(2))
x.test <- x.test + means
y.test <- c(rep(0, 5000), rep(1, 5000))
bayes.error <- sum(mixture.example$marginal * (prob * I(prob < 0.5) + (1-prob) * I(prob >= 0.5)))
library("FNN")
?knn.cv  # LOOCV for knn
mod15cv <- knn.cv(x, y, k=15)
str(mod15cv)
mod15cv
ks <- c(1, 3, 5, 7, 9, 11, 15, 17, 23, 25, 35, 45, 55, 75, 99)
misclass.train <- numeric(length=length(ks))
misclass.test  <- numeric(length=length(ks))
misclass.cv  <- numeric(length=length(ks))
for (i in seq(along=ks)) {
mod.train <- knn(x, x, y, k=ks[i])
mod.test  <- knn(x, x.test, y, k=ks[i])
mod.cv <- knn.cv(x, y, k=ks[i])
misclass.train[i] <- sum(mod.train != y) / length(y)
misclass.test[i] <- sum(mod.test != y.test) / length(y.test)
misclass.cv[i] <- sum(mod.cv != y) / length(y)
}
misclass <- data.frame(k=ks, train=misclass.train, test=misclass.test, cv=misclass.cv)
misclass
plot.mse <- ggplot(data=misclass) + geom_line(aes(x=k, y=train), color="red") + geom_point(aes(x=k, y=train)) +
geom_line(aes(x=k, y=test), color="blue") + geom_point(aes(x=k, y=test)) +
geom_line(aes(x=k, y=cv), color="green") + geom_point(aes(x=k, y=cv)) +
geom_hline(aes(yintercept = bayes.error), linetype="dashed") + scale_x_reverse(lim=c(100, 1))  + theme_bw()
plot.mse
plot.mse
library("ElemStatLearn")  # run install.packages("ElemStatLearn") if you haven't
x <- mixture.example$x
y <- mixture.example$y
prob <- mixture.example$prob
xnew <- mixture.example$xnew
px1 <- mixture.example$px1
px2 <- mixture.example$px2
# make dataframe for x and y (for ggplot use)
df.training <- data.frame(x1=x[ , 1], x2=x[ , 2], y=y)
df.training$y <- as.factor(df.training$y)
# dataframe for plotting the boundary
df.grid <- data.frame(x1=xnew[ , 1], x2=xnew[ , 2])
df.grid$prob <- prob
summary(df.grid)
# plot X and Y
library("ggplot2")
p0 <- ggplot() + geom_point(data=df.training, aes(x=x1, y=x2, color=y), size=4) + scale_color_manual(values=c("green", "red")) + theme_bw()
p0
p.true <- p0 + stat_contour(data=df.grid, aes(x=x1, y=x2, z=prob), breaks=c(0.5))
p.true
K <- 10  # 10-fold CV
RUN <- 20  # the number of repetitions of CV
I <- 9  # the max polynomical order to consider
#############################
## auto CV with cv.glm()
#############################
library("boot")
# prepare an empty data.frame to save the MSE for each *run* and *i*
err.kfold <- expand.grid(run=factor(1:RUN), i=1:I)
err.kfold$err <- 0
cost <- function(y, p = 0) {mean(y != (p > 0.5))}
set.seed(1688)
for (run in 1:RUN) {
for (i in 1:I) {
glm.fitted <- glm(y ~ poly(x1, i, raw=TRUE) + poly(x2, i, raw=TRUE), family=binomial(), data=df.training)
err.kfold[err.kfold$run == run & err.kfold$i == i, "err"] <- cv.glm(df.training, glm.fitted, cost, K=K)$delta[1]  # specify K=10 for k-fold CV
}
}
head(err.kfold)
# plot the Error for each run and each i
ggplot(data=err.kfold) + geom_line(aes(x=i, y=err, color=run)) + theme_bw()
N <- nrow(df.training)
fold <- sample(1:K, N, replace=TRUE)
table(fold)
fold <- sample(rep(seq(K), length=N))
table(fold)
err.kfold2 <- expand.grid(run=factor(1:RUN), i=1:I)
err.kfold2$err <- 0
set.seed(1688)
for (run in 1:RUN) {
# create a random partition
fold <- sample(rep(seq(K), length=N))
for (i in 1:I) {
err <- 0  # overall misclassfication errors in all k folds
# start the k-fold CV
for (k in 1:K) {
glm.fitted <- glm(y ~ poly(x1, i, raw=TRUE) + poly(x2, i, raw=TRUE), family=binomial(), data=df.training[fold != k, ])
glm.prob <- predict(glm.fitted, newdata=df.training[fold == k, ], type="response")
glm.pred <- ifelse(glm.prob > 0.5, 1, 0)
glm.err <- sum(glm.pred != df.training[fold == k, "y"])
err <- err + glm.err
}
err.kfold2[err.kfold2$run == run & err.kfold2$i == i, "err"] <- err / nrow(df.training)
}
}
head(err.kfold2)
# plot the Error for each run and each i
ggplot(data=err.kfold2) + geom_line(aes(x=i, y=err, color=run)) + theme_bw()
library("ROCR")
i <- 7
auc <- rep(0, RUN)
set.seed(16888)
for (run in 1:RUN) {
# create a random partition
fold <- sample(rep(seq(K), length=N))
# start the k-fold CV
for (k in 1:K) {
glm.fitted <- glm(y ~ poly(x1, i, raw=TRUE) + poly(x2, i, raw=TRUE), family=binomial(), data=df.training[fold != k, ])
df.training[fold == k, "prob"] <- predict(glm.fitted, newdata=df.training[fold == k, ], type="response")
}
pred <- prediction(df.training$prob, df.training$y)
perf <- performance(pred, measure="tpr", x.measure="fpr")
plot(perf, add=(run != 1))
auc[run] <- as.numeric(performance(pred, "auc")@y.values)
}
abline(a=0, b=1, lty=2)
plot(1:RUN, auc, type="l")
mean(auc)
library("ggplot2")
library("ISLR")
summary(Auto)
N.row <- nrow(Auto)
RUN <- 10  # the number of repetitions
I <- 10  # the max polynomical order to consider
mse.validation <- expand.grid(i=1:I, run=factor(1:RUN))
mse.validation$mse <- 0
head(mse.validation)
# start fitting models
set.seed(1)
for (run in 1:RUN) {
# in each run, first generate an index indicating which rows in data will be used as training data
train.index <- sample(N.row, round(0.5 * N.row))  # sample 50%
validation.index <- - train.index
# fit the i-th order polynomial regression
for (i in 1:I) {
glm.fitted <- glm(mpg ~ poly(horsepower, i, raw=TRUE), data=Auto, subset=train.index)  # use the training data
pred <- predict(glm.fitted, newdata=Auto[validation.index, ])
mse <- mean((Auto[validation.index, c("mpg")] - pred)^2)  # validation MSE calculation
mse.validation[mse.validation$i == i & mse.validation$run == run, "mse"] <- mse  # save the MSE into the data.frame
}
}
head(mse.validation)
summary(mse.validation)
ggplot(data=mse.validation) + geom_line(aes(x=i, y=mse, color=run)) + theme_bw()
library("boot")
?cv.glm
glm.fitted <- glm(mpg ~ horsepower, data=Auto)
cv.out <- cv.glm(Auto, glm.fitted)  # default is LOOCV
str(cv.out)
cv.out$delta[1]  # the CV Error is in delta[1]
mse.LOOCV <- data.frame(i=1:I)
mse.LOOCV$mse <- 0
# fit the i-th order polynomial regression
for (i in 1:I){
glm.fitted <- glm(mpg ~ poly(horsepower, i, raw=TRUE), data=Auto)
mse.LOOCV[i, "mse"] <- cv.glm(Auto, glm.fitted)$delta[1]
}
mse.LOOCV
ggplot(data=mse.LOOCV) + geom_line(aes(x=i, y=mse), color="black") + theme_bw()
mse.kfold <- expand.grid(i=1:I, run=factor(1:RUN))
mse.kfold$mse <- 0
set.seed(168)
for (run in 1:RUN) {
for (i in 1:I) {
glm.fitted <- glm(mpg ~ poly(horsepower, i, raw=TRUE), data=Auto)
mse.kfold[mse.kfold$i == i & mse.kfold$run == run, "mse"] <- cv.glm(Auto, glm.fitted, K=10)$delta[1]  # specify K=10 for k-fold CV
}
}
head(mse.kfold)
ggplot(data=mse.kfold) + geom_line(aes(x=i, y=mse, color=run)) + theme_bw()
RUN2 <- 50  # number of repetitions
K <- 30  # maximum k to try
mse.k <- expand.grid(run=1:RUN2, k=2:K)
mse.k$mse <- 0
glm.fitted <- glm(mpg ~ poly(horsepower, 7, raw=TRUE), data=Auto)
set.seed(16789)
for (run in 1:RUN2) {
for (k in 2:K) {
mse.k[mse.k$k == k & mse.k$run == run, "mse"] <- cv.glm(Auto, glm.fitted, K=k)$delta[1]
}
}
head(mse.k)
library(Hmisc)
ggplot(data=mse.k, aes(x=k, y=mse)) + geom_point(size=1) + theme_bw() +
stat_summary(fun.y=mean, geom="line", color="red") +  # add a line for the means
stat_summary(fun.data=mean_sdl, geom="errorbar", width=0.5, color="red")  # add errorbar with +/- one standard deviation
library("dplyr")
mse.k %>% group_by(k) %>% summarize(mean=mean(mse), sd=sd(mse))
library("ISLR")
names(Hitters)
dim(Hitters)
summary(Hitters)
sum(is.na(Hitters$Salary))
Hitters <- na.omit(Hitters)
sum(is.na(Hitters))
dim(Hitters)
n <- nrow(Hitters)  # number of data points in the sample
p <- ncol(Hitters) - 1  # number of predictors in the sample
summary(Hitters)
library("leaps")
?regsubsets
regfit.full <- regsubsets(Salary ~ ., data=Hitters)
summary(regfit.full)  # by default, regsubsets() evaluate models up to size 8
?plot.regsubsets
plot(regfit.full)
regfit.full <- regsubsets(Salary ~ ., data=Hitters, nvmax=19)
summary(regfit.full)
plot(regfit.full)
coef(regfit.full, 10)
reg.summary <- summary(regfit.full)
names(reg.summary)
reg.summary$rsq
plot(reg.summary$rsq, xlab="Number of Variables", ylab="rsq", type="o")
reg.summary$adjr2
plot(reg.summary$adjr2, xlab="Number of Variables", ylab="Adjusted RSq", type="o")
which.max(reg.summary$adjr2)
points(11, reg.summary$adjr2[11], col="red", cex=2, pch=20)
reg.summary$cp
plot(reg.summary$cp, xlab="Number of Variables", ylab="Cp", type='o')
which.min(reg.summary$cp)
points(10, reg.summary$cp[10], col="red", cex=2, pch=20)
reg.summary$bic
plot(reg.summary$bic, xlab="Number of Variables", ylab="BIC", type='o')
which.min(reg.summary$bic)
points(6, reg.summary$bic[6], col="red", cex=2, pch=20)
regfit.fwd <- regsubsets(Salary ~ ., data=Hitters, nvmax=19, method="forward")
summary(regfit.fwd)
plot(regfit.fwd)
regfit.bwd <- regsubsets(Salary ~ ., data=Hitters, nvmax=19, method="backward")
summary(regfit.bwd)
plot(regfit.bwd)
coef(regfit.full,7)
coef(regfit.fwd,7)
coef(regfit.bwd,7)
predict.regsubsets <- function(object, newdata, p, ...){
form <- as.formula(object$call[[2]])
mat <- model.matrix(form, newdata)
coefi <- coef(object, id=p)
xvars <- names(coefi)
mat[,xvars] %*% coefi
}
K <- 10  # k-fold CV
# partion data into K folds
set.seed(1)
fold <- sample(rep(seq(K), length=n))
table(fold)
predictions <- Hitters[, c("Salary"), drop=FALSE]
predictions$fold <- fold
head(predictions)
for(k in 1:K){
model.k <- regsubsets(Salary ~ ., data=Hitters[fold != k, ], nvmax=19)
for(i in 1:p){
pred <- predict(model.k, Hitters[fold == k, ], p=i)
predictions[fold == k, paste0("pred", i)] <- pred
}
}
head(predictions)
cv.residuals <- predictions$Salary - predictions[, 3:21]
cv.mse <- colMeans(cv.residuals ^ 2)
cv.mse
plot(cv.mse, type="o", ylim=c(90000, 160000), col="red")
lines(reg.summary$rss / nrow(Hitters), type="o")  # in-sample MSE
cv.rsq <- cor(predictions$Salary, predictions[, 3:21])^2
cv.rsq
plot(1:19, cv.rsq, type="o", ylim=c(0.25, 0.6), col="red")  # CV R^2
lines(1:19, reg.summary$rsq, type="o")  # in-sample R^22
p_opt <- which.min(cv.mse)
# final optimal model
regfit.best <- regsubsets(Salary ~ ., data=Hitters, nvmax=19)
coef(regfit.best, p_opt)
str(regfit.best)
best.summary <- summary(regfit.best)
best.summary$which[p_opt,]
set.seed(42)
library(neuralnet)
credit <- read.csv("credit.csv")
credit$default <- as.numeric(credit$default)-1
library(dplyr)
credit_cut <- select(credit,-default)
#onehot encoding
library(onehot)
encoder1 <- onehot(credit_cut, max_levels = 100)
credit_cut <- as.data.frame(predict(encoder1, credit_cut))
#replace the special character
names(credit_cut)<-gsub(" ","_",names(credit_cut))
names(credit_cut)<-gsub("-","_",names(credit_cut))
#normalize
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
credit_cut <- as.data.frame(lapply(credit_cut, normalize))
#build a formula
col_names <- colnames(credit_cut)
formula <- paste('default~',paste(col_names,collapse = '+'))
credit_cut[,paste0("default")] <- credit$default
#seperate the train and test dataset
train_nn <- credit_cut[1:900,]
test_nn <- credit_cut[901:1000,]
formula
model_nn <- neuralnet(formula,train_nn,act.fct="logistic", linear.output = FALSE)
pred_nn <- neuralnet::compute(model_nn,select(test_nn,-default))
library(Metrics)
mae(test_nn$default,pred_nn$net.result)
library(ROCR)
pred <- prediction(pred_nn$net.result,test_nn$default)
roc_nn <- performance(pred,"tpr","fpr")
plot(roc_nn,col='red')
abline(a=0,b=1,lty=2)
auc_nn <- performance(pred,"auc")@y.values
auc_nn
View(test_nn)
View(regfit.fwd)
View(regfit.fwd)
