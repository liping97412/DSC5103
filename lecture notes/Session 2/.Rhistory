library("ggplot2")
#############################
### Generating data
# Simulate a 3rd-order polynomial data
# y = beta0 + beta1 * x + beta2 * x^2 + beta3 * x^3 + epsilon
# epsilon ~ N(0, sigma^2)
#############################
## population parameters
beta0 <- 1
beta1 <- -2
beta2 <- 6
beta3 <- -1
sigma <- 2
set.seed(7890)
## training data
x <- runif(n=100, min=0, max=5)  # 100 points in Uniform(0, 5)
f_x <- beta0 + beta1 * x + beta2 * x^2 + beta3 * x^3
epsilon <- rnorm(n=100, mean=0, sd=sigma)  # 100 noise terms in Normal(0, sigma)
y <- f_x + epsilon
# visualize the training data (x, y) and the true model f_x
plot.train <- ggplot() + geom_point(aes(x=x, y=y), size=2) + geom_line(aes(x=x, y=f_x)) + theme_bw()
plot.train
## test data
x.test <- runif(n=50, min=0, max=5)
f_x.test <- beta0 + beta1 * x.test + beta2 * x.test^2 + beta3 * x.test^3
epsilon.test <- rnorm(n=length(x.test), mean=0, sd=sigma)
y.test <- f_x.test + epsilon.test
# visualize the test data (x, y) and the true model f_x
plot.test <- ggplot() + geom_point(aes(x=x.test, y=y.test), size=2) + geom_line(aes(x=x.test, y=f_x.test)) + theme_bw()
plot.test
# knn regression is in package "FNN"
library("FNN")
?knn.reg
# prepare the X data in the matrix format, as required by knn.reg()
train.x <- matrix(x, ncol=1)
test.x <- matrix(x.test, ncol=1)
## 1. k=15, training MSE
model15.train <- knn.reg(train=train.x, test=train.x, y=y, k=15)
str(model15.train)
# plot the fit
plot.train + geom_line(aes(x=x, y=model15.train$pred), col="blue")
# Training MSE
model15.train.mse <- mean((y - model15.train$pred)^2)
model15.train.mse
## 1b. k=15, test MSE
model15.test <- knn.reg(train=train.x, test=test.x, y=y, k=15)
str(model15.test)
# plot the fit
plot.test + geom_line(aes(x=x.test, y=model15.test$pred), col="blue")
# Test MSE
model15.test.mse <- mean((y.test - model15.test$pred)^2)
model15.test.mse
## 2. k=1
model1.train <- knn.reg(train=train.x, test=train.x, y=y, k=1)
str(model1.train)
# plot the fit
plot.train + geom_line(aes(x=x, y=model1.train$pred), col="blue")
# Training MSE
model1.train.mse <- mean((y - model1.train$pred)^2)
## 2b. k=1, test MSE
model1.test <- knn.reg(train=train.x, test=test.x, y=y, k=1)
str(model1.test)
# plot the fit
plot.test + geom_line(aes(x=x.test, y=model1.test$pred), col="blue")
# Test MSE
model1.test.mse <- mean((y.test - model1.test$pred)^2)
model1.test.mse
## 3. k=50
model50.train <- knn.reg(train=train.x, test=train.x, y=y, k=50)
str(model50.train)
# plot
plot.train + geom_line(aes(x=x, y=model50.train$pred), col="blue")
# Training MSE
model50.train.mse <- mean((y - model50.train$pred)^2)
model50.train.mse
## 4. Training and Test Error plot: to enumerate many many k values and measure MSE
# k's that will be evaluated
ks <- 1:30
# construct empty vectors for keeping the MSE for each k
mse.train <- numeric(length=length(ks))
mse.test  <- numeric(length=length(ks))
# loop over all the k and evaluate MSE in each of them
for (i in seq(along=ks)) {
model.train <- knn.reg(train.x, train.x, y, k=ks[i])
model.test  <- knn.reg(train.x, test.x, y, k=ks[i])
mse.train[i] <- mean((y - model.train$pred)^2)
mse.test[i] <- mean((y.test - model.test$pred)^2)
}
mse.train
mse.test
# optimal k
k.opt <- ks[which.min(mse.test)]
k.opt
# optimal MSE
mse.opt <- min(mse.test)
# plot MSE on Training and Test
ggplot() + geom_line(aes(x=ks, y=mse.train), color="red") + geom_point(aes(x=ks, y=mse.train)) + geom_line(aes(x=ks, y=mse.test), color="blue") + geom_point(aes(x=ks, y=mse.test)) + scale_x_reverse(lim=c(30, 1)) + geom_hline(yintercept=sigma^2, linetype=2) + theme_bw()
#############################
### Loading data
#############################
# load the dataset "mixture.example" in package "ElemStatLearn")
library("ElemStatLearn")  # run install.packages("ElemStatLearn") if you haven't
?mixture.example
str(mixture.example)
# copy important ones out
x <- mixture.example$x
y <- mixture.example$y
prob <- mixture.example$prob
xnew <- mixture.example$xnew
px1 <- mixture.example$px1
px2 <- mixture.example$px2
summary(x)
table(y)
summary(prob)
# visualize the "prob" matrix
prob.mat <- matrix(prob, nrow=length(px1), ncol=length(px2))
persp(px1, px2, prob.mat, phi = 45, theta = 45, xlab = "x1", ylab = "x2", main = "True Prob from the Data Generating Model")
# or use plotly for interactive plots
library("plotly")
plot_ly(x=px2, y=px1, z=prob.mat, type = "surface")
# make dataframe for the training data (with x1, x2, and y)
df.training <- data.frame(x1=x[ , 1], x2=x[ , 2], y=y)
summary(df.training)
df.training$y <- as.factor(df.training$y)
# make dataframe for the "test" data (with xnew1, xnew2, and true prob, but not y!!)
df.grid <- data.frame(x1=xnew[ , 1], x2=xnew[ , 2])
df.grid$prob <- prob
summary(df.grid)
# plot X and Y
library("ggplot2")
p0 <- ggplot() + geom_point(data=df.training, aes(x=x1, y=x2, color=y), size=4) + scale_color_manual(values=c("green", "red")) + theme_bw()
p0
# add the true boundary into the plot
p.true <- p0 + stat_contour(data=df.grid, aes(x=x1, y=x2, z=prob), breaks=c(0.5))
p.true
# knn() is in package "class"
#library("class")
library("FNN")
?knn
## 1. k=15
model15 <- knn(x, xnew, y, k=15, prob=TRUE)
str(model15)
prob15 <- attr(model15, "prob")
prob15 <- ifelse(model15 == "1", prob15, 1 - prob15)
df.grid$prob15 <- prob15
# plot
p15 <- p.true + stat_contour(data=df.grid, aes(x=x1, y=x2, z=prob15), breaks=c(0.5), color="blue", size=1)
p15
# Training Errors
y_hat15 <- ifelse(knn(x, x, y, k=15) == "1", 1, 0)
# total errors in training
sum(y_hat15 != y)
# misclassification rate in training
model15.train.err <- sum(y_hat15 != y) / length(y)
## 2. k=1
model1 <- knn(x, xnew, y, k=1, prob=TRUE)
str(model1)
prob1 <- attr(model1, "prob")
prob1 <- ifelse(model1 == "1", prob1, 1 - prob1)
df.grid$prob1 <- prob1
# plot
p1 <- p.true + stat_contour(data=df.grid, aes(x=x1, y=x2, z=prob1), breaks=c(0.5), color="blue", size=1)
p1
# Training Errors
y_hat1 <- ifelse(knn(x, x, y, k=1) == "1", 1, 0)
# total errors in training
sum(y_hat1 != y)
# misclassification rate in training
model1.train.err <- sum(y_hat1 != y) / length(y)
## 3. k=100
model100 <- knn(x, xnew, y, k=100, prob=TRUE)
str(model100)
prob100 <- attr(model100, "prob")
prob100 <- ifelse(model100 == "1", prob100, 1 - prob100)
df.grid$prob100 <- prob100
# plot
p100 <- p.true + stat_contour(data=df.grid, aes(x=x1, y=x2, z=prob100), breaks=c(0.5), color="blue", size=1)
p100
# Training Errors
y_hat100 <- ifelse(knn(x, x, y, k=100) == "1", 1, 0)
# total errors in training
sum(y_hat100 != y)
# misclassification rate in training
model100.train.err <- sum(y_hat100 != y) / length(y)
model100.train.err
# DO NOT WORRY about the secret algorithm for generating test data
library("mvtnorm")
set.seed(123)
centers <- c(sample(1:10, 5000, replace=TRUE),
sample(11:20, 5000, replace=TRUE))
means <- mixture.example$means
means <- means[centers, ]
x.test <- rmvnorm(10000, c(0, 0), 0.2 * diag(2))
x.test <- x.test + means
y.test <- c(rep(0, 5000), rep(1, 5000))
# irreducible error (the number comes from the data generating model)
bayes.error <- sum(mixture.example$marginal * (prob * I(prob < 0.5) + (1-prob) * I(prob >= 0.5)))
# enumerate many many k values and measure misclassification rate
ks <- c(1, 3, 5, 7, 9, 11, 15, 17, 23, 25, 35, 45, 55, 75, 99)
misclass.train <- numeric(length=length(ks))
misclass.test  <- numeric(length=length(ks))
for (i in seq(along=ks)) {
model.train <- knn(x, x, y, k=ks[i])
model.test  <- knn(x, x.test, y, k=ks[i])
misclass.train[i] <- sum(model.train != y) / length(y)
misclass.test[i] <- sum(model.test != y.test) / length(y.test)
}
misclass.train
misclass.test
# optimal k
k.opt <- ks[which.min(misclass.test)]
# optimal misclassification rate
misclass.opt <- min(misclass.test)
# plot misclassification rate on Training and Test
plot.mse <- ggplot() + geom_line(aes(x=ks, y=misclass.train), color="red") + geom_point(aes(x=ks, y=misclass.train))  + geom_line(aes(x=ks, y=misclass.test), color="blue") + geom_point(aes(x=ks, y=misclass.test))  + scale_x_reverse(lim=c(100, 1))  + theme_bw()
plot.mse
# add irreducible error to the plot
plot.mse + geom_hline(aes(yintercept = bayes.error), linetype="dashed")
# set global chunk options
library("knitr")
opts_chunk$set(cache=TRUE, message=FALSE)
letters  # a built-in vector of English letters
sample(x=letters, size=5)  # sample 5 letters randomly
sample(x=letters, size=5)  # sample another 5 letters, which is independent of the previous sample
outcomes <- c("head", "tail")
sample(x=outcomes, size=6, replace=TRUE)  # flip a coin 6 times
outcomes <- c("head", "tail")
sample(x=outcomes, size=6, replace=TRUE)  # flip a coin 6 times
sample(x=outcomes, size=6, replace=TRUE, prob=c(0.3, 0.7))
runif(n=10, min=2, max=4)  # type ?runif for details
x.pois <- rpois(n=200, lambda=6)  # type ?rpois for details
head(x.pois)
mean(x.pois)  # mean
sd(x.pois)  # standard deviation
table(x.pois)  # frequency table
plot(table(x.pois) / 200, ylim=c(0, 0.2))  # plot sample distribution
lines(0:15, dpois(0:15, lambda=6), col="blue")  # overlay of the theoretical distribution
points(0:15, dpois(0:15, lambda=6), col="blue")
x.binom <- rbinom(n=200, size=10, p=0.7)  # type ?rbinom for details
head(x.binom)
plot(table(x.binom) / 200, ylim=c(0, 0.3))  # plot sample distribution
lines(0:10, dbinom(0:10, size=10, p=0.7), col="blue")  # overlay of the theoretical distribution
points(0:10, dbinom(0:10, size=10, p=0.7), col="blue")
x.norm <- rnorm(n=1000, mean=0, sd=1)  # type ?rnorm for details
plot(x.norm, main='random draws from a standard normal')
mean(x.norm)
sd(x.norm)
hist(x.norm, probability=TRUE, breaks=30)  # plot sample distribution
curve(dnorm(x, mean=0, sd=1), add=TRUE, col="blue")  # overlay of the theoretical distribution
library("mvtnorm")  # multivariate normal distribution in package "mvtnorm"
x.mvnorm <- rmvnorm(n=2000, mean=c(1, 2), sigma=matrix(c(9, 3, 3, 4), nrow=2, ncol=2))  # type ?rmvnorm for details
head(x.mvnorm)
colMeans(x.mvnorm)  # column means
cov(x.mvnorm)  # covariance matrix
plot(x.mvnorm)  # scatter plot of (x1, x2)
library("MASS")
den3d <- kde2d(x.mvnorm[, 1], x.mvnorm[, 2])  # summarize the sample density
persp(den3d, col="lightblue", box=FALSE, theta=0, phi=40)  # 3d plot
contour(den3d)  # contour plot
beta0 <- 0  # intercept
beta1 <- 2  # slope
sigma <- 30  # epsilon ~ N(0, sigma^2)
x <- runif(n=100, min=0, max=100)
epsilon <- rnorm(n=100, mean=0, sd=sigma)
y <- beta0 + beta1*x + epsilon
plot(x, y)  # sample data
abline(a=beta0, b=beta1, col="blue")  # the TRUE data generating model
lr <- lm(y ~ x)
summary(lr)
red.center <- c(2, 0)
blue.center <- c(0, 2)
cov.mat <- matrix(c(1, 0, 0, 1), nrow=2, ncol=2)
reds <- rmvnorm(n=100, mean=red.center, sigma=cov.mat)
head(reds)
blues <- rmvnorm(n=100, mean=blue.center, sigma=cov.mat)
head(blues)
plot(x=c(-4, 6), y=c(-4, 6), type="n", xlab="x1", ylab="x2")  # lay down the canvas of proper size
points(reds, col="red")  # plot the "red" points
points(blues, col="blue")  # plot the "blue" points
points(red.center[1], red.center[2], col="red", pch=9, cex=3, lwd=3)  # center for "red"
points(blue.center[1], blue.center[2], col="blue", pch=9, cex=3, lwd=3)  # center for "blue"
set.seed(5678)
rnorm(n=5)
set.seed(5678)
rnorm(n=5)
