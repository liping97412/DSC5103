return(predict(model, test_data))
})
return(rowSums(preds) / length(models))
}
models_rf <- train_random_forest(50, 4, train_data, "y")
library(rpart)
library(ggplot2)
rawdata <- read.csv("A3_train.csv")
# use the first 1500 records in A3_train.csv for training and the last 500 records in A3_train.csv for testing
train_data <- rawdata[1:1500, ]
test_data <- rawdata[1501:2000, ]
set.seed(1234)
# Train a single tree model as our base model, to which we can compare with
# our random forest implementation later
model_rpart <- rpart(y ~., train_data)
pred_rpart <- predict(model_rpart, test_data)
rmse <- function(predicted, actual) {
return(sqrt(mean((predicted - actual)^2)))
}
cat('Single tree model RMSE:', rmse(pred_rpart, test_data$y), '\n')
####################################
####    Manual Random Forest     ###
####################################
library(dplyr)
train_random_forest <- function(n_trees, n_features,
training_data, target_col_name){
models <- lapply(1:n_trees, function(i) {
# bootstrapping
n_samples <- nrow(training_data)
#generate a number between 0-2000
sample_row_ids <- sample(1:n_samples, replace=TRUE)
new_training_data <- training_data[sample_row_ids, ]
### START CODE HERE ### (â 5 lines)
# Subset n_features columns.
# Be careful to prevent target column from being sampled,
# but make sure it's eventually present in new_training_data
target_col_num <- which(colnames(training_data) == target_col_name)
n_cols <- ncol(training_data[,-target_col_num])
# Subset n_features columns.
sample_col_ids <- sample(1:n_cols, size=n_features, replace=TRUE)
new_training_data2 <- new_training_data[,-target_col_num][,sample_col_ids]
new_training_data2[,target_col_num] <- new_training_data[,target_col_num]
### END CODE HERE ###
formula <- as.formula(paste(target_col_name, '~.'))
new_model <- rpart(formula, data=new_training_data2)
### START CODE HERE ### (â 2 lines)
# post-prune the rpart model & return it
best_cp <- new_model$cptable[which.min(new_model$cptable[,'xerror']), 'CP']
new_model <- rpart(formula, data=new_training_data2, control=rpart.control(cp=best_cp))
return(new_model)
### END CODE HERE ###
})
return(models)
}
predict_random_forest <- function(models, test_data) {
preds <- sapply(models, function(model) {
return(predict(model, test_data,type="class"))
})
preds1 <- mapply(preds,FUN=as.numeric)
preds <- matrix(data = preds1, nrow = nrow(preds), ncol = ncol(preds))
return(ifelse((rowSums(preds)/ length(models))>= 0.5, 1,0))
}
models_rf <- train_random_forest(50, 4, train_data, "y")
library(rpart)
library(ggplot2)
rawdata <- read.csv("A3_train.csv")
# use the first 1500 records in A3_train.csv for training and the last 500 records in A3_train.csv for testing
train_data <- rawdata[1:1500, ]
test_data <- rawdata[1501:2000, ]
set.seed(1234)
# Train a single tree model as our base model, to which we can compare with
# our random forest implementation later
model_rpart <- rpart(y ~., train_data)
pred_rpart <- predict(model_rpart, test_data)
rmse <- function(predicted, actual) {
return(sqrt(mean((predicted - actual)^2)))
}
cat('Single tree model RMSE:', rmse(pred_rpart, test_data$y), '\n')
####################################
####    Manual Random Forest     ###
####################################
train_random_forest <- function(n_trees, n_features,
training_data, target_col_name){
models <- lapply(1:n_trees, function(i) {
# bootstrapping
n_samples <- nrow(training_data)
#generate a number between 0-2000
sample_row_ids <- sample(1:n_samples, replace=TRUE)
new_training_data <- training_data[sample_row_ids, ]
### START CODE HERE ### (â 5 lines)
# Subset n_features columns.
# Be careful to prevent target column from being sampled,
# but make sure it's eventually present in new_training_data
target_col_num <- which(colnames(training_data) == target_col_name)
n_cols <- ncol(training_data[,-target_col_num])
# Subset n_features columns.
sample_col_ids <- sample(1:n_cols, size=n_features, replace=FALSE)
new_training_data2 <- new_training_data[,-target_col_num][,sample_col_ids]
new_training_data2[,target_col_num] <- new_training_data[,target_col_num]
### END CODE HERE ###
formula <- as.formula(paste(target_col_name, '~.'))
new_model <- rpart(formula, data=new_training_data2)
### START CODE HERE ### (â 2 lines)
# post-prune the rpart model & return it
best_cp <- new_model$cptable[which.min(new_model$cptable[,'xerror']), 'CP']
new_model <- rpart(formula, data=new_training_data2, control=rpart.control(cp=best_cp))
return(new_model)
### END CODE HERE ###
})
return(models)
}
predict_random_forest <- function(models, test_data) {
preds <- sapply(models, function(model) {
return(predict(model, test_data,type="class"))
})
preds1 <- mapply(preds,FUN=as.numeric)
preds <- matrix(data = preds1, nrow = nrow(preds), ncol = ncol(preds))
return(ifelse((rowSums(preds)/ length(models))>= 0.5, 1,0))
}
models_rf <- train_random_forest(50, 4, train_data, "y")
train_data[,-y]
select(train_data,-y)
train_data[,-"y"]
train_data[,-y]
library(rpart)
library(ggplot2)
rawdata <- read.csv("A3_train.csv")
# use the first 1500 records in A3_train.csv for training and the last 500 records in A3_train.csv for testing
train_data <- rawdata[1:1500, ]
test_data <- rawdata[1501:2000, ]
set.seed(1234)
# Train a single tree model as our base model, to which we can compare with
# our random forest implementation later
model_rpart <- rpart(y ~., train_data)
pred_rpart <- predict(model_rpart, test_data)
rmse <- function(predicted, actual) {
return(sqrt(mean((predicted - actual)^2)))
}
cat('Single tree model RMSE:', rmse(pred_rpart, test_data$y), '\n')
####################################
####    Manual Random Forest     ###
####################################
library(dplyr)
train_random_forest <- function(n_trees, n_features,
training_data, target_col_name){
models <- lapply(1:n_trees, function(i) {
# bootstrapping
n_samples <- nrow(training_data)
#generate a number between 0-2000
sample_row_ids <- sample(1:n_samples, replace=TRUE)
new_training_data <- training_data[sample_row_ids, ]
### START CODE HERE ### (â 5 lines)
# Subset n_features columns.
# Be careful to prevent target column from being sampled,
# but make sure it's eventually present in new_training_data
new_training_data2 <- select(new_training_data,-target_col_name)
n_cols <- ncol(new_training_data2)
# Subset n_features columns.
sample_col_ids <- sample(c(1:n_cols), size=n_features, replace=FALSE)
new_training_data3 <- new_training_data2[,sample_col_ids]
new_training_data3[,target_col_name] <- new_training_data[,target_col_name]
### END CODE HERE ###
formula <- as.formula(paste(target_col_name, '~.'))
new_model <- rpart(formula, data=new_training_data2)
### START CODE HERE ### (â 2 lines)
# post-prune the rpart model & return it
best_cp <- new_model$cptable[which.min(new_model$cptable[,'xerror']), 'CP']
new_model <- rpart(formula, data=new_training_data2, control=rpart.control(cp=best_cp))
return(new_model)
### END CODE HERE ###
})
return(models)
}
predict_random_forest <- function(models, test_data) {
preds <- sapply(models, function(model) {
return(predict(model, test_data,type="class"))
})
preds1 <- mapply(preds,FUN=as.numeric)
preds <- matrix(data = preds1, nrow = nrow(preds), ncol = ncol(preds))
return(ifelse((rowSums(preds)/ length(models))>= 0.5, 1,0))
}
models_rf <- train_random_forest(50, 4, train_data, "y")
library(rpart)
library(ggplot2)
rawdata <- read.csv("A3_train.csv")
# use the first 1500 records in A3_train.csv for training and the last 500 records in A3_train.csv for testing
train_data <- rawdata[1:1500, ]
test_data <- rawdata[1501:2000, ]
set.seed(1234)
# Train a single tree model as our base model, to which we can compare with
# our random forest implementation later
model_rpart <- rpart(y ~., train_data)
pred_rpart <- predict(model_rpart, test_data)
rmse <- function(predicted, actual) {
return(sqrt(mean((predicted - actual)^2)))
}
cat('Single tree model RMSE:', rmse(pred_rpart, test_data$y), '\n')
####################################
####    Manual Random Forest     ###
####################################
library(dplyr)
train_random_forest <- function(n_trees, n_features,
training_data, target_col_name){
models <- lapply(1:n_trees, function(i) {
# bootstrapping
n_samples <- nrow(training_data)
#generate a number between 0-2000
sample_row_ids <- sample(1:n_samples, replace=TRUE)
new_training_data <- training_data[sample_row_ids, ]
### START CODE HERE ### (â 5 lines)
# Subset n_features columns.
# Be careful to prevent target column from being sampled,
# but make sure it's eventually present in new_training_data
new_training_data2 <- select(new_training_data,-target_col_name)
n_cols <- ncol(new_training_data2)
# Subset n_features columns.
sample_col_ids <- sample(c(1:n_cols), size=n_features, replace=FALSE)
new_training_data3 <- new_training_data2[,sample_col_ids]
new_training_data3[,target_col_name] <- new_training_data[,target_col_name]
### END CODE HERE ###
formula <- as.formula(paste(target_col_name, '~.'))
new_model <- rpart(formula, data=new_training_data3)
### START CODE HERE ### (â 2 lines)
# post-prune the rpart model & return it
best_cp <- new_model$cptable[which.min(new_model$cptable[,'xerror']), 'CP']
new_model <- rpart(formula, data=new_training_data3, control=rpart.control(cp=best_cp))
return(new_model)
### END CODE HERE ###
})
return(models)
}
predict_random_forest <- function(models, test_data) {
preds <- sapply(models, function(model) {
return(predict(model, test_data,type="class"))
})
preds1 <- mapply(preds,FUN=as.numeric)
preds <- matrix(data = preds1, nrow = nrow(preds), ncol = ncol(preds))
return(ifelse((rowSums(preds)/ length(models))>= 0.5, 1,0))
}
models_rf <- train_random_forest(50, 4, train_data, 'y')
pred_rf <- predict_random_forest(models_rf, test_data)
library(rpart)
library(ggplot2)
rawdata <- read.csv("A3_train.csv")
# use the first 1500 records in A3_train.csv for training and the last 500 records in A3_train.csv for testing
train_data <- rawdata[1:1500, ]
test_data <- rawdata[1501:2000, ]
set.seed(1234)
# Train a single tree model as our base model, to which we can compare with
# our random forest implementation later
model_rpart <- rpart(y ~., train_data)
pred_rpart <- predict(model_rpart, test_data)
rmse <- function(predicted, actual) {
return(sqrt(mean((predicted - actual)^2)))
}
cat('Single tree model RMSE:', rmse(pred_rpart, test_data$y), '\n')
####################################
####    Manual Random Forest     ###
####################################
library(dplyr)
train_random_forest <- function(n_trees, n_features,
training_data, target_col_name){
models <- lapply(1:n_trees, function(i) {
# bootstrapping
n_samples <- nrow(training_data)
#generate a number between 0-2000
sample_row_ids <- sample(1:n_samples, replace=TRUE)
new_training_data <- training_data[sample_row_ids, ]
### START CODE HERE ### (â 5 lines)
# Subset n_features columns.
# Be careful to prevent target column from being sampled,
# but make sure it's eventually present in new_training_data
new_training_data2 <- select(new_training_data,-target_col_name)
n_cols <- ncol(new_training_data2)
# Subset n_features columns.
sample_col_ids <- sample(c(1:n_cols), size=n_features, replace=FALSE)
new_training_data3 <- new_training_data2[,sample_col_ids]
new_training_data3[,target_col_name] <- new_training_data[,target_col_name]
### END CODE HERE ###
formula <- as.formula(paste(target_col_name, '~.'))
new_model <- rpart(formula, data=new_training_data3)
### START CODE HERE ### (â 2 lines)
# post-prune the rpart model & return it
best_cp <- new_model$cptable[which.min(new_model$cptable[,'xerror']), 'CP']
new_model <- rpart(formula, data=new_training_data3, control=rpart.control(cp=best_cp))
return(new_model)
### END CODE HERE ###
})
return(models)
}
predict_random_forest <- function(models, test_data) {
preds <- sapply(models, function(model) {
return(predict(model, test_data,type="class"))
})
preds1 <- mapply(preds,FUN=as.numeric)
preds <- matrix(data = preds1, nrow = nrow(preds), ncol = ncol(preds))
return(ifelse((rowSums(preds)/ length(models))>= 0.5, 1,0))
}
models_rf <- train_random_forest(50, 4, train_data, 'y')
pred_rf <- predict_random_forest(models_rf, test_data)
knitr::opts_chunk$set(echo = TRUE)
library(rpart)
library(ggplot2) # for the diamonds dataset
set.seed(1234)
raw_train <- read.csv("A3_train.csv")
# use the first 1500 records in A3_train.csv for training and the last 500 records in A3_train.csv for testing
train_data <- raw_train[1:1500, ]
test_data <- raw_train[1501:2000, ]
train_data$y <- as.factor(train_data$y)
# Train a single tree model as our base model, to which we can compare with
# our random forest implementation later
model_rpart <- rpart(y ~., train_data)
pred_rpart <- predict(model_rpart, test_data, type ="class")
# test accuracy
mean(pred_rpart == test_data$y)
####################################
####    Manual Random Forest     ###
####################################
library(dplyr)
train_random_forest <- function(n_trees, n_features,
train_data, target_col_name){
models <- lapply(1:n_trees, function(i) {
# bootstrapping
n_samples <- nrow(train_data)
sample_row_ids <- sample(1:n_samples, replace=TRUE)
new_training_data <- train_data[sample_row_ids, ]
### START CODE HERE ### (â 5 lines)
new_training_data2 <- select(new_training_data, -target_col_name)
n_cols <- ncol(new_training_data2)
# Subset n_features columns.
sample_col_ids <- sample(c(1:n_cols), size=n_features, replace=FALSE)
new_training_data3 <- new_training_data2[ ,sample_col_ids]
new_training_data3[,target_col_name]<- new_training_data[,target_col_name]
formula <- as.formula(paste(target_col_name, '~.'))
new_model <- rpart(formula, data=new_training_data3)
best_cp <- new_model$cptable[which.min(new_model$cptable[,'xerror']), 'CP']
new_model <- rpart(formula, data=new_training_data3, control = rpart.control(cp=best_cp))
return(new_model)
})
return(models)
}
predict_random_forest <- function(models, test_data) {
preds <- sapply(models, function(model) {
return(predict(model, test_data,type ="class"))
})
preds1 <- mapply(preds,FUN=as.numeric)
preds <- matrix(data = preds1, nrow = nrow(preds), ncol = ncol(preds))
return(ifelse((rowSums(preds)/ length(models))>= 0.5, 1,0))
}
models_rf <- train_random_forest(30,4, train_data, 'y')
pred_rf <- predict_random_forest(models_rf, test_data)
# test accuracy
mean(pred_rf == test_data$y)
library(rpart)
library(ggplot2)
rawdata <- read.csv("A3_train.csv")
# use the first 1500 records in A3_train.csv for training and the last 500 records in A3_train.csv for testing
train_data <- rawdata[1:1500, ]
test_data <- rawdata[1501:2000, ]
set.seed(1234)
# Train a single tree model as our base model, to which we can compare with
# our random forest implementation later
model_rpart <- rpart(y ~., train_data)
pred_rpart <- predict(model_rpart, test_data)
rmse <- function(predicted, actual) {
return(sqrt(mean((predicted - actual)^2)))
}
mean(pred_rpart==test_data$y)
cat('Single tree model RMSE:', rmse(pred_rpart, test_data$y), '\n')
library(rpart)
library(ggplot2)
rawdata <- read.csv("A3_train.csv")
# use the first 1500 records in A3_train.csv for training and the last 500 records in A3_train.csv for testing
train_data <- rawdata[1:1500, ]
test_data <- rawdata[1501:2000, ]
set.seed(1234)
# Train a single tree model as our base model, to which we can compare with
# our random forest implementation later
model_rpart <- rpart(y ~., train_data)
pred_rpart <- predict(model_rpart, test_data,type = "class")
library(rpart)
library(ggplot2)
rawdata <- read.csv("A3_train.csv")
# use the first 1500 records in A3_train.csv for training and the last 500 records in A3_train.csv for testing
train_data <- rawdata[1:1500, ]
test_data <- rawdata[1501:2000, ]
set.seed(1234)
# Train a single tree model as our base model, to which we can compare with
# our random forest implementation later
model_rpart <- rpart(y ~., train_data)
pred_rpart <- predict(model_rpart, test_data)
rmse <- function(predicted, actual) {
return(sqrt(mean((predicted - actual)^2)))
}
mean(pred_rpart==test_data$y)
cat('Single tree model RMSE:', rmse(pred_rpart, test_data$y), '\n')
####################################
####    Manual Random Forest     ###
####################################
library(dplyr)
train_random_forest <- function(n_trees, n_features,
training_data, target_col_name){
models <- lapply(1:n_trees, function(i) {
# bootstrapping
n_samples <- nrow(training_data)
#generate a number between 0-2000
sample_row_ids <- sample(1:n_samples, replace=TRUE)
new_training_data <- training_data[sample_row_ids, ]
### START CODE HERE ### (â 5 lines)
# Subset n_features columns.
# Be careful to prevent target column from being sampled,
# but make sure it's eventually present in new_training_data
new_training_data2 <- select(new_training_data,-target_col_name)
n_cols <- ncol(new_training_data2)
# Subset n_features columns.
sample_col_ids <- sample(c(1:n_cols), size=n_features, replace=FALSE)
new_training_data3 <- new_training_data2[,sample_col_ids]
new_training_data3[,target_col_name] <- new_training_data[,target_col_name]
### END CODE HERE ###
formula <- as.formula(paste(target_col_name, '~.'))
new_model <- rpart(formula, data=new_training_data3)
### START CODE HERE ### (â 2 lines)
# post-prune the rpart model & return it
best_cp <- new_model$cptable[which.min(new_model$cptable[,'xerror']), 'CP']
new_model <- rpart(formula, data=new_training_data3, control=rpart.control(cp=best_cp))
return(new_model)
### END CODE HERE ###
})
return(models)
}
predict_random_forest <- function(models, test_data) {
preds <- sapply(models, function(model) {
return(predict(model, test_data))
})
preds1 <- mapply(preds,FUN=as.numeric)
preds <- matrix(data = preds1, nrow = nrow(preds), ncol = ncol(preds))
return(ifelse((rowSums(preds)/ length(models))>= 0.5, 1,0))
}
models_rf <- train_random_forest(50, 4, train_data, 'y')
pred_rf <- predict_random_forest(models_rf, test_data)
mean(pred_rf == test_data$y)
# set global chunk options
library("knitr")
library("ggplot2")
library("tree")
library("tidyr")
opts_chunk$set(cache=TRUE, message=FALSE)
# set global chunk options
library("knitr")
library("ggplot2")
library("tree")
library("tidyr")
opts_chunk$set(cache=TRUE, message=FALSE)
set.seed(135790)
# static parameters
x.min <- -1
x.max <- 1
sigma <- 0.5
# function for generating data
data.gen <- function(N=2) {
x <- runif(n=N, min=x.min, max=x.max)
f.x <- sin(pi * x)
epsilon <- rnorm(n=N, mean=0, sd=sigma)
y <- f.x + epsilon
return(data.frame(x=x, y=y))
}
M <- 10000
x.out <- sort(runif(n=M, min=x.min, max=x.max))  # order the data points by x for easy plots
f.x.out <- sin(pi * x.out)
y.out <- f.x.out + rnorm(n=M, mean=0, sd=sigma)
data.out <- data.frame(x=x.out, y=y.out)
# function to generate constant prediction
h0 <- function(data) {
return(c(mean(data$y), 0))  # return the intercept and slope as a vector (slope is zero)
}
# function to generate line prediciton
h1 <- function(data) {
lr <- lm(y ~ x, data)  # run a linear regression over the N data points
return(coef(lr))  # return the intercept and slope
}
# function to generate tree prediciton
h3 <- function(data.train, data.test) {
tree.fit <- tree(y ~ x, data.train)  # run a regression tree over the N data points
return(predict(tree.fit, newdata=data.test))
}
N <- 50
set.seed(4680)
data0 <- data.gen(N=N)
coef.h0 <- h0(data0)
coef.h1 <- h1(data0)
yhat.out.h3 <- h3(data0, data.out)
x.lim <- c(x.min, x.max)
y.lim <- c(-2.5, 2.5)
ggplot()  + geom_abline(intercept=coef.h0[1], slope=coef.h0[2], color="red") +
geom_abline(intercept=coef.h1[1], slope=coef.h1[2], color="blue") +
geom_line(aes(x=x.out, y=yhat.out.h3), color="green") +
geom_point(data=data0, aes(x=x, y=y)) +
geom_line(aes(x=x.out, y=f.x.out), color="darkblue", size=1) + labs(x="x", y="y") +
coord_cartesian(ylim=y.lim, xlim=x.lim) + theme_bw()
# function for generating Bootstrap data sample
data.bootstrap <- function(data) {
index <- sample(N, N, replace=TRUE)
return(data[index, ])
}
heart <- read.csv(file="Heart.csv", row.names=1)
heart <- read.csv(file="Heart.csv", row.names=1)
heart <- read.csv(file="Heart.csv", row.names=1)
getwd()
setwd(C:/Users/Administrator/Desktop)
setwd(C:\Users\Administrator\Desktop)
heart <- read.csv(file="Heart.csv", row.names=1)
getwd()
