heart$Sex <- as.factor(heart$Sex)
heart$Fbs <- as.factor(heart$Fbs)
heart$RestECG <- as.factor(heart$RestECG)
heart$ExAng <- as.factor(heart$ExAng)
heart$Slope <- as.factor(heart$Slope)
heart$Ca <- as.factor(heart$Ca)
summary(heart)
# split training and test data 50/50
N <- nrow(heart)
N.train <- round(N/2)
set.seed(456)
train.index <- sample(1:N, N.train)
data.train <- heart[train.index, ]
data.test <- heart[- train.index,]
# put your R code here inside the blocks
library("glmnetUtils")
#Ridge Regression model
ridge <- glmnet(AHD ~ ., data.train,family="binomial",alpha=0,lambda = 0.0402, use.model.frame=TRUE)
#print the coefficients of the optimal model
coef(ridge)
#LASSO model
lasso <- glmnet(AHD ~ .,data.train, family="binomial",alpha=1,lambda = 0.0186, use.model.frame=TRUE)
#print the coefficients of the optimal model
coef(lasso)
# put your R code here inside the blocks
coefficient_ridge <- matrix()
coefficient_lasso <- matrix()
for(i in 1:500){
n_samples <- nrow(data.train)
sample_row_ids <- sample(1:n_samples,n_samples, replace=TRUE)
new.data.train <- data.train[sample_row_ids, ]
ridge_boot <- glmnet(AHD ~ ., new.data.train,family="binomial",alpha=0,lambda = 0.0402, use.model.frame=TRUE)
coefficient_ridge[i,] <- coef(ridge_boot)
lasso_boot <- glmnet(AHD ~ .,new.data.train, family="binomial",alpha=1,lambda = 0.0186, use.model.frame=TRUE)
coefficient_lasso[i,] <- coef(lasso_boot)
}
options(width = 100)  # set output width
# Section G?
# Group ??
# Members: YOUR NAMES HERE
heart <- read.csv(file="Heart.csv", row.names=1)
summary(heart)
# clean the NA's
heart <- na.omit(heart)
# convert to factors
heart$Sex <- as.factor(heart$Sex)
heart$Fbs <- as.factor(heart$Fbs)
heart$RestECG <- as.factor(heart$RestECG)
heart$ExAng <- as.factor(heart$ExAng)
heart$Slope <- as.factor(heart$Slope)
heart$Ca <- as.factor(heart$Ca)
summary(heart)
# split training and test data 50/50
N <- nrow(heart)
N.train <- round(N/2)
set.seed(456)
train.index <- sample(1:N, N.train)
data.train <- heart[train.index, ]
data.test <- heart[- train.index,]
# put your R code here inside the blocks
library("glmnetUtils")
#Ridge Regression model
ridge <- glmnet(AHD ~ ., data.train,family="binomial",alpha=0,lambda = 0.0402, use.model.frame=TRUE)
#print the coefficients of the optimal model
coef(ridge)
#LASSO model
lasso <- glmnet(AHD ~ .,data.train, family="binomial",alpha=1,lambda = 0.0186, use.model.frame=TRUE)
#print the coefficients of the optimal model
coef(lasso)
# put your R code here inside the blocks
coefficient_ridge <- matrix()
coefficient_lasso <- matrix()
for(i in 1:500){
n_samples <- nrow(data.train)
sample_row_ids <- sample(1:n_samples,n_samples, replace=TRUE)
new.data.train <- data.train[sample_row_ids, ]
ridge_boot <- glmnet(AHD ~ ., new.data.train,family="binomial",alpha=0,lambda = 0.0402, use.model.frame=TRUE)
coefficient_ridge[i] <- coef(ridge_boot)
lasso_boot <- glmnet(AHD ~ .,new.data.train, family="binomial",alpha=1,lambda = 0.0186, use.model.frame=TRUE)
coefficient_lasso[i] <- coef(lasso_boot)
}
summary(coefficient_lasso)
options(width = 100)  # set output width
# Section G?
# Group ??
# Members: YOUR NAMES HERE
heart <- read.csv(file="Heart.csv", row.names=1)
summary(heart)
# clean the NA's
heart <- na.omit(heart)
# convert to factors
heart$Sex <- as.factor(heart$Sex)
heart$Fbs <- as.factor(heart$Fbs)
heart$RestECG <- as.factor(heart$RestECG)
heart$ExAng <- as.factor(heart$ExAng)
heart$Slope <- as.factor(heart$Slope)
heart$Ca <- as.factor(heart$Ca)
summary(heart)
# split training and test data 50/50
N <- nrow(heart)
N.train <- round(N/2)
set.seed(456)
train.index <- sample(1:N, N.train)
data.train <- heart[train.index, ]
data.test <- heart[- train.index,]
# put your R code here inside the blocks
library("glmnetUtils")
#Ridge Regression model
ridge <- glmnet(AHD ~ ., data.train,family="binomial",alpha=0,lambda = 0.0402, use.model.frame=TRUE)
#print the coefficients of the optimal model
coef(ridge)
#LASSO model
lasso <- glmnet(AHD ~ .,data.train, family="binomial",alpha=1,lambda = 0.0186, use.model.frame=TRUE)
#print the coefficients of the optimal model
coef(lasso)
# put your R code here inside the blocks
coefficient_ridge <- list()
coefficient_lasso <- list()
for(i in 1:500){
n_samples <- nrow(data.train)
sample_row_ids <- sample(1:n_samples,n_samples, replace=TRUE)
new.data.train <- data.train[sample_row_ids, ]
ridge_boot <- glmnet(AHD ~ ., new.data.train,family="binomial",alpha=0,lambda = 0.0402, use.model.frame=TRUE)
coefficient_ridge[i] <- coef(ridge_boot)
lasso_boot <- glmnet(AHD ~ .,new.data.train, family="binomial",alpha=1,lambda = 0.0186, use.model.frame=TRUE)
coefficient_lasso[i] <- coef(lasso_boot)
}
summary(coefficient_lasso)
options(width = 100)  # set output width
# Section G?
# Group ??
# Members: YOUR NAMES HERE
heart <- read.csv(file="Heart.csv", row.names=1)
summary(heart)
# clean the NA's
heart <- na.omit(heart)
# convert to factors
heart$Sex <- as.factor(heart$Sex)
heart$Fbs <- as.factor(heart$Fbs)
heart$RestECG <- as.factor(heart$RestECG)
heart$ExAng <- as.factor(heart$ExAng)
heart$Slope <- as.factor(heart$Slope)
heart$Ca <- as.factor(heart$Ca)
summary(heart)
# split training and test data 50/50
N <- nrow(heart)
N.train <- round(N/2)
set.seed(456)
train.index <- sample(1:N, N.train)
data.train <- heart[train.index, ]
data.test <- heart[- train.index,]
# put your R code here inside the blocks
library("glmnetUtils")
#Ridge Regression model
ridge <- glmnet(AHD ~ ., data.train,family="binomial",alpha=0,lambda = 0.0402, use.model.frame=TRUE)
#print the coefficients of the optimal model
coef(ridge)
#LASSO model
lasso <- glmnet(AHD ~ .,data.train, family="binomial",alpha=1,lambda = 0.0186, use.model.frame=TRUE)
#print the coefficients of the optimal model
coef(lasso)
# put your R code here inside the blocks
coefficient_ridge <- as.data.frame()
# put your R code here inside the blocks
coefficient_ridge <- data.frame()
coefficient_lasso <- data.frame()
for(i in 1:500){
n_samples <- nrow(data.train)
sample_row_ids <- sample(1:n_samples,n_samples, replace=TRUE)
new.data.train <- data.train[sample_row_ids, ]
ridge_boot <- glmnet(AHD ~ ., new.data.train,family="binomial",alpha=0,lambda = 0.0402, use.model.frame=TRUE)
coefficient_ridge[i,] <- coef(ridge_boot)
lasso_boot <- glmnet(AHD ~ .,new.data.train, family="binomial",alpha=1,lambda = 0.0186, use.model.frame=TRUE)
coefficient_lasso[i,] <- coef(lasso_boot)
}
colnames(data.train)
options(width = 100)  # set output width
# Section G?
# Group ??
# Members: YOUR NAMES HERE
heart <- read.csv(file="Heart.csv", row.names=1)
summary(heart)
# clean the NA's
heart <- na.omit(heart)
# convert to factors
heart$Sex <- as.factor(heart$Sex)
heart$Fbs <- as.factor(heart$Fbs)
heart$RestECG <- as.factor(heart$RestECG)
heart$ExAng <- as.factor(heart$ExAng)
heart$Slope <- as.factor(heart$Slope)
heart$Ca <- as.factor(heart$Ca)
summary(heart)
# split training and test data 50/50
N <- nrow(heart)
N.train <- round(N/2)
set.seed(456)
train.index <- sample(1:N, N.train)
data.train <- heart[train.index, ]
data.test <- heart[- train.index,]
# put your R code here inside the blocks
library("glmnetUtils")
#Ridge Regression model
ridge <- glmnet(AHD ~ ., data.train,family="binomial",alpha=0,lambda = 0.0402, use.model.frame=TRUE)
#print the coefficients of the optimal model
coef(ridge)
#LASSO model
lasso <- glmnet(AHD ~ .,data.train, family="binomial",alpha=1,lambda = 0.0186, use.model.frame=TRUE)
#print the coefficients of the optimal model
coef(lasso)
options(width = 100)  # set output width
# Section G?
# Group ??
# Members: YOUR NAMES HERE
heart <- read.csv(file="Heart.csv", row.names=1)
summary(heart)
# clean the NA's
heart <- na.omit(heart)
# convert to factors
heart$Sex <- as.factor(heart$Sex)
heart$Fbs <- as.factor(heart$Fbs)
heart$RestECG <- as.factor(heart$RestECG)
heart$ExAng <- as.factor(heart$ExAng)
heart$Slope <- as.factor(heart$Slope)
heart$Ca <- as.factor(heart$Ca)
summary(heart)
# split training and test data 50/50
N <- nrow(heart)
N.train <- round(N/2)
set.seed(456)
train.index <- sample(1:N, N.train)
data.train <- heart[train.index, ]
data.test <- heart[- train.index,]
# put your R code here inside the blocks
library("glmnetUtils")
#Ridge Regression model
ridge <- glmnet(AHD ~ ., data.train,family="binomial",alpha=0,lambda = 0.0402, use.model.frame=TRUE)
#print the coefficients of the optimal model
coef(ridge)
#LASSO model
lasso <- glmnet(AHD ~ .,data.train, family="binomial",alpha=1,lambda = 0.0186, use.model.frame=TRUE)
#print the coefficients of the optimal model
coef(lasso)
# put your R code here inside the blocks
coefficient_ridge <- data.frame(nrow=500,ncol=21)
coefficient_lasso <- data.frame(nrow=500,ncol=21)
for(i in 1:500){
n_samples <- nrow(data.train)
sample_row_ids <- sample(1:n_samples,n_samples, replace=TRUE)
new.data.train <- data.train[sample_row_ids, ]
ridge_boot <- glmnet(AHD ~ ., new.data.train,family="binomial",alpha=0,lambda = 0.0402, use.model.frame=TRUE)
coefficient_ridge[i,] <- coef(ridge_boot)
lasso_boot <- glmnet(AHD ~ .,new.data.train, family="binomial",alpha=1,lambda = 0.0186, use.model.frame=TRUE)
coefficient_lasso[i,] <- coef(lasso_boot)
}
View(coefficient_lasso)
?data.frame
colnames(data.train)
paste(colnames(data.train),collapse = +)
paste(colnames(data.train),collapse = "+")
paste(colnames(data.train),collapse = ",")
options(width = 100)  # set output width
# Section G?
# Group ??
# Members: YOUR NAMES HERE
heart <- read.csv(file="Heart.csv", row.names=1)
summary(heart)
# clean the NA's
heart <- na.omit(heart)
# convert to factors
heart$Sex <- as.factor(heart$Sex)
heart$Fbs <- as.factor(heart$Fbs)
heart$RestECG <- as.factor(heart$RestECG)
heart$ExAng <- as.factor(heart$ExAng)
heart$Slope <- as.factor(heart$Slope)
heart$Ca <- as.factor(heart$Ca)
summary(heart)
# split training and test data 50/50
N <- nrow(heart)
N.train <- round(N/2)
set.seed(456)
train.index <- sample(1:N, N.train)
data.train <- heart[train.index, ]
data.test <- heart[- train.index,]
# put your R code here inside the blocks
library("glmnetUtils")
#Ridge Regression model
ridge <- glmnet(AHD ~ ., data.train,family="binomial",alpha=0,lambda = 0.0402, use.model.frame=TRUE)
#print the coefficients of the optimal model
coef(ridge)
#LASSO model
lasso <- glmnet(AHD ~ .,data.train, family="binomial",alpha=1,lambda = 0.0186, use.model.frame=TRUE)
#print the coefficients of the optimal model
coef(lasso)
# put your R code here inside the blocks
coefficient_ridge <- setNames(as.data.frame(matrix(nrow = 500, ncol = 21)),c('Intercept','Age','Sex1','ChestPainnonanginal','ChestPainnontypical','ChestPaintypical','RestBP','Chol','Fbs1','RestECG1','RestECG2','MaxHR','ExAng1','Oldpeak','Slope2','Slope3','Ca1','Ca2','Ca3','Thalnormal','Thalreversable'))
coefficient_lasso <- setNames(as.data.frame(matrix(nrow = 500, ncol = 21)),c('Intercept','Age','Sex1','ChestPainnonanginal','ChestPainnontypical','ChestPaintypical','RestBP','Chol','Fbs1','RestECG1','RestECG2','MaxHR','ExAng1','Oldpeak','Slope2','Slope3','Ca1','Ca2','Ca3','Thalnormal','Thalreversable'))
for(i in 1:500){
n_samples <- nrow(data.train)
sample_row_ids <- sample(1:n_samples,n_samples, replace=TRUE)
new.data.train <- data.train[sample_row_ids, ]
ridge_boot <- glmnet(AHD ~ ., new.data.train,family="binomial",alpha=0,lambda = 0.0402, use.model.frame=TRUE)
coefficient_ridge[i,] <- coef(ridge_boot)
lasso_boot <- glmnet(AHD ~ .,new.data.train, family="binomial",alpha=1,lambda = 0.0186, use.model.frame=TRUE)
coefficient_lasso[i,] <- coef(lasso_boot)
}
View(coefficient_lasso)
hist(coefficient_lasso$Intercept)
hist(coefficient_ridge$Intercept)
hist(coefficient_ridge$Age)
hist(coefficient_ridge$MaxHR)
hist(coefficient_ridge$Thalnormal)
# put your R code here inside the blocks
hist(coefficient_ridge$Intercept)
hist(coefficient_ridge$Age)
hist(coefficient_ridge$MaxHR)
hist(coefficient_ridge$Thalnormal)
hist(coefficient_lasso$Intercept)
hist(coefficient_lasso$Age)
hist(coefficient_lasso$MaxHR)
hist(coefficient_lasso$Thalnormal)
plot(coefficient_ridge$Intercept)
hist(coefficient_ridge$Age)
# put your R code here inside the blocks
#for ridge models
hist(coefficient_ridge$Intercept)
hist(coefficient_ridge$Age)
hist(coefficient_ridge$MaxHR)
hist(coefficient_ridge$Thalnormal)
#for lasso models
hist(coefficient_lasso$Intercept)
hist(coefficient_lasso$Age)
hist(coefficient_lasso$MaxHR)
hist(coefficient_lasso$Thalnormal)
# put your R code here inside the blocks
#for ridge models
hist(coefficient_ridge$Intercept)
hist(coefficient_ridge$Age)
hist(coefficient_ridge$MaxHR)
hist(coefficient_ridge$Thalnormal)
#for lasso models
hist(coefficient_lasso$Intercept)
hist(coefficient_lasso$Age)
hist(coefficient_lasso$MaxHR)
hist(coefficient_lasso$Thalnormal)
library(rpart)
library(ggplot2)
rawdata <- read.csv("A3_train.csv")
# use the first 1500 records in A3_train.csv for training and the last 500 records in A3_train.csv for testing
train_data <- rawdata[1:1500, ]
test_data <- rawdata[1501:2000, ]
set.seed(1234)
# Train a single tree model as our base model, to which we can compare with
# our random forest implementation later
model_rpart <- rpart(y ~., train_data)
pred_rpart <- predict(model_rpart, test_data)
rmse <- function(predicted, actual) {
return(sqrt(mean((predicted - actual)^2)))
}
mean(pred_rpart==test_data$y)
cat('Single tree model RMSE:', rmse(pred_rpart, test_data$y), '\n')
####################################
####    Manual Random Forest     ###
####################################
library(dplyr)
train_random_forest <- function(n_trees, n_features,
training_data, target_col_name){
models <- lapply(1:n_trees, function(i) {
# bootstrapping
n_samples <- nrow(training_data)
#generate a number between 0-2000
sample_row_ids <- sample(1:n_samples, replace=TRUE)
new_training_data <- training_data[sample_row_ids, ]
### START CODE HERE ### (â 5 lines)
# Subset n_features columns.
# Be careful to prevent target column from being sampled,
# but make sure it's eventually present in new_training_data
new_training_data2 <- select(new_training_data,-target_col_name)
n_cols <- ncol(new_training_data2)
# Subset n_features columns.
sample_col_ids <- sample(c(1:n_cols), size=n_features, replace=FALSE)
new_training_data3 <- new_training_data2[,sample_col_ids]
new_training_data3[,target_col_name] <- new_training_data[,target_col_name]
### END CODE HERE ###
formula <- as.formula(paste(target_col_name, '~.'))
new_model <- rpart(formula, data=new_training_data3)
### START CODE HERE ### (â 2 lines)
# post-prune the rpart model & return it
best_cp <- new_model$cptable[which.min(new_model$cptable[,'xerror']), 'CP']
new_model <- rpart(formula, data=new_training_data3, control=rpart.control(cp=best_cp))
return(new_model)
### END CODE HERE ###
})
return(models)
}
predict_random_forest <- function(models, test_data) {
preds <- sapply(models, function(model) {
return(predict(model, test_data))
})
preds1 <- mapply(preds,FUN=as.numeric)
preds <- matrix(data = preds1, nrow = nrow(preds), ncol = ncol(preds))
return(ifelse((rowSums(preds)/ length(models))>= 0.5, 1,0))
}
models_rf <- train_random_forest(50, 4, train_data, 'y')
pred_rf <- predict_random_forest(models_rf, test_data)
mean(pred_rf == test_data$y)
library(C50)
library(FNN)
train_a <- train_data[1:500,]
train_b <- train_data[501:1000,]
train_c <- train_data[1001:1500,]
model_c50 <- C5.0(y~.,train_a)
library(rpart)
library(ggplot2)
rawdata <- read.csv("A3_train.csv")
# use the first 1500 records in A3_train.csv for training and the last 500 records in A3_train.csv for testing
rawdata$y <- as.factor(rawdata$y)
train_data <- rawdata[1:1500, ]
test_data <- rawdata[1501:2000, ]
set.seed(1234)
# Train a single tree model as our base model, to which we can compare with
# our random forest implementation later
model_rpart <- rpart(y ~., train_data)
pred_rpart <- predict(model_rpart, test_data)
rmse <- function(predicted, actual) {
return(sqrt(mean((predicted - actual)^2)))
}
mean(pred_rpart==test_data$y)
cat('Single tree model RMSE:', rmse(pred_rpart, test_data$y), '\n')
####################################
####    Manual Random Forest     ###
####################################
library(dplyr)
train_random_forest <- function(n_trees, n_features,
training_data, target_col_name){
models <- lapply(1:n_trees, function(i) {
# bootstrapping
n_samples <- nrow(training_data)
#generate a number between 0-2000
sample_row_ids <- sample(1:n_samples, replace=TRUE)
new_training_data <- training_data[sample_row_ids, ]
### START CODE HERE ### (â 5 lines)
# Subset n_features columns.
# Be careful to prevent target column from being sampled,
# but make sure it's eventually present in new_training_data
new_training_data2 <- select(new_training_data,-target_col_name)
n_cols <- ncol(new_training_data2)
# Subset n_features columns.
sample_col_ids <- sample(c(1:n_cols), size=n_features, replace=FALSE)
new_training_data3 <- new_training_data2[,sample_col_ids]
new_training_data3[,target_col_name] <- new_training_data[,target_col_name]
### END CODE HERE ###
formula <- as.formula(paste(target_col_name, '~.'))
new_model <- rpart(formula, data=new_training_data3)
### START CODE HERE ### (â 2 lines)
# post-prune the rpart model & return it
best_cp <- new_model$cptable[which.min(new_model$cptable[,'xerror']), 'CP']
new_model <- rpart(formula, data=new_training_data3, control=rpart.control(cp=best_cp))
return(new_model)
### END CODE HERE ###
})
return(models)
}
predict_random_forest <- function(models, test_data) {
preds <- sapply(models, function(model) {
return(predict(model, test_data))
})
preds1 <- mapply(preds,FUN=as.numeric)
preds <- matrix(data = preds1, nrow = nrow(preds), ncol = ncol(preds))
return(ifelse((rowSums(preds)/ length(models))>= 0.5, 1,0))
}
models_rf <- train_random_forest(50, 4, train_data, 'y')
pred_rf <- predict_random_forest(models_rf, test_data)
mean(pred_rf == test_data$y)
library(C50)
library(FNN)
train_a <- train_data[1:500,]
train_b <- train_data[501:1000,]
train_c <- train_data[1001:1500,]
model_c50 <- C5.0(y~.,train_a)
model_knn <- knn(train_b,train_b,cl="y",k=3)
library(C50)
library(FNN)
train_a <- train_data[1:500,]
train_b <- train_data[501:1000,]
train_c <- train_data[1001:1500,]
model_c50 <- C5.0(y~.,train_a)
model_knn <- knn(train_b,train_b,cl=train_b$y,k=3)
library(C50)
library(FNN)
train_a <- train_data[1:500,]
train_b <- train_data[501:1000,]
train_b$y <- as.numeric(train_b$y)
train_c <- train_data[1001:1500,]
model_c50 <- C5.0(y~.,train_a)
model_knn <- knn(train_b,train_b,cl=train_b$y,k=3)
models_rf <- train_random_forest(50, 4, train_data, 'y')
pred_a <- predict(model_c50,train_a)
pred_b <- predict(model_knn,train_b)
folds <- createFolds(training$Class, 5)
library(caret)
folds <- createFolds(training$Class, 5)
library(caret)
folds <- createFolds(train_data$y, 5)
control <- trainControl(method='repeatedcv', number=5, repeats=3, index=folds, savePredictions='final', classProbs=TRUE, summaryFunction=twoClassSummary)
algos <- c('C5.0Tree', 'knn','rf')
models <- caretList(y~., data=train_data, metric='AUC', trControl=control, methodList=algos)
library(caret)
library(caretEnsemble)
folds <- createFolds(train_data$y, 5)
control <- trainControl(method='repeatedcv', number=5, repeats=3, index=folds, savePredictions='final', classProbs=TRUE, summaryFunction=twoClassSummary)
algos <- c('C5.0Tree', 'knn','rf')
models <- caretList(y~., data=train_data, metric='AUC', trControl=control, methodList=algos)
