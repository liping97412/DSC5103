# Convert the text to lower case
transcripts <- tm_map(transcripts, content_transformer(tolower))
# Remove numbers
transcripts <- tm_map(transcripts, removeNumbers)
# Remove english common stopwords
transcripts <- tm_map(transcripts, removeWords, stopwords('english'))
# Remove punctuations
transcripts <- tm_map(transcripts, removePunctuation)
# Eliminate extra white spaces
transcripts <- tm_map(transcripts, stripWhitespace)
#stem our words
transcripts <- tm_map(transcripts, stemDocument)
textual_data <- read.csv("transcripts.csv")
library(tm)
doc_ids <- c(1:2467)
df <- data.frame(doc_id = doc_ids, text = textual_data$transcript, stringsAsFactors = FALSE)
transcripts <- Corpus(DataframeSource(df))
# Convert the text to lower case
transcripts <- tm_map(transcripts, content_transformer(tolower))
# Remove numbers
transcripts <- tm_map(transcripts, removeNumbers)
# Remove english common stopwords
transcripts <- tm_map(transcripts, removeWords, stopwords('english'))
# Remove punctuations
transcripts <- tm_map(transcripts, removePunctuation)
# Eliminate extra white spaces
transcripts <- tm_map(transcripts, stripWhitespace)
#stem our words
transcripts <- tm_map(transcripts, stemDocument)
transcripts <- as.data.frame(transcripts)
View(transcripts)
View(transcripts)
install.packages("SentimentAnalysis")
library(SentimentAnalysis)
sentiment <- analyzeSentiment(transcripts)
convertToBinaryResponse(sentiment)$SentimentQDAP
sentiment$SentimentQDAP
plotSentimentResponse(sentiment$SentimentQDAP, response)
heatmap(emotions)
emotions <- sentiment$SentimentQDAP
heatmap(emotions)
heatmap(as.matrix(emotions))
sentiment$WordCount
?analyzeSentiment
sentiment$SentimentGI
emotion <- data.frame(doc_id = doc_ids, emotion = emotions, stringsAsFactors = FALSE)
View(emotion)
heatmap(as.matrix(emotion))
ted <- read.csv("ted_main.csv")
View(ted)
install.packages("kernlab")
install.packages("caret")
install.packages("tm")
install.packages("dplyr")
install.packages("splitstackshape")
install.packages("e1071")
install.packages("kernlab")
install.packages("jsonlite")
View(ted)
ted$ratings <- gsub("\'","\"", ted$ratings)
View(ted)
options(width = 100)  # set output width, turn off scientific notation for big numbers
# Section A1
# Group 13
# Members: YOUR NAMES HERE
heart <- read.csv(file="Heart.csv", row.names=1)
summary(heart)
# clean the NA's
heart <- na.omit(heart)
# convert to factors
heart$Sex <- as.factor(heart$Sex)
heart$Fbs <- as.factor(heart$Fbs)
heart$RestECG <- as.factor(heart$RestECG)
heart$ExAng <- as.factor(heart$ExAng)
heart$Slope <- as.factor(heart$Slope)
heart$Ca <- as.factor(heart$Ca)
summary(heart)
# split training and test data 50/50
N <- nrow(heart)
set.seed(456)
train.index <- sample(1:N, round(N/2))
test.index <- - train.index
x.test <- heart[test.index, 1:13]
y.test <- heart[test.index, 14]
# put your R code here inside the blocks
### Tree as a benchmark
library("tree")
# grow a tree
heart.tree <- tree(AHD ~ ., heart, subset=train.index)
summary(heart.tree)
# CV
set.seed(12345)
heart.tree.cv <- cv.tree(heart.tree)
plot(heart.tree.cv)
# prune
heart.tree.cv$size[which.min(heart.tree.cv$dev)]
heart.tree.pruned <- prune.tree(heart.tree, best=2)
# predict the probs of Yes
yhat.tree <- predict(heart.tree.pruned, newdata=x.test,type = "vector")[,2]
# put your R code here inside the blocks
library("randomForest")
## tuning RF
# tune random forest (mtry) by tuneRF (highly variable)
set.seed(12345)
tuneRF(x=heart[train.index, -14], y=heart[train.index, 14], ntreeTry=501, stepFactor=0.5)
##final model
heart.rf <- randomForest(AHD ~ ., data=heart, subset=train.index,ntree=501,mtry=1)
# predict the probs of Yes
yhat.rf <- predict(heart.rf, newdata=x.test,type = "prob")[,2]
# put your R code here inside the blocks
# rebuild the model with "importance=TRUE" for more variable importance (%IncMSE)
heart.rf2 <- randomForest(AHD ~ ., data=heart, subset=train.index,ntree=501,mtry=1,importance=TRUE)
heart.rf2
# variable importance
importance(heart.rf2)
varImpPlot(heart.rf2)
objective <- "binary:logistic"
cv.fold <- 10
# parameter ranges
max_depths <- c( 1, 2, 4,  6, 8)  # candidates for d
etas <- c(0.01, 0.005, 0.001, 0.0005, 0.0001)  # candidates for lambda 0.05,
subsamples <- c(0.5, 0.75, 1) #
colsamples <- c(0.6, 0.8, 1) #
set.seed(12345)
fold <- sample(rep(seq(cv.fold), length=nrow(heart[train.index, ])))
fold.list <- list()
for (k in 1:cv.fold) {
fold.list[[k]] <- which(fold==k)
}
tune.out <- data.frame()
for (max_depth in max_depths) {
for (eta in etas) {
for (subsample in subsamples) {
for (colsample in colsamples) {
# **calculate max n.trees by my secret formula**
n.max <- round(100 / (eta * sqrt(max_depth)))
if (objective == "reg:linear") {
xgb.cv.fit <- xgb.cv(data = dtrain, objective=objective, verbose=0,
folds=fold.list, early_stopping_rounds=100,
nrounds=n.max, max_depth=max_depth, eta=eta, subsample=subsample, colsample_bytree=colsample)
n.best <- xgb.cv.fit$best_ntreelimit
cv.err <- xgb.cv.fit$evaluation_log$test_rmse_mean[n.best]
} else if (objective == "binary:logistic") {
xgb.cv.fit <- xgb.cv(data = dtrain, objective=objective,  verbose=0,
folds=fold.list, early_stopping_rounds=1000, eval_metric="logloss", # "error", "auc"
nrounds=n.max, max_depth=max_depth, eta=eta, subsample=subsample, colsample_bytree=colsample)
n.best <- xgb.cv.fit$best_ntreelimit
#cv.err <- xgb.cv.fit$evaluation_log$test_error_mean[n.best]
cv.err <- xgb.cv.fit$evaluation_log$test_logloss_mean[n.best]
#cv.err <- xgb.cv.fit$evaluation_log$test_auc_mean[n.best]
}
out <- data.frame(max_depth=max_depth, eta=eta, subsample=subsample, colsample=colsample, n.max=n.max, nrounds=n.best, cv.err=cv.err)
print(out)
tune.out <- rbind(tune.out, out)
}
}
}
}
library("xgboost")
set.seed(12345)
fold <- sample(rep(seq(cv.fold), length=nrow(heart[train.index, ])))
fold.list <- list()
for (k in 1:cv.fold) {
fold.list[[k]] <- which(fold==k)
}
tune.out <- data.frame()
for (max_depth in max_depths) {
for (eta in etas) {
for (subsample in subsamples) {
for (colsample in colsamples) {
# **calculate max n.trees by my secret formula**
n.max <- round(100 / (eta * sqrt(max_depth)))
if (objective == "reg:linear") {
xgb.cv.fit <- xgb.cv(data = dtrain, objective=objective, verbose=0,
folds=fold.list, early_stopping_rounds=100,
nrounds=n.max, max_depth=max_depth, eta=eta, subsample=subsample, colsample_bytree=colsample)
n.best <- xgb.cv.fit$best_ntreelimit
cv.err <- xgb.cv.fit$evaluation_log$test_rmse_mean[n.best]
} else if (objective == "binary:logistic") {
xgb.cv.fit <- xgb.cv(data = dtrain, objective=objective,  verbose=0,
folds=fold.list, early_stopping_rounds=1000, eval_metric="logloss", # "error", "auc"
nrounds=n.max, max_depth=max_depth, eta=eta, subsample=subsample, colsample_bytree=colsample)
n.best <- xgb.cv.fit$best_ntreelimit
#cv.err <- xgb.cv.fit$evaluation_log$test_error_mean[n.best]
cv.err <- xgb.cv.fit$evaluation_log$test_logloss_mean[n.best]
#cv.err <- xgb.cv.fit$evaluation_log$test_auc_mean[n.best]
}
out <- data.frame(max_depth=max_depth, eta=eta, subsample=subsample, colsample=colsample, n.max=n.max, nrounds=n.best, cv.err=cv.err)
print(out)
tune.out <- rbind(tune.out, out)
}
}
}
}
# generate data partition
set.seed(12345)
fold <- sample(rep(seq(cv.fold), length=nrow(heart[train.index, ])))
fold.list <- list()
for (k in 1:cv.fold) {
fold.list[[k]] <- which(fold==k)
}
tune.out <- data.frame()
for (max_depth in max_depths) {
for (eta in etas) {
for (subsample in subsamples) {
for (colsample in colsamples) {
# **calculate max n.trees by my secret formula**
n.max <- round(100 / (eta * sqrt(max_depth)))
if (objective == "reg:linear") {
xgb.cv.fit <- xgb.cv(data = heart[train.index, ], objective=objective, verbose=0,
folds=fold.list, early_stopping_rounds=100,
nrounds=n.max, max_depth=max_depth, eta=eta, subsample=subsample, colsample_bytree=colsample)
n.best <- xgb.cv.fit$best_ntreelimit
cv.err <- xgb.cv.fit$evaluation_log$test_rmse_mean[n.best]
} else if (objective == "binary:logistic") {
xgb.cv.fit <- xgb.cv(data = dtrain, objective=objective,  verbose=0,
folds=fold.list, early_stopping_rounds=1000, eval_metric="logloss", # "error", "auc"
nrounds=n.max, max_depth=max_depth, eta=eta, subsample=subsample, colsample_bytree=colsample)
n.best <- xgb.cv.fit$best_ntreelimit
#cv.err <- xgb.cv.fit$evaluation_log$test_error_mean[n.best]
cv.err <- xgb.cv.fit$evaluation_log$test_logloss_mean[n.best]
#cv.err <- xgb.cv.fit$evaluation_log$test_auc_mean[n.best]
}
out <- data.frame(max_depth=max_depth, eta=eta, subsample=subsample, colsample=colsample, n.max=n.max, nrounds=n.best, cv.err=cv.err)
print(out)
tune.out <- rbind(tune.out, out)
}
}
}
}
set.seed(12345)
fold <- sample(rep(seq(cv.fold), length=nrow(heart[train.index, ])))
fold.list <- list()
for (k in 1:cv.fold) {
fold.list[[k]] <- which(fold==k)
}
tune.out <- data.frame()
for (max_depth in max_depths) {
for (eta in etas) {
for (subsample in subsamples) {
for (colsample in colsamples) {
# **calculate max n.trees by my secret formula**
n.max <- round(100 / (eta * sqrt(max_depth)))
if (objective == "reg:linear") {
xgb.cv.fit <- xgb.cv(data = heart[train.index, ], objective=objective, verbose=0,
folds=fold.list, early_stopping_rounds=100,
nrounds=n.max, max_depth=max_depth, eta=eta, subsample=subsample, colsample_bytree=colsample)
n.best <- xgb.cv.fit$best_ntreelimit
cv.err <- xgb.cv.fit$evaluation_log$test_rmse_mean[n.best]
} else if (objective == "binary:logistic") {
xgb.cv.fit <- xgb.cv(data = heart[train.index, ], objective=objective,  verbose=0,
folds=fold.list, early_stopping_rounds=1000, eval_metric="logloss", # "error", "auc"
nrounds=n.max, max_depth=max_depth, eta=eta, subsample=subsample, colsample_bytree=colsample)
n.best <- xgb.cv.fit$best_ntreelimit
#cv.err <- xgb.cv.fit$evaluation_log$test_error_mean[n.best]
cv.err <- xgb.cv.fit$evaluation_log$test_logloss_mean[n.best]
#cv.err <- xgb.cv.fit$evaluation_log$test_auc_mean[n.best]
}
out <- data.frame(max_depth=max_depth, eta=eta, subsample=subsample, colsample=colsample, n.max=n.max, nrounds=n.best, cv.err=cv.err)
print(out)
tune.out <- rbind(tune.out, out)
}
}
}
}
options(width = 100)  # set output width, turn off scientific notation for big numbers
# Section A1
# Group 13
# Members: YOUR NAMES HERE
heart <- read.csv(file="Heart.csv", row.names=1)
summary(heart)
# clean the NA's
heart <- na.omit(heart)
# convert to factors
heart$Sex <- as.factor(heart$Sex)
heart$Fbs <- as.factor(heart$Fbs)
heart$RestECG <- as.factor(heart$RestECG)
heart$ExAng <- as.factor(heart$ExAng)
heart$Slope <- as.factor(heart$Slope)
heart$Ca <- as.factor(heart$Ca)
summary(heart)
# split training and test data 50/50
N <- nrow(heart)
set.seed(456)
train.index <- sample(1:N, round(N/2))
test.index <- - train.index
x.test <- heart[test.index, 1:13]
y.test <- heart[test.index, 14]
# put your R code here inside the blocks
### Tree as a benchmark
library("tree")
# grow a tree
heart.tree <- tree(AHD ~ ., heart, subset=train.index)
summary(heart.tree)
# CV
set.seed(12345)
heart.tree.cv <- cv.tree(heart.tree)
plot(heart.tree.cv)
# prune
heart.tree.cv$size[which.min(heart.tree.cv$dev)]
heart.tree.pruned <- prune.tree(heart.tree, best=2)
# predict the probs of Yes
yhat.tree <- predict(heart.tree.pruned, newdata=x.test,type = "vector")[,2]
# put your R code here inside the blocks
library("randomForest")
## tuning RF
# tune random forest (mtry) by tuneRF (highly variable)
set.seed(12345)
tuneRF(x=heart[train.index, -14], y=heart[train.index, 14], ntreeTry=501, stepFactor=0.5)
##final model
heart.rf <- randomForest(AHD ~ ., data=heart, subset=train.index,ntree=501,mtry=1)
# predict the probs of Yes
yhat.rf <- predict(heart.rf, newdata=x.test,type = "prob")[,2]
# put your R code here inside the blocks
# rebuild the model with "importance=TRUE" for more variable importance (%IncMSE)
heart.rf2 <- randomForest(AHD ~ ., data=heart, subset=train.index,ntree=501,mtry=1,importance=TRUE)
heart.rf2
# variable importance
importance(heart.rf2)
varImpPlot(heart.rf2)
# put your R code here inside the blocks
### TUNING xgboost
library("xgboost")
heart$AHD <- as.numeric(heart$AHD)-1
x.train <- model.matrix(AHD ~ ., data=heart[train.index, ])[, -1]
y.train <- heart[train.index, "AHD"]
x.test <- model.matrix(AHD ~ ., data=heart[-train.index, ])[ , -1]
y.test <- heart[-train.index, "AHD"]
## Construct a dense matrix for xgboost to consume
dtrain <- xgb.DMatrix(data=x.train, label=y.train)
## input required
# data: dtrain in xgb.DMatrix format
objective <- "binary:logistic"
cv.fold <- 10
# parameter ranges
max_depths <- c( 1, 2, 4,  6, 8)  # candidates for d
etas <- c(0.01, 0.005, 0.001, 0.0005, 0.0001)  # candidates for lambda 0.05,
subsamples <- c(0.5, 0.75, 1) #
colsamples <- c(0.6, 0.8, 1) #
# generate data partition
set.seed(12345)
fold <- sample(rep(seq(cv.fold), length=nrow(heart[train.index, ])))
fold.list <- list()
for (k in 1:cv.fold) {
fold.list[[k]] <- which(fold==k)
}
tune.out <- data.frame()
for (max_depth in max_depths) {
for (eta in etas) {
for (subsample in subsamples) {
for (colsample in colsamples) {
# **calculate max n.trees by my secret formula**
n.max <- round(100 / (eta * sqrt(max_depth)))
if (objective == "reg:linear") {
xgb.cv.fit <- xgb.cv(data = dtrain, objective=objective, verbose=0,
folds=fold.list, early_stopping_rounds=100,
nrounds=n.max, max_depth=max_depth, eta=eta, subsample=subsample, colsample_bytree=colsample)
n.best <- xgb.cv.fit$best_ntreelimit
cv.err <- xgb.cv.fit$evaluation_log$test_rmse_mean[n.best]
} else if (objective == "binary:logistic") {
xgb.cv.fit <- xgb.cv(data = dtrain, objective=objective,  verbose=0,
folds=fold.list, early_stopping_rounds=1000, eval_metric="logloss", # "error", "auc"
nrounds=n.max, max_depth=max_depth, eta=eta, subsample=subsample, colsample_bytree=colsample)
n.best <- xgb.cv.fit$best_ntreelimit
#cv.err <- xgb.cv.fit$evaluation_log$test_error_mean[n.best]
cv.err <- xgb.cv.fit$evaluation_log$test_logloss_mean[n.best]
#cv.err <- xgb.cv.fit$evaluation_log$test_auc_mean[n.best]
}
out <- data.frame(max_depth=max_depth, eta=eta, subsample=subsample, colsample=colsample, n.max=n.max, nrounds=n.best, cv.err=cv.err)
print(out)
tune.out <- rbind(tune.out, out)
}
}
}
}
library(lattice)
library(grDevices)
library(vcd)
library(flexclust)
library(tidyverse)
load(file = "InterestPreferenceSurvey.Rda")
str(csb)
map_df(csb,mean)
require(flexclust)
library(lattice)
library(grDevices)
library(vcd)
library(flexclust)
library(tidyverse)
install.packages("vcd")
install.packages("flexclust")
library(lattice)
library(grDevices)
library(vcd)
library(flexclust)
library(tidyverse)
load(file = "InterestPreferenceSurvey.Rda")
getwd()
getwd()
load(file = "InterestPreferenceSurvey.Rda")
str(csb)
map_df(csb,mean)
View(csb)
require(flexclust)
fc_cont <- new("flexclustControl")
fc_cont@tolerance <- 0.1 ## this doesn't seem to work as expected
fc_cont@iter.max <- 500 ## seems to be effective convergence
my_seed <- 0
my_family <- "ejaccard"
num_clust <- 4
my_seed <- my_seed + 1
set.seed(my_seed)
cl <- kcca(csb, k = num_clust, save.data = TRUE, control = fc_cont,
family = kccaFamily(my_family))
summary(cl)
info(cl,"help")
info(cl,"size")
info(cl,"av_dist")
info(cl,"max_dist")
info(cl,"separation")
info(cl,"distsum")
pop_av_dist <- with(cl@clusinfo, sum(size*av_dist)/sum(size))
cl@family@name
main_txt <- paste("kcca ", cl@family@name, " - ",
num_clust, " clusters (",
20, "k sample, seed = ", my_seed,
")", sep = "")
csb_pca <- prcomp(csb)
plot(cl,
data = as.matrix(csb),
project = csb_pca, which=1:2,
points = TRUE,
main = main_txt,
sub = paste("\nAv Dist = ", format(pop_av_dist, digits = 5),
", k = ", cl@k, sep = "")
)
print(barchart(cl,
main = main_txt, strip.prefix = "#",
scales = list(cex = 0.6),
shade = TRUE,
legend = TRUE
))
library(tidyverse)
# Try theoretical kmeans 3 cluster
set.seed(27)
centers <- tibble(
cluster = factor(1:3),
num_points = c(100, 150, 50),  # number points in each cluster
x1 = c(5, 0, -3),              # x1 coordinate of cluster center
x2 = c(-1, 1, -2)              # x2 coordinate of cluster center
)
labelled_points <- centers %>%
mutate(
x1 = map2(num_points, x1, rnorm),
x2 = map2(num_points, x2, rnorm)
) %>%
select(-num_points) %>%
unnest(x1, x2)
ggplot(labelled_points, aes(x1, x2, color = cluster)) +
geom_point()
points <- labelled_points %>%
select(-cluster)
kclust <- kmeans(points, centers = 3)
found_points <- labelled_points %>%
add_column(found_cluster = kclust$cluster) %>%
gather(key = group, value = cluster, -x1, -x2)
found_points
ggplot(found_points, aes(x1, x2, color = cluster)) +
geom_point() +
facet_grid(~group)
library(broom)
kclusts <- tibble(k = 1:9) %>%
mutate(
kclust = map(k, ~kmeans(points, .x)),
tidied = map(kclust, tidy),
glanced = map(kclust, glance),
augmented = map(kclust, augment, points)
)
kclusts
clusters <- kclusts %>%
unnest(tidied)
assignments <- kclusts %>%
unnest(augmented)
clusterings <- kclusts %>%
unnest(glanced, .drop = TRUE)
p1 <- ggplot(assignments, aes(x1, x2)) +
geom_point(aes(color = .cluster)) +
facet_wrap(~ k)
p1
p2 <- p1 + geom_point(data = clusters, size = 10, shape = "x")
p2
ggplot(clusterings, aes(k, tot.withinss)) +
geom_line()
load(file = "InterestPreferenceSurvey.Rda")
str(csb)
map_df(csb,mean)
kclusts <- tibble(k = 1:9) %>%
mutate(
kclust = map(k, ~kmeans(csb, .x)),
tidied = map(kclust, tidy),
glanced = map(kclust, glance),
augmented = map(kclust, augment, csb)
)
kclusts
clusters <- kclusts %>%
unnest(tidied)
assignments <- kclusts %>%
unnest(augmented)
clusterings <- kclusts %>%
unnest(glanced, .drop = TRUE)
ggplot(clusterings, aes(k, tot.withinss)) +
geom_line()
