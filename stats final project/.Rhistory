# choose optimal alpha manually
minlossplot(en.mod, cv.type="min")
alpha_opt <- en.mod$alpha[10] #0.729
lambda_opt <- en.mod$modlist[[10]]$lambda.min #0.005080836
alpha_opt
lambda_opt
en.pred <- predict(en.mod,alpha=alpha_opt, s=lambda_opt, newdata=test_norm,exact=TRUE)
RMSE(en.pred, test_norm$stars)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(caret)
library(glmnetUtils)
library(caret)
library(dplyr)
# import data
yelp <- read.csv('yelp_clean.csv') %>% na.omit()
dmy <- dummyVars(" ~ .", data = yelp, fullRank = T)
data_transformed <- data.frame(predict(dmy, newdata = yelp))
# Get train data and test data
train_data <- data_transformed [1:round(nrow(data_transformed)/2),]
test_data <- data_transformed [(round(nrow(data_transformed)/2)+1):nrow(data_transformed),]
fitControl <- trainControl(method = "cv", number = 5)
outcomeName<-'stars'
predictors<-names(data_transformed)[!names(data_transformed) %in% outcomeName]
# create normalization function and normalized dataset
normalize_data <- function(testdata, traindata){
n_testdata <- testdata
for (n in names(testdata)){
n_testdata[,n] <-
(testdata[,n] - min(traindata[,n])) /  (max(traindata[,n]) -  min(traindata[,n]))
}
return(n_testdata)
}
train_norm <- as.data.frame(normalize_data(
train_data %>% dplyr::select(-stars), train_data %>% dplyr::select(-stars)))
train_norm$stars <- train_data$stars
test_norm <- as.data.frame(normalize_data(
test_data %>% dplyr::select(-stars), train_data %>% dplyr::select(-stars)))
test_norm$stars <- test_data$stars
lm.mod <- lm(stars ~ ., train_norm)
lm.pred <- predict(lm.mod, test_norm)
RMSE(lm.pred, test_norm$stars)
set.seed(5103)
library(MASS)
step <- stepAIC(lm.mod,trace=FALSE)
step$anova
lm.mod_step <- lm(stars ~ city.Chandler + city.Gilbert + city.Mesa + city.Others + city.Peoria + city.Phoenix + city.Scottsdale + city.Surprise + city.Tempe + review_count.x + funny + useful + cool + review_count.y + yelp.votes.funny + yelp.votes.useful + yelp.votes.cool + cuisine.AfternoonTea + cuisine.American + cuisine.Breakfast + cuisine.British + cuisine.Cajun + cuisine.Chinese + cuisine.European + cuisine.Fast.Food + cuisine.French + cuisine.Fusion + cuisine.Italian + cuisine.Japanese + cuisine.Korean + cuisine.LatinAmerican + cuisine.Mediterranean + cuisine.Mexican + cuisine.MiddleEastern + cuisine.Nightlife + cuisine.Snacks + cuisine.SouthAsia + cuisine.Thai + cuisine.Vegan + cuisine.Vegetarian + cuisine.Vietnamese + wcount + qmark + emark, train_norm)
lm_step.pred <- predict(lm.mod_step, test_norm)
RMSE(lm_step.pred, test_norm$stars)
set.seed(5103)
lasso.cv <- cv.glmnet(stars ~ ., train_norm, alpha=1, use.model.frame=TRUE)
# optimal lambda
lasso.lam <- lasso.cv$lambda.min #0.004461396
lasso.mod <- glmnet(stars ~ ., train_norm, alpha=1, lambda=lasso.lam, use.model.frame=TRUE)
lasso.pred <- predict(lasso.mod, test_norm)
RMSE(lasso.pred, test_norm$stars)
set.seed(5103)
ridge.cv <- cv.glmnet(stars ~ ., train_norm, alpha=0, use.model.frame=TRUE)
# optimal lambda
ridge.lam <- ridge.cv$lambda.min #0.06178527
ridge.mod <- glmnet(stars ~ ., train_norm, alpha=0, lambda=ridge.lam, use.model.frame=TRUE)
ridge.pred <- predict(ridge.mod, test_norm)
RMSE(ridge.pred, test_norm$stars)
set.seed(5103)
# CV
en.mod <- cva.glmnet(stars ~ ., train_norm, use.model.frame=TRUE)
# choose optimal alpha manually
minlossplot(en.mod, cv.type="min")
alpha_opt <- en.mod$alpha[10] #0.729
lambda_opt <- en.mod$modlist[[10]]$lambda.min #0.004218199
# prediciton using optimal lambda
en.pred <- predict(en.mod,alpha=alpha_opt, s=lambda_opt, newdata=test_norm,exact=TRUE)
RMSE(en.pred, test_norm$stars)
minlossplot(en.mod, cv.type="min")
lasso.lam
ridge.lam
alpha_opt
lambda_opt
varImp(lm.pred)
varImp(lm.mod)
plot(varImp(lm.mod))
plot(varImp(lm.mod))
?varImp
varImp(lm.mod)
set.seed(5103)
library(MASS)
step <- stepAIC(lm.mod,trace=FALSE)
step$anova
lm.mod_step <- lm(stars ~ city.Mesa + city.Others + city.Scottsdale + city.Tempe +
review_count.x + review_count.y + yelp.votes.funny + yelp.votes.useful +
yelp.votes.cool + cuisine.American + cuisine.Breakfast +
cuisine.Chinese + cuisine.Fast.Food + cuisine.Fusion + cuisine.Italian +
cuisine.Japanese + cuisine.Korean + cuisine.Mexican + cuisine.Nightlife +
cuisine.Snacks + cuisine.SouthAsia + cuisine.Thai + cuisine.Vietnamese +
wcount + qmark + emark, train_norm)
lm_step.pred <- predict(lm.mod_step, test_norm)
RMSE(lm_step.pred, test_norm$stars)
set.seed(5103)
library(MASS)
step <- stepAIC(lm.mod,trace=FALSE)
step$anova
lm.mod_step <- lm(stars ~ city.Mesa + city.Others + city.Scottsdale + city.Tempe +
review_count.x + review_count.y + yelp.votes.funny + yelp.votes.useful +
yelp.votes.cool + cuisine.American + cuisine.Breakfast +
cuisine.Chinese + cuisine.Fast.Food + cuisine.Fusion + cuisine.Italian +
cuisine.Japanese + cuisine.Korean + cuisine.Mexican + cuisine.Nightlife +
cuisine.Snacks + cuisine.SouthAsia + cuisine.Thai + cuisine.Vietnamese +
wcount + qmark + emark, train_norm)
lm_step.pred <- predict(lm.mod_step, test_norm)
RMSE(lm_step.pred, test_norm$stars)
set.seed(5103)
lasso.cv <- cv.glmnet(stars ~ ., train_norm, alpha=1, use.model.frame=TRUE)
# optimal lambda
lasso.lam <- lasso.cv$lambda.min #0.003393452
lasso.mod <- glmnet(stars ~ ., train_norm, alpha=1, lambda=lasso.lam, use.model.frame=TRUE)
lasso.pred <- predict(lasso.mod, test_norm)
RMSE(lasso.pred, test_norm$stars)
set.seed(5103)
ridge.cv <- cv.glmnet(stars ~ ., train_norm, alpha=0, use.model.frame=TRUE)
# optimal lambda
ridge.lam <- ridge.cv$lambda.min #0.01853597
ridge.mod <- glmnet(stars ~ ., train_norm, alpha=0, lambda=ridge.lam, use.model.frame=TRUE)
ridge.pred <- predict(ridge.mod, test_norm)
RMSE(ridge.pred, test_norm$stars)
#1.118149
#Elastic Net
set.seed(5103)
set.seed(5103)
# CV
en.mod <- cva.glmnet(stars ~ ., train_norm, use.model.frame=TRUE)
# choose optimal alpha manually
minlossplot(en.mod, cv.type="min")
alpha_opt <- en.mod$alpha[10] #0.729
lambda_opt <- en.mod$modlist[[10]]$lambda.min #0.00465494
# prediciton using optimal lambda
en.pred <- predict(en.mod,alpha=alpha_opt, s=lambda_opt, newdata=test_norm,exact=TRUE)
RMSE(en.pred, test_norm$stars)
#1.116241
summary(varImp(lm.mod))
order(varImp(lm.mod))
summary(lm.mod)
lm.varimp <- as.data.frame(varImp(lm.mod))
View(lm.varimp)
?filter
lm.varimp <- as.data.frame(varImp(lm.mod)) %>% arrange(desc=TRUE)
?arrange
lm.varimp <- as.data.frame(varImp(lm.mod))
arrange(desc(lm.varimp$Overall))
arrange(lm.varimp,desc(lm.varimp$Overall))
lm.varimp <- as.data.frame(varImp(lm.mod)) %>% arrange(desc(Overall))
View(lm.varimp)
lm.varimp <- as.data.frame(varImp(lm.mod))
View(lm.varimp)
write.csv(lm.varimp,"lm.varimp.csv")
varImp(lm.mod)
knitr::opts_chunk$set(echo = TRUE)
####  preparation ######
library(tidyverse)
library(caret)
library(glmnetUtils)
library(caret)
library(dplyr)
# import data
yelp <- read.csv('yelp_clean.csv') %>% na.omit()
# onehot category variables
dmy <- dummyVars(" ~ .", data = yelp, fullRank = T)
data_transformed <- data.frame(predict(dmy, newdata = yelp))
# Get train data and test data
train_data <- data_transformed [1:round(nrow(data_transformed)/2),]
test_data <- data_transformed [(round(nrow(data_transformed)/2)+1):nrow(data_transformed),]
# set fixed parameter used in the following step
fitControl <- trainControl(method = "cv", number = 5)
outcomeName<-'stars'
predictors<-names(data_transformed)[!names(data_transformed) %in% outcomeName]
# create normalization function and normalized dataset
normalize_data <- function(testdata, traindata){
n_testdata <- testdata
for (n in names(testdata)){
n_testdata[,n] <-
(testdata[,n] - min(traindata[,n])) /  (max(traindata[,n]) -  min(traindata[,n]))
}
return(n_testdata)
}
train_norm <- as.data.frame(normalize_data(
train_data %>% dplyr::select(-stars), train_data %>% dplyr::select(-stars)))
train_norm$stars <- train_data$stars
test_norm <- as.data.frame(normalize_data(
test_data %>% dplyr::select(-stars), train_data %>% dplyr::select(-stars)))
test_norm$stars <- test_data$stars
###### baseline model ######
#linear regression
lm.mod <- lm(stars ~ ., train_norm)
lm.pred <- predict(lm.mod, test_norm)
lm.varimp <- as.data.frame(varImp(lm.mod)) %>% arrange(desc(Overall))
write.csv(lm.varimp,"lm.varimp.csv")
RMSE(lm.pred, test_norm$stars)
set.seed(5103)
library(MASS)
step <- stepAIC(lm.mod,trace=FALSE)
step$anova
lm.mod_step <- lm(stars ~ city.Mesa + city.Others + city.Scottsdale + city.Tempe +
review_count.x + review_count.y + yelp.votes.funny + yelp.votes.useful +
yelp.votes.cool + cuisine.American + cuisine.Breakfast +
cuisine.Chinese + cuisine.Fast.Food + cuisine.Fusion + cuisine.Italian +
cuisine.Japanese + cuisine.Korean + cuisine.Mexican + cuisine.Nightlife +
cuisine.Snacks + cuisine.SouthAsia + cuisine.Thai + cuisine.Vietnamese +
wcount + qmark + emark, train_norm)
lm_step.pred <- predict(lm.mod_step, test_norm)
RMSE(lm_step.pred, test_norm$stars)
set.seed(5103)
lasso.cv <- cv.glmnet(stars ~ ., train_norm, alpha=1, use.model.frame=TRUE)
# optimal lambda
lasso.lam <- lasso.cv$lambda.min #0.003393452
lasso.mod <- glmnet(stars ~ ., train_norm, alpha=1, lambda=lasso.lam, use.model.frame=TRUE)
lasso.pred <- predict(lasso.mod, test_norm)
RMSE(lasso.pred, test_norm$stars)
varImp(lasso.mod
varImp(lasso.mod)
varImp(lasso.mod)
lm.varimp <- as.data.frame(varImp(lasso.mod，lambda=lasso.lam))
set.seed(5103)
lasso.cv <- cv.glmnet(stars ~ ., train_norm, alpha=1, use.model.frame=TRUE)
# optimal lambda
lasso.lam <- lasso.cv$lambda.min #0.003393452
lasso.mod <- glmnet(stars ~ ., train_norm, alpha=1, lambda=lasso.lam, use.model.frame=TRUE)
lasso.pred <- predict(lasso.mod, test_norm)
RMSE(lasso.pred, test_norm$stars)
varImp(lasso.mod，lambda=lasso.lam)
lasso.varimp <- as.data.frame(varImp(lasso.mod，lambda=lasso.lam))
####  preparation ######
library(tidyverse)
library(caret)
library(glmnetUtils)
library(caret)
library(dplyr)
# import data
yelp <- read.csv('yelp_clean.csv') %>% na.omit()
# onehot category variables
dmy <- dummyVars(" ~ .", data = yelp, fullRank = T)
data_transformed <- data.frame(predict(dmy, newdata = yelp))
# Get train data and test data
train_data <- data_transformed [1:round(nrow(data_transformed)/2),]
test_data <- data_transformed [(round(nrow(data_transformed)/2)+1):nrow(data_transformed),]
# set fixed parameter used in the following step
fitControl <- trainControl(method = "cv", number = 5)
outcomeName<-'stars'
predictors<-names(data_transformed)[!names(data_transformed) %in% outcomeName]
# create normalization function and normalized dataset
normalize_data <- function(testdata, traindata){
n_testdata <- testdata
for (n in names(testdata)){
n_testdata[,n] <-
(testdata[,n] - min(traindata[,n])) /  (max(traindata[,n]) -  min(traindata[,n]))
}
return(n_testdata)
}
train_norm <- as.data.frame(normalize_data(
train_data %>% dplyr::select(-stars), train_data %>% dplyr::select(-stars)))
train_norm$stars <- train_data$stars
test_norm <- as.data.frame(normalize_data(
test_data %>% dplyr::select(-stars), train_data %>% dplyr::select(-stars)))
test_norm$stars <- test_data$stars
###### baseline model ######
#linear regression
lm.mod <- lm(stars ~ ., train_norm)
lm.pred <- predict(lm.mod, test_norm)
lm.varimp <- as.data.frame(varImp(lm.mod))
write.csv(lm.varimp,"lm.varimp.csv")
View(lm.varimp)
RMSE(lm.pred, test_norm$stars)
lm.mod_step <- lm(stars ~ city.Mesa + city.Others + city.Scottsdale + city.Tempe +
review_count.x + review_count.y + yelp.votes.funny + yelp.votes.useful +
yelp.votes.cool + cuisine.American + cuisine.Breakfast +
cuisine.Chinese + cuisine.Fast.Food + cuisine.Fusion + cuisine.Italian +
cuisine.Japanese + cuisine.Korean + cuisine.Mexican + cuisine.Nightlife +
cuisine.Snacks + cuisine.SouthAsia + cuisine.Thai + cuisine.Vietnamese +
wcount + qmark + emark, train_norm)
lm_step.pred <- predict(lm.mod_step, test_norm)
RMSE(lm_step.pred, test_norm$stars)
lm.varimp_step <- as.data.frame(varImp(lm.mod_step))
View(lm.varimp_step)
write.csv(lm.varimp_step,"lm.varimp_step.csv")
set.seed(5103)
lasso.cv <- cv.glmnet(stars ~ ., train_norm, alpha=1, use.model.frame=TRUE)
# optimal lambda
lasso.lam <- lasso.cv$lambda.min #0.003393452
lasso.mod <- glmnet(stars ~ ., train_norm, alpha=1, lambda=lasso.lam, use.model.frame=TRUE)
lasso.pred <- predict(lasso.mod, test_norm)
RMSE(lasso.pred, test_norm$stars)
lasso.varimp <- as.data.frame(varImp(lasso.mod，lambda=lasso.lam))
ridge.varimp <- as.data.frame(varImp(ridge.mod，lambda=ridge.lam))
set.seed(5103)
ridge.cv <- cv.glmnet(stars ~ ., train_norm, alpha=0, use.model.frame=TRUE)
# optimal lambda
ridge.lam <- ridge.cv$lambda.min #0.01853597
ridge.mod <- glmnet(stars ~ ., train_norm, alpha=0, lambda=ridge.lam, use.model.frame=TRUE)
ridge.pred <- predict(ridge.mod, test_norm)
RMSE(ridge.pred, test_norm$stars)
ridge.varimp <- as.data.frame(varImp(ridge.mod，lambda=ridge.lam))
lasso.varimp <- as.data.frame(varImp(lasso.mod))
?varImp
lasso.varimp <- as.data.frame(varImp(lasso.mod))
lasso.varimp <- as.data.frame(varImp(lasso.mod,lambda=lasso.lam))
View(train_norm)
lasso.varimp <- as.data.frame(varImp(lasso.mod,lambda = NULL))
lasso.varimp <- as.data.frame(varImp(lasso.cv,lambda = NULL))
lasso.varimp <- as.data.frame(varImp(lasso.mod,lambda = NULL))
lasso.varimp <- as.data.frame(varImp(lasso.mod,lambda = lasso.lam))
lasso.varimp <- as.data.frame(varImp(lasso.mod,s = lasso.lam))
lasso.varimp <- as.data.frame(varImp(lasso.mod,lambda = lasso.lam))
summary(lasso.mod)
####  preparation ######
library(tidyverse)
library(caret)
library(glmnetUtils)
library(caret)
library(dplyr)
# import data
yelp <- read.csv('yelp_clean.csv') %>% na.omit()
# onehot category variables
dmy <- dummyVars(" ~ .", data = yelp, fullRank = T)
data_transformed <- data.frame(predict(dmy, newdata = yelp))
# Get train data and test data
train_data <- data_transformed [1:round(nrow(data_transformed)/2),]
test_data <- data_transformed [(round(nrow(data_transformed)/2)+1):nrow(data_transformed),]
# set fixed parameter used in the following step
fitControl <- trainControl(method = "cv", number = 5)
outcomeName<-'stars'
predictors<-names(data_transformed)[!names(data_transformed) %in% outcomeName]
# create normalization function and normalized dataset
normalize_data <- function(testdata, traindata){
n_testdata <- testdata
for (n in names(testdata)){
n_testdata[,n] <-
(testdata[,n] - min(traindata[,n])) /  (max(traindata[,n]) -  min(traindata[,n]))
}
return(n_testdata)
}
train_norm <- as.data.frame(normalize_data(
train_data %>% dplyr::select(-stars), train_data %>% dplyr::select(-stars)))
train_norm$stars <- train_data$stars
test_norm <- as.data.frame(normalize_data(
test_data %>% dplyr::select(-stars), train_data %>% dplyr::select(-stars)))
test_norm$stars <- test_data$stars
lm.mod <- lm(stars ~ ., train_norm)
lm.pred <- predict(lm.mod, test_norm)
lm.varimp <- as.data.frame(varImp(lm.mod))
RMSE(lm.pred, test_norm$stars)
set.seed(5103)
library(MASS)
step <- stepAIC(lm.mod,trace=FALSE)
step$anova
lm.mod_step <- lm(stars ~ city.Mesa + city.Others + city.Scottsdale + city.Tempe +
review_count.x + review_count.y + yelp.votes.funny + yelp.votes.useful +
yelp.votes.cool + cuisine.American + cuisine.Breakfast +
cuisine.Chinese + cuisine.Fast.Food + cuisine.Fusion + cuisine.Italian +
cuisine.Japanese + cuisine.Korean + cuisine.Mexican + cuisine.Nightlife +
cuisine.Snacks + cuisine.SouthAsia + cuisine.Thai + cuisine.Vietnamese +
wcount + qmark + emark, train_norm)
lm.varimp_step <- as.data.frame(varImp(lm.mod_step))
lm_step.pred <- predict(lm.mod_step, test_norm)
RMSE(lm_step.pred, test_norm$stars)
set.seed(5103)
lasso.cv <- cv.glmnet(stars ~ ., train_norm, alpha=1, use.model.frame=TRUE)
# optimal lambda
lasso.lam <- lasso.cv$lambda.min #0.003393452
lasso.mod <- glmnet(stars ~ ., train_norm, alpha=1, lambda=lasso.lam, use.model.frame=TRUE)
lasso.pred <- predict(lasso.mod, test_norm)
RMSE(lasso.pred, test_norm$stars)
varImp(lasso.mod, lambda = lasso.lam)
summary(lasso.mod)
coef(lasso.mod,s=lasso.lam)
coef_lasso <- as.data.frame(coef(lasso.mod,s=lasso.lam))
coef(lasso.mod,s=lasso.lam)
set.seed(5103)
ridge.cv <- cv.glmnet(stars ~ ., train_norm, alpha=0, use.model.frame=TRUE)
# optimal lambda
ridge.lam <- ridge.cv$lambda.min #0.01853597
ridge.mod <- glmnet(stars ~ ., train_norm, alpha=0, lambda=ridge.lam, use.model.frame=TRUE)
ridge.pred <- predict(ridge.mod, test_norm)
RMSE(ridge.pred, test_norm$stars)
coef(ridge.mod,s=ridge.lam)
coef(en.mod,alpha=alpha_opt,s=lambda_opt)
en.mod <- cva.glmnet(stars ~ ., train_norm, use.model.frame=TRUE)
# choose optimal alpha manually
minlossplot(en.mod, cv.type="min")
alpha_opt <- en.mod$alpha[10] #0.729
lambda_opt <- en.mod$modlist[[10]]$lambda.min #0.00465494
# prediciton using optimal lambda
en.pred <- predict(en.mod,alpha=alpha_opt, s=lambda_opt, newdata=test_norm,exact=TRUE)
RMSE(en.pred, test_norm$stars)
coef(en.mod,alpha=alpha_opt,s=lambda_opt)
lm.mod <- lm(stars ~ ., train_norm)
lm.pred <- predict(lm.mod, test_norm)
lm.varimp <- as.data.frame(varImp(lm.mod))
RMSE(lm.pred, test_norm$stars)
summary(lm.mod)
summary(lm.mod_step)
####  preparation ######
library(tidyverse)
library(caret)
library(glmnetUtils)
library(caret)
library(dplyr)
# import data
yelp <- read.csv('yelp_clean.csv') %>% na.omit()
# onehot category variables
dmy <- dummyVars(" ~ .", data = yelp, fullRank = T)
data_transformed <- data.frame(predict(dmy, newdata = yelp))
# Get train data and test data
train_data <- data_transformed [1:round(nrow(data_transformed)/2),]
test_data <- data_transformed [(round(nrow(data_transformed)/2)+1):nrow(data_transformed),]
# set fixed parameter used in the following step
fitControl <- trainControl(method = "cv", number = 5)
outcomeName<-'stars'
predictors<-names(data_transformed)[!names(data_transformed) %in% outcomeName]
# create normalization function and normalized dataset
normalize_data <- function(testdata, traindata){
n_testdata <- testdata
for (n in names(testdata)){
n_testdata[,n] <-
(testdata[,n] - min(traindata[,n])) /  (max(traindata[,n]) -  min(traindata[,n]))
}
return(n_testdata)
}
train_norm <- as.data.frame(normalize_data(
train_data %>% dplyr::select(-stars), train_data %>% dplyr::select(-stars)))
train_norm$stars <- train_data$stars
test_norm <- as.data.frame(normalize_data(
test_data %>% dplyr::select(-stars), train_data %>% dplyr::select(-stars)))
test_norm$stars <- test_data$stars
###### baseline model ######
#linear regression
lm.mod <- lm(stars ~ ., train_norm)
lm.pred <- predict(lm.mod, test_norm)
lm.varimp <- as.data.frame(varImp(lm.mod))
RMSE(lm.pred, test_norm$stars)
summary(lm.mod)
#write.csv(lm.varimp,"lm.varimp.csv")
#1.117211
#step AIC
set.seed(5103)
library(MASS)
step <- stepAIC(lm.mod,trace=FALSE)
step$anova
lm.mod_step <- lm(stars ~ city.Mesa + city.Others + city.Scottsdale + city.Tempe +
review_count.x + review_count.y + yelp.votes.funny + yelp.votes.useful +
yelp.votes.cool + cuisine.American + cuisine.Breakfast +
cuisine.Chinese + cuisine.Fast.Food + cuisine.Fusion + cuisine.Italian +
cuisine.Japanese + cuisine.Korean + cuisine.Mexican + cuisine.Nightlife +
cuisine.Snacks + cuisine.SouthAsia + cuisine.Thai + cuisine.Vietnamese +
wcount + qmark + emark, train_norm)
lm.varimp_step <- as.data.frame(varImp(lm.mod_step))
lm_step.pred <- predict(lm.mod_step, test_norm)
RMSE(lm_step.pred, test_norm$stars)
summary(lm.mod_step)
#1.11369
#write.csv(lm.varimp_step,"lm.varimp_step.csv")
# glmnet
#lasso
set.seed(5103)
lasso.cv <- cv.glmnet(stars ~ ., train_norm, alpha=1, use.model.frame=TRUE)
# optimal lambda
lasso.lam <- lasso.cv$lambda.min #0.003393452
lasso.mod <- glmnet(stars ~ ., train_norm, alpha=1, lambda=lasso.lam, use.model.frame=TRUE)
lasso.pred <- predict(lasso.mod, test_norm)
RMSE(lasso.pred, test_norm$stars)
coef(lasso.mod,s=lasso.lam)
# 1.116176
#ridge
set.seed(5103)
ridge.cv <- cv.glmnet(stars ~ ., train_norm, alpha=0, use.model.frame=TRUE)
# optimal lambda
ridge.lam <- ridge.cv$lambda.min #0.01853597
ridge.mod <- glmnet(stars ~ ., train_norm, alpha=0, lambda=ridge.lam, use.model.frame=TRUE)
ridge.pred <- predict(ridge.mod, test_norm)
RMSE(ridge.pred, test_norm$stars)
coef(ridge.mod,s=ridge.lam)
#1.118149
#ridge
set.seed(5103)
ridge.cv <- cv.glmnet(stars ~ ., train_norm, alpha=0, use.model.frame=TRUE)
# optimal lambda
ridge.lam <- ridge.cv$lambda.min #0.01853597
ridge.mod <- glmnet(stars ~ ., train_norm, alpha=0, lambda=ridge.lam, use.model.frame=TRUE)
ridge.pred <- predict(ridge.mod, test_norm)
RMSE(ridge.pred, test_norm$stars)
coef(ridge.mod,s=ridge.lam)
#1.118149
#Elastic Net
set.seed(5103)
# CV
en.mod <- cva.glmnet(stars ~ ., train_norm, use.model.frame=TRUE)
# choose optimal alpha manually
minlossplot(en.mod, cv.type="min")
alpha_opt <- en.mod$alpha[10] #0.729
lambda_opt <- en.mod$modlist[[10]]$lambda.min #0.00465494
# prediciton using optimal lambda
en.pred <- predict(en.mod,alpha=alpha_opt, s=lambda_opt, newdata=test_norm,exact=TRUE)
RMSE(en.pred, test_norm$stars)
coef(en.mod,alpha=alpha_opt,s=lambda_opt)
#1.116241
?stepAIC
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(caret)
library(glmnetUtils)
library(caret)
library(dplyr)
set.seed(5103)
library(MASS)
?stepAIC
