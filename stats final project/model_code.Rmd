---
title: "new model"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


Baseline model 
```{r message=FALSE, warning=FALSE}
####  preparation ###### 
library(tidyverse)
library(caret)
library(adabag)
library(glmnetUtils)

# import data
whole_data <- read.csv('clean_yelp.csv')

# onehot category variables   
dmy <- dummyVars(" ~ .", data = whole_data, fullRank = T)
data_transformed <- data.frame(predict(dmy, newdata = whole_data))

# Get train data and test data 
set.seed(5152)
train_id <- sample(1:nrow(data_transformed), nrow(data_transformed)/10)
train_data <- data_transformed[train_id,]
after_train_data <- data_transformed[-train_id,]
set.seed(5152)
test_id <- sample(1:nrow(after_train_data), nrow(after_train_data)/10)
test_data <- after_train_data[test_id,]

# set fixed parameter used in the following step
fitControl <- trainControl(method = "cv", number = 5)
outcomeName<-'stars.x'
predictors<-names(data_transformed)[!names(data_transformed) %in% outcomeName]

###### baseline model ######

# rpart
#tune_rpart <- expand.grid(.cp=1:12)
tune_rpart <- expand.grid(.cp=12)
model_rpart <- train(train_data[,predictors], train_data[,outcomeName], method = 'rpart',tuneGrid = tune_rpart )
pred_rpart <- predict(model_rpart, test_data[,predictors], type = 'raw')
RMSE(pred_rpart, test_data$stars.x)

# glmnet
set.seed(5152)
lasso.cv <- cv.glmnet(stars.x ~ ., train_data, alpha=1, use.model.frame=TRUE)
# optimal lambda
lasso.lam <- lasso.cv$lambda.min
lasso <- glmnet(stars.x ~ ., train_data, alpha=1, lambda=lasso.lam, use.model.frame=TRUE)
pred_lasso <- predict(lasso, test_data)
RMSE(pred_lasso, test_data$stars.x)


#KNN
#ks <- c( 3, 5, 7, 9, 11, 15)
#rmse <- numeric(length=length(ks))
#pred <- numeric(length=length(ks))
#for (i in seq(along=ks)) {
#    model <- knnreg(train_data[,predictors], train_data[,outcomeName], k=ks[i])
#    pred[i] <- predict(model, train_data[,predictors])
#   rmse[i] <- RMSE(pred[i], train_data$stars.x)
#}
#opt_k <- ks[which.min(rmse)]
opt_k = 15
model_knn <- knnreg(train_data[,predictors], train_data[,outcomeName], k=opt_k)
pred_knn<- predict(model_knn, test_data[,predictors])
RMSE(pred_knn, test_data$stars.x)

#Random Forest
#tune_rf <- expand.grid(.mtry = 10:25)
tune_rf <- expand.grid(.mtry = 16)
model_rf <- train( train_data[,predictors], train_data[,outcomeName], method = 'rf', tuneGrid = tune_rf, trControl=fitControl )
pred_rf <- predict(model_rf, test_data, type = 'raw')
RMSE(pred_rf, test_data$stars.x)

#neuralnet
#tune_nn <- expand.grid(.layer1=1:5, .layer2=1:5, .layer3=1:5)
tune_nn <- expand.grid(.layer1=1, .layer2=256, .layer3=0)
model_nn <- train( train_data[,predictors], train_data[,outcomeName], method = 'neuralnet', threshold = 0.03, stepmax = 1e+05, tuneGrid = tune_nn, trControl=fitControl )
pred_nn <- predict(model_nn, test_data, type = 'raw')
RMSE(pred_nn, test_data$stars.x)

#svm
train_data_svm <- train_data %>% select(-state.SC,-state.CO) # delete constant variables
test_data_svm <- test_data %>% select(-state.SC, -state.CO)
outcomeName_svm<-'stars.x'
predictor_svm <-  names(train_data_svm)[!names(train_data_svm) %in% outcomeName_svm]
#tune_svm <- expand.grid(.C=c(0.1,1,3,5))
tune_svm <- expand.grid(.C=c(1))
model_svm <- train( train_data_svm[,predictor_svm], train_data_svm[,outcomeName_svm], method = 'svmLinear', tuneGrid = tune_svm, trControl=fitControl)
pred_svm <- predict(model_svm, test_data_svm[,predictor_svm], type = 'raw')
RMSE(pred_svm, test_data_svm$stars.x)
varImp(model_svm)


# xgboost
set.seed(5152)
#tune_xgb <- expand.grid(.nrounds = c(20,50,60,100), .max_depth = c(3,4), .eta = c(0.1,0.2,0.25), .gamma = c(0.1,0.05), .colsample_bytree =c(0.6,0.8,1), .min_child_weight = c(3,4,5), .subsample = c(0.6,0.8,1))
tune_xgb <- expand.grid(.nrounds = c(50), .max_depth = c(4), .eta = c(0.2), .gamma = c(0.1), .colsample_bytree =c(0.8), .min_child_weight = c(4), .subsample = c(0.8))
model_xgb <- train( train_data[,predictors], train_data[,outcomeName], method = 'xgbTree', tuneGrid = tune_xgb, trControl=fitControl)
pred_xgb <- predict(model_xgb, test_data, type = 'raw')
RMSE(pred_xgb, test_data$stars.x)
varImp(model_xgb)


###### stacking ########

library(Metrics)

pred_knn<-as.data.frame(pred_knn)
pred_rf<-as.data.frame(pred_rf)
pred_rpart<-as.data.frame(pred_rpart)
pred_xgb<-as.data.frame(pred_xgb)
pred_svm<-as.data.frame(pred_svm)
C<-cbind(pred_xgb,pred_knn,pred_rf,pred_rpart, pred_svm)
C<-cbind(C,test_data$stars.x)
colnames(C)[6]<-"y"
lm1<-lm(y~.,data=C)
lm1_pred<-predict(lm1,C[-6])
lm1_pred<-as.data.frame(lm1_pred)
C<-cbind(C,lm1_pred)
rmse(C$y,C$lm1_pred)

```


Feature engineering and new model 
```{r message=FALSE, warning=FALSE}

##### Feature Engineering ######
# create new features and redo the preparation 
whole_data$score_emark<-whole_data$scores*whole_data$emark
whole_data$score_qmark<-whole_data$scores*whole_data$qmark
whole_data$senti_by_word<-whole_data$scores/whole_data$wcount

# onehot category variables   
dmy <- dummyVars(" ~ .", data = whole_data, fullRank = T)
data_transformed <- data.frame(predict(dmy, newdata = whole_data))

# Get train data and test data 
set.seed(5152)
train_id <- sample(1:nrow(data_transformed), nrow(data_transformed)/10)
train_data <- data_transformed[train_id,]
after_train_data <- data_transformed[-train_id,]
set.seed(5152)
test_id <- sample(1:nrow(after_train_data), nrow(after_train_data)/10)
test_data <- after_train_data[test_id,]

# set fixed parameter used in the following step
fitControl <- trainControl(method = "cv", number = 5)
outcomeName<-'stars.x'
predictors<-names(data_transformed)[!names(data_transformed) %in% outcomeName]


###### new model after feature engineering  ######

# Rpart could not find a good way to split the features in baseline model session, which is the reason why pred_rpart get the same value, so we didn't include rpart in this session. 


#KNN
#ks <- c( 3, 5, 7, 9, 11, 15)
#rmse <- numeric(length=length(ks))
#pred <- numeric(length=length(ks))
#for (i in seq(along=ks)) {
#    model <- knnreg(train_data[,predictors], train_data[,outcomeName], k=ks[i])
#    pred[i] <- predict(model, train_data[,predictors])
#   rmse[i] <- RMSE(pred[i], train_data$stars.x)
#}
#opt_k <- ks[which.min(rmse)]
opt_k=15
model_knn_new <- knnreg(train_data[,predictors], train_data[,outcomeName], k=opt_k)
pred_knn_new<- predict(model_knn_new, test_data[,predictors])
RMSE(pred_knn_new, test_data$stars.x)

# glmnet
set.seed(5152)
lasso.cv <- cv.glmnet(stars.x ~ ., train_data, alpha=1, use.model.frame=TRUE)
# optimal lambda
lasso.lam <- lasso.cv$lambda.min
lasso_new <- glmnet(stars.x ~ ., train_data, alpha=1, lambda=lasso.lam, use.model.frame=TRUE)
pred_lasso_new <- predict(lasso_new, test_data)
RMSE(pred_lasso_new, test_data$stars.x)


#Random Forest
#tune_rf <- expand.grid(.mtry = 10:25)
tune_rf <- expand.grid(.mtry = 16)
model_rf_new <- train( train_data[,predictors], train_data[,outcomeName], method = 'rf', tuneGrid = tune_rf, trControl=fitControl)
pred_rf_new <- predict(model_rf_new, test_data, type = 'raw')
RMSE(pred_rf_new, test_data$stars.x)

#neuralnet
#tune_nn <- expand.grid(.layer1=1:5, .layer2=1:5, .layer3=1:5)
tune_nn <- expand.grid(.layer1=1, .layer2=256, .layer3=0)
model_nn_new <- train( train_data[,predictors], train_data[,outcomeName], method = 'neuralnet', threshold = 0.03, stepmax = 1e+05, tuneGrid = tune_nn, trControl=fitControl )
pred_nn_new <- predict(model_nn_new, test_data[,predictors], type = 'raw')
RMSE(pred_nn_new, test_data$stars.x)

#svm
train_data_svm_new <- train_data %>% select(-state.SC,-state.CO) # delete constant variables
test_data_svm_new <- test_data %>% select(-state.SC, -state.CO)
outcomeName_svm_new<-'stars.x'
predictor_svm_new <-  names(train_data_svm_new)[!names(train_data_svm_new) %in% outcomeName_svm_new]
#tune_svm <- expand.grid(.C=c(0.1,1,3,5))
tune_svm <- expand.grid(.C=c(1))
model_svm_new <- train( train_data_svm_new[,predictor_svm_new], train_data_svm_new[,outcomeName_svm_new], method = 'svmLinear', tuneGrid = tune_svm, trControl=fitControl)
pred_svm_new <- predict(model_svm_new, test_data_svm_new[,predictor_svm_new], type = 'raw')
RMSE(pred_svm_new, test_data_svm_new$stars.x)
varImp(model_svm_new)


# xgboost
set.seed(5152)
#tune_xgb <- expand.grid(.nrounds = c(20,50,60,100), .max_depth = c(3,4), .eta = c(0.1,0.2,0.25), .gamma = c(0.1,0.05), .colsample_bytree =c(0.6,0.8,1), .min_child_weight = c(3,4,5), .subsample = c(0.6,0.8,1))
tune_xgb <- expand.grid(.nrounds = c(50), .max_depth = c(4), .eta = c(0.2), .gamma = c(0.1), .colsample_bytree =c(0.8), .min_child_weight = c(4), .subsample = c(0.8))
model_xgb_new <- train( train_data[,predictors], train_data[,outcomeName], method = 'xgbTree', tuneGrid = tune_xgb, trControl=fitControl)
pred_xgb_new <- predict(model_xgb_new, test_data, type = 'raw')
RMSE(pred_xgb_new, test_data$stars.x)
varImp(model_xgb_new)


######  Stacking  #####

pred_rf_new<-as.data.frame(pred_rf_new)
pred_xgb_new<-as.data.frame(pred_xgb_new)
pred_svm_new<-as.data.frame(pred_svm_new)
C1<-cbind(pred_svm_new,pred_rf_new,pred_xgb_new,test_data$stars.x)
colnames(C1)[4]<-"y"
lm2<-lm(y~.,data=C1)
lm2_pred<-predict(lm2,C1[-4])
lm2_pred<-as.data.frame(lm2_pred)
C1<-cbind(C1,lm2_pred)
rmse(C1$y,C1$lm2_pred) 

```

