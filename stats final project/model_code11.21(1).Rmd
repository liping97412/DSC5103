---
title: "new model"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Data Preprocessing

```{r message=FALSE, warning=FALSE}
####  preparation ###### 
library(tidyverse)
library(caret)
library(glmnetUtils)
library(caret)
library(dplyr)

# import data
yelp <- read.csv('yelp_clean.csv') %>% na.omit()
##yelp <- yelp %>% dplyr::select(-average_stars)
##write.csv(yelp, 'yelp_clean.csv', row.names = FALSE)


# onehot category variables   
dmy <- dummyVars(" ~ .", data = yelp, fullRank = T)
data_transformed <- data.frame(predict(dmy, newdata = yelp))

# Get train data and test data 
train_data <- data_transformed [1:round(nrow(data_transformed)/2),]
test_data <- data_transformed [(round(nrow(data_transformed)/2)+1):nrow(data_transformed),]

# set fixed parameter used in the following step
fitControl <- trainControl(method = "cv", number = 5)
outcomeName<-'stars'
predictors<-names(data_transformed)[!names(data_transformed) %in% outcomeName]

# create normalization function and normalized dataset
normalize_data <- function(testdata, traindata){
  n_testdata <- testdata 
  for (n in names(testdata)){
    n_testdata[,n] <-  
      (testdata[,n] - min(traindata[,n])) /  (max(traindata[,n]) -  min(traindata[,n]))
  } 
  return(n_testdata)
}

train_norm <- as.data.frame(normalize_data(
  train_data %>% dplyr::select(-stars), train_data %>% dplyr::select(-stars)))
train_norm$stars <- train_data$stars

test_norm <- as.data.frame(normalize_data(
  test_data %>% dplyr::select(-stars), train_data %>% dplyr::select(-stars)))
test_norm$stars <- test_data$stars
```


## Baseline Model

1.linear regression

```{r message=FALSE, warning=FALSE}
###### baseline model ######

#linear regression
lm.mod <- lm(stars ~ ., train_norm)
lm.pred <- predict(lm.mod, test_norm)
RMSE(lm.pred, test_norm$stars)
summary(lm.mod)
#1.117211
#varImp(lm.mod)
```
-First we construct a linear regression model using all the variables in the dataset, from the result above, we can see that the coefficients of $review_count.x$,$yelp.votes.cool$,$emark$ are positive and significant, which means these features have a postive impact on ratings. The coefficients of $yelp.votes.funny$,$yelp.votes.useful$,$wcount$,$qmark$ are negative and significant, which means these features have a negative impact on ratings. And we get a RMSE of 1.117211 in this model.


2.stepAIC

```{r message=FALSE, warning=FALSE}
#step AIC
set.seed(5103)
library(MASS)
step <- stepAIC(lm.mod,trace=FALSE)
step$anova
lm.mod_step <- lm(stars ~ city.Mesa + city.Others + city.Scottsdale + city.Tempe + 
    review_count.x + review_count.y + yelp.votes.funny + yelp.votes.useful + 
    yelp.votes.cool + cuisine.American + cuisine.Breakfast + 
    cuisine.Chinese + cuisine.Fast.Food + cuisine.Fusion + cuisine.Italian + 
    cuisine.Japanese + cuisine.Korean + cuisine.Mexican + cuisine.Nightlife + 
    cuisine.Snacks + cuisine.SouthAsia + cuisine.Thai + cuisine.Vietnamese + 
    wcount + qmark + emark, train_norm)
lm_step.pred <- predict(lm.mod_step, test_norm)
RMSE(lm_step.pred, test_norm$stars)
summary(lm.mod_step)
#1.11369
#varImp(lm.mod_step)
#write.csv(lm.varimp_step,"lm.varimp_step.csv")
```
-To improve the performance of linear model, we use stepAIC method to select some of the variables in the dataset as independent variables, then we got the final formula as shown above. We get a RMSE of 1.11369 in this model. According to the result of this model, most of the coefficients are significant here. We will focus on the variables that have larger abusolute value. And we will explain these variables one by one.

  -$yelp.votes.funny$, the coefficient is significant and negative, which means if a review    is voted as funny, it always the case that the content of the review is to make jokes       about the restaurant, so the overall sentiment should be negative, such as the food         tastes like shit.
  -$yelp.votes.useful$,the coefficient is significant and negative, which means if a review    is useful to other customers, the review should be a negative review that remind other      customers not to come to this restaurant again
  -$yelp.votes.cool$,the coefficient is significant and positive, which means if a review is    voted as cool, this review is more likely to be a positive review.
  -$emark$, the coefficient is significant and positive, which means the more exclamation      mark in a review, the higher the rating tend to be. 
  -$qmark$, the coefficient is significant and negative, which means the more question mark    in a review, the more likely it is a negative review. 
  -$wcount$, the coefficient is significant and negative, which means nowadays people tend     to write more words in a negative review to criticize the detail of a restaurant.


3.glmnet-lasso

```{r message=FALSE, warning=FALSE}
# glmnet
#lasso
set.seed(5103)
lasso.cv <- cv.glmnet(stars ~ ., train_norm, alpha=1, use.model.frame=TRUE)
# optimal lambda
lasso.lam <- lasso.cv$lambda.min #0.003393452
lasso.mod <- glmnet(stars ~ ., train_norm, alpha=1, lambda=lasso.lam, use.model.frame=TRUE)
lasso.pred <- predict(lasso.mod, test_norm)
RMSE(lasso.pred, test_norm$stars)
coef(lasso.mod,s=lasso.lam)
# 1.116176
```

4.glmnet-ridge

```{r message=FALSE, warning=FALSE}
#ridge
set.seed(5103)
ridge.cv <- cv.glmnet(stars ~ ., train_norm, alpha=0, use.model.frame=TRUE)
# optimal lambda
ridge.lam <- ridge.cv$lambda.min #0.01853597
ridge.mod <- glmnet(stars ~ ., train_norm, alpha=0, lambda=ridge.lam, use.model.frame=TRUE)
ridge.pred <- predict(ridge.mod, test_norm)
RMSE(ridge.pred, test_norm$stars)
coef(ridge.mod,s=ridge.lam)
#1.118149
```

5.glmnet-elastic net

```{r message=FALSE, warning=FALSE}
#Elastic Net
set.seed(5103)
# CV
en.mod <- cva.glmnet(stars ~ ., train_norm, use.model.frame=TRUE)
# choose optimal alpha manually
minlossplot(en.mod, cv.type="min")
alpha_opt <- en.mod$alpha[10] #0.729
lambda_opt <- en.mod$modlist[[10]]$lambda.min #0.00465494
# prediciton using optimal lambda
en.pred <- predict(en.mod,alpha=alpha_opt, s=lambda_opt, newdata=test_norm,exact=TRUE)
RMSE(en.pred, test_norm$stars)
coef(en.mod,alpha=alpha_opt,s=lambda_opt)
#1.116241
```
-Then, we fit three glmnet models. The most important variables are all the same with the linear models above.The best models of lasso, ridge and elastic net give us the RMSE of 1.116176,1.118149 and 1.116241 respectively.



```{r message=FALSE, warning=FALSE}
#KNN
#stratified CV for training
library("FNN")
folds <- 5
cvIndex <- createMultiFolds(train_norm$stars, folds)

#control for cv - SAME FOR ALL MODELS
ctrl_model <- trainControl(index = cvIndex,
                          method="cv",
                          number=folds
                          )
#knn_grid = expand.grid(.k=c(10:20))
knn_grid = expand.grid(.k=c(15))
set.seed(5103)
knn.mod <- train(stars ~., 
                 data=train_norm,
                 method = "knn",
                 trControl=ctrl_model,
                 tuneGrid=knn_grid,
                 metric = "RMSE"
)

knn.pred<- predict(knn.mod, test_norm[,predictors])
RMSE(knn.pred, test_data$stars)
#1.15793
knnimp <- as.data.frame(varImp(knn.mod)$importance)
write.csv(knnimp, 'knnimp.csv')
```


## Ensamble Methods

```{r message=FALSE, warning=FALSE}


#Random Forest
# tune_rf <- expand.grid(.mtry = 10:25)
set.seed(5103)
tune_rf <- expand.grid(.mtry = 16)
rf.mod <- train( train_norm[,predictors], train_norm[,outcomeName], method = 'rf', tuneGrid = tune_rf, trControl=fitControl, metric = "RMSE",importance=T )
rf.pred <- predict(rf.mod, test_norm, type = 'raw')
RMSE(rf.pred, test_data$stars)
#1.10221
rfimp <- as.data.frame(varImp(rf.mod)$importance)
write.csv(rfimp, 'rfimp.csv')

# xgboost
set.seed(5103)
#tune_xgb <- expand.grid(.nrounds = c(20,50,60,100), .max_depth = c(3,4), .eta = c(0.1,0.2,0.25), .gamma = c(0.1,0.05), .colsample_bytree =c(0.6,0.8,1), .min_child_weight = c(3,4,5), .subsample = c(0.6,0.8,1))
tune_xgb <- expand.grid(.nrounds = c(50), .max_depth = c(4), .eta = c(0.2), .gamma = c(0.1), .colsample_bytree =c(0.8), .min_child_weight = c(4), .subsample = c(0.8))
xgb.mod <- train( train_norm[,predictors], train_norm[,outcomeName], method = 'xgbTree', tuneGrid = tune_xgb, trControl=fitControl, metric = "RMSE")
xgb.pred <- predict(xgb.mod, test_norm, type = 'raw')
RMSE(xgb.pred, test_data$stars)
varImp(xgb.mod)
# 1.090363
xgbimp <- as.data.frame(varImp(xgb.mod)$importance)
write.csv(xgbimp, 'xgbimp.csv')

# partial plot
library("pdp")

## emark
plotPartial(partial(rf.mod, train=train_norm, pred.var = "emark", chull = TRUE))
plotPartial(partial(xgb.mod, train=train_norm, pred.var = "emark", chull = TRUE))

## qmark
plotPartial(partial(rf.mod, train=train_norm, pred.var = "qmark", chull = TRUE))
plotPartial(partial(xgb.mod, train=train_norm, pred.var = "qmark", chull = TRUE))

## yelp.votes.useful
plotPartial(partial(rf.mod, train=train_norm, pred.var = "yelp.votes.useful", chull = TRUE))
plotPartial(partial(xgb.mod, train=train_norm, pred.var = "yelp.votes.useful", chull = TRUE))

## yelp.votes.funny
plotPartial(partial(rf.mod, train=train_norm, pred.var = "yelp.votes.funny", chull = TRUE))
plotPartial(partial(xgb.mod, train=train_norm, pred.var = "yelp.votes.funny", chull = TRUE))

## yelp.votes.cool
plotPartial(partial(rf.mod, train=train_norm, pred.var = "yelp.votes.cool", chull = TRUE))
plotPartial(partial(xgb.mod, train=train_norm, pred.var = "yelp.votes.cool", chull = TRUE))

## wcount
plotPartial(partial(rf.mod, train=train_norm, pred.var = "wcount", chull = TRUE))
plotPartial(partial(xgb.mod, train=train_norm, pred.var = "wcount", chull = TRUE))



```

1. Model performance:
The RMSE of xgboost is 1.09, slightly beats random forest and knn in RMSE.
2. Variable Interpretation:
    In both randomforest and xgboost, one or two emarks (exclamation mark) increases the stars while more exclamation marks may indicate a strong sense of anger or unsatisfaction.
    Both models suggests that question marks are inversely related to stars. Number of stars drops significantly with one or two question marks.
    We also found that "yelp.votes.useful" and "yelp.votes.funny" are negatively related to number of.The reason, which is consistent with the results of the linear models above. But from partial plot, we can see that the number of stars drop sigificantly with very few votes.
    In addition, the number of stars is generally decreasing with word count - the longer the review, the lower the rating. So we can conclude that review with larger word counts are more likely to be complaints. 
    

### Stacking

```{r message=FALSE, warning=FALSE}
###### stacking ###################################################################3

# see model correlations
#model_list <- list(
#  knn = knn.mod,
#  rf = rf.mod,
#  xgb = xgb.mod,
#  lm = lm.mod,
#  step = lm.mod_step,
#  ridge = ridge.mod,
#  lasso = lasso.mod,
#  elastic = en.mod
#)

#results <- resamples(model_list)
#modelCor(results)


# 1.111083

```

We use the baseline and ensamble models with top performance as our level 0. They include randomForest, xgboost, step_AIC and elastic net linear regression. Besides, we split the test data into halfs-- one part is used to train level 1 meta-learner, the other part is for testing. We choose Lasso regression as meta-learner since it it simplier with high accuracy. It turns out that the RMSE is 1.11 after stacking, slightly worse than randomForest and xgboost alone, which may because of the high correlation among level 1 model predictions. We have also tried to use xgboost as meta-learner but it gives even worse RMSE than lasso regression.
Overall,stacking gives poor performance might be because of the following two reasons:
1.There are a lot of overlap exists in the predictions of the ensembled models relative to the base models. 
2.The level of noise is related to the size of the sample. If your sample is small then the noise can have a bigger effect. It would not be too surprising for the test set to occasionally do better than the dev. If the sample is big then the performance should be similar on both.



## Feature Engineering

```{r message=FALSE, warning=FALSE}

## Data Preprocessing

# variable selection
 data_transformed <-  data_transformed %>% dplyr::select("stars","yelp.votes.useful", "yelp.votes.funny","yelp.votes.cool","wcount","useful","review_count.y","review_count.x","qmark","funny","emark","cuisine.Vegetarian","cuisine.Mexican","cuisine.Fast.Food","cuisine.Chinese","cuisine.Breakfast","cuisine.AfternoonTea","cool")

# Get train data and test data 
train_data <- data_transformed [1:round(nrow(data_transformed)/2),]
test_data <- data_transformed [(round(nrow(data_transformed)/2)+1):nrow(data_transformed),]

# set fixed parameter used in the following step
fitControl <- trainControl(method = "cv", number = 5)
outcomeName<-'stars'
predictors<-names(data_transformed)[!names(data_transformed) %in% outcomeName]

# create normalization function and normalized dataset
normalize_data <- function(testdata, traindata){
  n_testdata <- testdata 
  for (n in names(testdata)){
    n_testdata[,n] <-  
      (testdata[,n] - min(traindata[,n])) /  (max(traindata[,n]) -  min(traindata[,n]))
  } 
  return(n_testdata)
}

train_norm <- as.data.frame(normalize_data(
  train_data %>% dplyr::select(-stars), train_data %>% dplyr::select(-stars)))
train_norm$stars <- train_data$stars

test_norm <- as.data.frame(normalize_data(
  test_data %>% dplyr::select(-stars), train_data %>% dplyr::select(-stars)))
test_norm$stars <- test_data$stars
```

Feature Selection:
We extract top 20 important variables from baseline and ensamble models, count their occurance and choose those occur more than 3 times. The variables selected are as follows:
"stars","yelp.votes.useful","yelp.votes.funny","yelp.votes.cool","wcount","useful","review_count.y","review_count.x","qmark","funny","emark","cuisine.Vegetarian","cuisine.Mexican","cuisine.Fast.Food","cuisine.Chinese","cuisine.Breakfast","cuisine.AfternoonTea","cool"


```{r message=FALSE, warning=FALSE}
#step AIC
library(MASS)
lm.mod <- lm(stars ~ ., train_norm)
lm.pred <- predict(lm.mod, test_norm)
step <- stepAIC(lm.mod,trace=FALSE)
step$anova
lm.mod_step2 <- lm(stars ~ yelp.votes.useful + yelp.votes.funny + yelp.votes.cool + 
    wcount + review_count.y + review_count.x + qmark + emark + 
    cuisine.Vegetarian + cuisine.Mexican + cuisine.Fast.Food + 
    cuisine.Chinese + cuisine.Breakfast + cuisine.AfternoonTea, test_norm)
lm_step.pred2 <- predict(lm.mod_step2, test_norm)
RMSE(lm_step.pred2, test_norm$stars)
#1.106715

#Elastic Net
set.seed(5103)
# CV
en.mod2 <- cva.glmnet(stars ~ ., train_norm, use.model.frame=TRUE)
# choose optimal alpha manually
minlossplot(en.mod2, cv.type="min")
alpha_opt <- en.mod2$alpha[10] #0.729
lambda_opt <- en.mod2$modlist[[10]]$lambda.min #0.005080836
# prediciton using optimal lambda
en.pred2 <- predict(en.mod2,alpha=alpha_opt, s=lambda_opt, newdata=test_norm,exact=TRUE)
RMSE(en.pred2, test_norm$stars)
#1.116569

#Random Forest
# tune_rf <- expand.grid(.mtry = 10:25)
set.seed(5103)
tune_rf <- expand.grid(.mtry = 3)
rf.mod2 <- train( train_norm[,predictors], train_norm[,outcomeName], method = 'rf', tuneGrid = tune_rf, trControl=fitControl, metric = "RMSE",importance=T )
rf.pred2 <- predict(rf.mod2, test_norm, type = 'raw')
RMSE(rf.pred2, test_data$stars)
# 1.098695


# xgboost
set.seed(5103)
tune_xgb <- expand.grid(.nrounds = c(100), .max_depth = c(4), .eta = c(0.1), .gamma = c(0.1), .colsample_bytree =c(0.6), .min_child_weight = c(3), .subsample = c(0.6))
xgb.mod2 <- train( train_norm[,predictors], train_norm[,outcomeName], method = 'xgbTree', tuneGrid = tune_xgb, trControl=fitControl, metric = "RMSE")
xgb.pred2 <- predict(xgb.mod2, test_norm, type = 'raw')
RMSE(xgb.pred2, test_data$stars)
# 1.090604


# Stacking

library(Metrics)

level1 <- test_norm [1:round(nrow(test_data)/2),]
test <- test_norm[(round(nrow(test_data)/2)+1):nrow(test_data),]

pred.rf <- predict(rf.mod2, level1, type = 'raw')
pred.xgb <- predict(xgb.mod2, level1, type = 'raw')
pred.aic <- predict(lm.mod_step2, level1)
pred.en <- predict(en.mod2,alpha=alpha_opt, s=lambda_opt, newdata=level1,exact=TRUE)
stack_data <- as.data.frame(cbind(pred.rf, pred.xgb, pred.aic, pred.en, train_norm$stars))
names(stack_data) <- c('pred.rf', 'pred.xgb', 'pred.aic', 'stars')

## CV for optimal lambda

lasso.cv_stack <- cv.glmnet(stars~., stack_data, alpha=1, use.model.frame=TRUE)
lasso.lam_stack <- lasso.cv_stack$lambda.min
coef(lasso.cv_stack, s=lasso.lam_stack)

# predict on the test set

pred.rf <- predict(rf.mod2, test, type = 'raw')
pred.xgb <- predict(xgb.mod2, test, type = 'raw')
pred.aic <- predict(lm.mod_step2, test)
pred.en <- predict(en.mod2,alpha=alpha_opt, s=lambda_opt, newdata=test, exact=TRUE)


stack_data <- as.data.frame(cbind(pred.rf, pred.xgb, pred.aic, pred.en, test_norm$stars))
names(stack_data) <- c('pred.rf', 'pred.xgb', 'pred.aic', 'stars')
pred.stack <- predict(lasso.cv_stack, s=lasso.lam_stack, newdata=stack_data, exact=TRUE)
###pred.stack <- predict(xgb_stack, test, type = 'raw')
RMSE(pred.stack, test$stars)
# 1.105083
```



