library("ggplot2")
## population parameters
beta0 <- 1
beta1 <- -2
beta2 <- 6
beta3 <- -1
sigma <- 2
set.seed(7890)
## training data
x <- runif(n=100, min=0, max=5)
f_x <- beta0 + beta1 * x + beta2 * x^2 + beta3 * x^3
epsilon <- rnorm(n=length(x), mean=0, sd=sigma)
y <- f_x + epsilon
## test data
x.test <- runif(n=50, min=0, max=5)
f_x.test <- beta0 + beta1 * x.test + beta2 * x.test^2 + beta3 * x.test^3
epsilon.test <- rnorm(n=length(x.test), mean=0, sd=sigma)
y.test <- f_x.test + epsilon.test
library("mvtnorm")  # package for multivariate normal distribution, INSTALL IT BEFORE RUNNING
z <- rmvnorm(n=length(x), mean=rep(0, 20))  # covariance matrix is identity matrix by default, no need to specify here
z.test <- rmvnorm(n=length(x.test), mean=rep(0, 20))
head(z)
train.x <- cbind(x, z)
test.x <- cbind(x.test, z.test)
head(train.x)
# put your R code here inside the blocks
library("FNN")
model15.train <- knn.reg(train=train.x, test=train.x, y=y, k=15)
model15.train.mse <- mean((y - model15.train$pred)^2)
model15.test <- knn.reg(train=train.x, test=test.x, y=y, k=15)
model15.test.mse <- mean((y.test - model15.test$pred)^2)
# put your R code here inside the blocks
ks <- 1:30
mse.train <- numeric(length=length(ks))
mse.test  <- numeric(length=length(ks))
for (i in seq(along=ks)) {
model.train <- knn.reg(train.x, train.x, y, k=ks[i])
model.test  <- knn.reg(train.x, test.x, y, k=ks[i])
mse.train[i] <- mean((y - model.train$pred)^2)
mse.test[i] <- mean((y.test - model.test$pred)^2)
}
mse.train
mse.test
k.opt <- ks[which.min(mse.test)]
# optimal MSE
mse.opt <- min(mse.test)
# plot MSE on Training and Test
ggplot() + geom_line(aes(x=ks, y=mse.train), color="red") + geom_point(aes(x=ks, y=mse.train)) + geom_line(aes(x=ks, y=mse.test), color="blue") + geom_point(aes(x=ks, y=mse.test)) + scale_x_reverse(lim=c(30, 1)) + geom_hline(yintercept=sigma^2, linetype=2) + theme_bw()
# put your R code here inside the blocks
library("FNN")
model15.train <- knn.reg(train=train.x, test=train.x, y=y, k=15)
model15.train.mse <- mean((y - model15.train$pred)^2)
model15.test <- knn.reg(train=train.x, test=test.x, y=y, k=15)
model15.test.mse <- mean((y.test - model15.test$pred)^2)
# put your R code here inside the blocks
# Construct the optimal K and MSE agaist j
j.mse.train <- vector(mode="numeric",length=0)
j.mse.test <- vector(mode="numeric",length=0)
j <- 1:20
for (n in seq(along=j)){
z.train.x <- cbind(x, z[,1:n])
z.test.x <- cbind(x.test, z.test[,1:n])
ks <- 1:50
z.mse.train <- numeric(length=length(ks))
z.mse.test  <- numeric(length=length(ks))
for (i in seq(along=ks)) {
z.model.train <- knn.reg(z.train.x, z.train.x, y, k=ks[i])
z.model.test  <- knn.reg(z.train.x, z.test.x, y, k=ks[i])
z.mse.train[i] <- mean((y - z.model.train$pred)^2)
z.mse.test[i] <- mean((y.test - z.model.test$pred)^2)}
j.mse.train[n] <- min(z.mse.train)
j.mse.test[n] <- min(z.mse.test)
j.opt <- which.min(z.mse.test)
}
#Plot the optimal K and MSE agaist j
ggplot() + geom_line(aes(x=j, y=j.mse.train), color="red") + geom_point(aes(x=j,
y=j.mse.train)) + geom_line(aes(x=j, y=j.mse.test), color="blue") +
geom_point(aes(x=j, y=j.mse.test)) + scale_x_reverse(lim=c(20, 1)) +
geom_hline(yintercept=sigma^2, linetype=2) + theme_bw()
library("ggplot2")
beta0 <- 1
beta1 <- -2
beta2 <- 6
beta3 <- -1
sigma <- 2
set.seed(7890)
x <- runif(n=100, min=0, max=5)  # 100 points in Uniform(0, 5)
f_x <- beta0 + beta1 * x + beta2 * x^2 + beta3 * x^3
epsilon <- rnorm(n=100, mean=0, sd=sigma)  # 100 noise terms in Normal(0, sigma)
y <- f_x + epsilon
plot.train <- ggplot() + geom_point(aes(x=x, y=y), size=2) + geom_line(aes(x=x, y=f_x)) + theme_bw()
plot.train
x.test <- runif(n=50, min=0, max=5)
f_x.test <- beta0 + beta1 * x.test + beta2 * x.test^2 + beta3 * x.test^3
epsilon.test <- rnorm(n=length(x.test), mean=0, sd=sigma)
y.test <- f_x.test + epsilon.test
plot.test <- ggplot() + geom_point(aes(x=x.test, y=y.test), size=2) + geom_line(aes(x=x.test, y=f_x.test)) + theme_bw()
plot.test
library("FNN")
?knn.reg
train.x <- matrix(x, ncol=1)
test.x <- matrix(x.test, ncol=1)
model15.train <- knn.reg(train=train.x, test=train.x, y=y, k=15)
str(model15.train)
plot.train + geom_line(aes(x=x, y=model15.train$pred), col="blue")
model15.train.mse <- mean((y - model15.train$pred)^2)
model15.test <- knn.reg(train=train.x, test=test.x, y=y, k=15)
str(model15.test)
plot.test + geom_line(aes(x=x.test, y=model15.test$pred), col="blue")
model15.test.mse <- mean((y.test - model15.test$pred)^2)
model1.train <- knn.reg(train=train.x, test=train.x, y=y, k=1)
str(model1.train)
# put your R code here inside the blocks
set.seed(1234)
Population <- rnorm(pop.size,pop.mean,pop.sd)
pop.size <- 10000
pop.mean <- 100
pop.sd <- 15
num.of.samples <- 200
sample.size <- 100
# put your R code here inside the blocks
set.seed(1234)
Population <- rnorm(pop.size,pop.mean,pop.sd)
Sample <- matrix(data=NA,nrow=100,ncol=200)
Mean <- vector(mode="numeric",length=0)
Sd <- vector(mode="numeric",length=0)
for (i in 1:200){
Sample[,i] <- sample(x=Population, size=100, replace=FALSE)
Mean[i] <- mean(Sample[,i])
Sd[i] <- sd(Sample[,i])
}
hist(Mean, probability=TRUE, breaks=30)
hist(Sd, probability=TRUE, breaks=30)
?length
# put your R code here inside the blocks
ks <- 1:50
mse.train <- numeric(length=length(ks))
mse.test  <- numeric(length=length(ks))
for (i in seq(along=ks)) {
model.train <- knn.reg(train.x, train.x, y, k=ks[i])
model.test  <- knn.reg(train.x, test.x, y, k=ks[i])
mse.train[i] <- mean((y - model.train$pred)^2)
mse.test[i] <- mean((y.test - model.test$pred)^2)
}
mse.train
mse.test
k.opt <- ks[which.min(mse.test)]
mse.opt <- min(mse.test)
# plot MSE on Training and Test
ggplot() + geom_line(aes(x=ks, y=mse.train), color="red") + geom_point(aes(x=ks, y=mse.train)) + geom_line(aes(x=ks, y=mse.test), color="blue") + geom_point(aes(x=ks, y=mse.test)) + scale_x_reverse(lim=c(50, 1)) + geom_hline(yintercept=sigma^2, linetype=2) + theme_bw()
?str
# put your R code here inside the blocks
# Construct the optimal K and MSE agaist j
dimen <- 1:20
# construct empty vectors for keeping optimal k and optimal MSE for z with each dimension number
k.optimal<- numeric(length=length(dimen))
mse.optimal<- numeric(length=length(dimen))
for (j in seq(along=dimen)) {
train.x <- cbind(x, z[,c(1:j)])
test.x <- cbind(x.test, z.test[,c(1:j)])
for (i in seq(along=ks)) {
model.train <- knn.reg(train.x, train.x, y, k=ks[i])
model.test  <- knn.reg(train.x, test.x, y, k=ks[i])
mse.train[i] <- mean((y - model.train$pred)^2)
mse.test[i] <- mean((y.test - model.test$pred)^2)
k.optimal[j] <- ks[which.min(mse.test)]
mse.optimal[j] <- min(mse.test)
#Plot the optimal K and MSE agaist j
mseplot<-ggplot() + geom_line(aes(x=dimension, y=mse.optimal),color = "red") + geom_point(aes(x=dimension, y=mse.optimal)) + geom_line(aes(x=dimension, y=k.optimal), color = "blue") + geom_point(aes(x=dimension, y=k.optimal))
print(mseplot +ggtitle("Minimum MSE (red) and optimal k (blue) at different z dimentionality")+labs(y="Minimum MSE or Optimal k", x="Dimension"))
# put your R code here inside the blocks
ks <- 1:30
# construct empty vectors for keeping the MSE for each k
mse.train <- numeric(length=length(ks))
mse.test  <- numeric(length=length(ks))
# loop over all the k and evaluate MSE in each of them
for (i in seq(along=ks)) {
model.train <- knn.reg(train.x, train.x, y, k=ks[i])
model.test  <- knn.reg(train.x, test.x, y, k=ks[i])
mse.train[i] <- mean((y - model.train$pred)^2)
mse.test[i] <- mean((y.test - model.test$pred)^2)
}
mse.train
mse.test
# optimal k
k.opt <- ks[which.min(mse.test)]
# optimal MSE
mse.opt <- min(mse.test)
# plot MSE on Training and Test
ggplot() + geom_line(aes(x=ks, y=mse.train), color="red") + geom_point(aes(x=ks, y=mse.train)) + geom_line(aes(x=ks, y=mse.test), color="blue") + geom_point(aes(x=ks, y=mse.test)) + scale_x_reverse(lim=c(50, 1)) + geom_hline(yintercept=sigma^2, linetype=2) + theme_bw()
# put your R code here inside the blocks
# Construct the optimal K and MSE agaist j
dimen <- 1:20
# construct empty vectors for keeping optimal k and optimal MSE for z with each dimension number
k.optimal<- numeric(length=length(dimen))
mse.optimal<- numeric(length=length(dimen))
for (j in seq(along=dimen)) {
train.x <- cbind(x, z[,c(1:j)])
test.x <- cbind(x.test, z.test[,c(1:j)])
for (i in seq(along=ks)) {
model.train <- knn.reg(train.x, train.x, y, k=ks[i])
model.test  <- knn.reg(train.x, test.x, y, k=ks[i])
mse.train[i] <- mean((y - model.train$pred)^2)
mse.test[i] <- mean((y.test - model.test$pred)^2)
k.optimal[j] <- ks[which.min(mse.test)]
mse.optimal[j] <- min(mse.test)
#Plot the optimal K and MSE agaist j
mseplot<-ggplot() + geom_line(aes(x=dimension, y=mse.optimal),color = "red") + geom_point(aes(x=dimension, y=mse.optimal)) + geom_line(aes(x=dimension, y=k.optimal), color = "blue") + geom_point(aes(x=dimension, y=k.optimal))
print(mseplot +ggtitle("Minimum MSE (red) and optimal k (blue) at different z dimentionality")+labs(y="Minimum MSE or Optimal k", x="Dimension"))
# put your R code here inside the blocks
# Construct the optimal K and MSE agaist j
dimension<-1:20
# construct empty vectors for keeping optimal k and optimal MSE for z with each dimension number
k.optimal<- numeric(length=length(dimension))
mse.optimal<- numeric(length=length(dimension))
#loop over j to get a new training and test x every time
for (j in seq(along=dimension)) {
train.x <- cbind(x, z[,c(1:j)])
test.x <- cbind(x.test, z.test[,c(1:j)])
# loop over all the k and evaluate MSE in each of them
for (i in seq(along=ks)) {
model.train <- knn.reg(train.x, train.x, y, k=ks[i])
model.test  <- knn.reg(train.x, test.x, y, k=ks[i])
mse.train[i] <- mean((y - model.train$pred)^2)
mse.test[i] <- mean((y.test - model.test$pred)^2)
}
# optimal k at each j
k.optimal[j] <- ks[which.min(mse.test)]
# optimal MSE at each j
mse.optimal[j] <- min(mse.test)
}
# put your R code here inside the blocks
# Construct the optimal K and MSE agaist j
dimension<-1:20
# construct empty vectors for keeping optimal k and optimal MSE for z with each dimension number
k.optimal<- numeric(length=length(dimension))
mse.optimal<- numeric(length=length(dimension))
#loop over j to get a new training and test x every time
for (j in seq(along=dimension)) {
train.x <- cbind(x, z[,c(1:j)])
test.x <- cbind(x.test, z.test[,c(1:j)])
# loop over all the k and evaluate MSE in each of them
for (i in seq(along=ks)) {
model.train <- knn.reg(train.x, train.x, y, k=ks[i])
model.test  <- knn.reg(train.x, test.x, y, k=ks[i])
mse.train[i] <- mean((y - model.train$pred)^2)
mse.test[i] <- mean((y.test - model.test$pred)^2)
}
# optimal k at each j
k.optimal[j] <- ks[which.min(mse.test)]
# optimal MSE at each j
mse.optimal[j] <- min(mse.test)
}
mseplot<-ggplot() + geom_line(aes(x=dimension, y=mse.optimal),color = "red") + geom_point(aes(x=dimension, y=mse.optimal)) + geom_line(aes(x=dimension, y=k.optimal), color = "blue") + geom_point(aes(x=dimension, y=k.optimal))
print(mseplot +ggtitle("Minimum MSE (red) and optimal k (blue) at different z dimentionality")+labs(y="Minimum MSE or Optimal k", x="Dimension"))
# put your R code here inside the blocks
# Construct the optimal K and MSE agaist j
dimen<-1:20
# construct empty vectors for keeping optimal k and optimal MSE for z with each dimension number
k.optimal<- numeric(length=length(dimen))
mse.optimal<- numeric(length=length(dimen))
#loop over j to get a new training and test x every time
for (j in seq(along=dimen)) {
train.x <- cbind(x, z[,c(1:j)])
test.x <- cbind(x.test, z.test[,c(1:j)])
# loop over all the k and evaluate MSE in each of them
for (i in seq(along=ks)) {
model.train <- knn.reg(train.x, train.x, y, k=ks[i])
model.test  <- knn.reg(train.x, test.x, y, k=ks[i])
mse.train[i] <- mean((y - model.train$pred)^2)
mse.test[i] <- mean((y.test - model.test$pred)^2)
}
# optimal k at each j
k.optimal[j] <- ks[which.min(mse.test)]
# optimal MSE at each j
mse.optimal[j] <- min(mse.test)
}
mseplot<-ggplot() + geom_line(aes(x=dimen, y=mse.optimal),color = "red") + geom_point(aes(x=dimen, y=mse.optimal)) + geom_line(aes(x=dimen, y=k.optimal), color = "blue") + geom_point(aes(x=dimen, y=k.optimal))
print(mseplot +ggtitle("Minimum MSE (red) and optimal k (blue) at different z dimentionality")+labs(y="Minimum MSE or Optimal k", x="Dimension"))
# put your R code here inside the blocks
# Construct the optimal K and MSE agaist j
dimen<-1:20
# construct empty vectors for keeping optimal k and optimal MSE for z with each dimension number
k.optimal<- numeric(length=length(dimen))
mse.optimal<- numeric(length=length(dimen))
for (j in seq(along=dimen)) {
train.x <- cbind(x, z[,c(1:j)])
test.x <- cbind(x.test, z.test[,c(1:j)])
for (i in seq(along=ks)) {
model.train <- knn.reg(train.x, train.x, y, k=ks[i])
model.test  <- knn.reg(train.x, test.x, y, k=ks[i])
mse.train[i] <- mean((y - model.train$pred)^2)
mse.test[i] <- mean((y.test - model.test$pred)^2)
}
k.optimal[j] <- ks[which.min(mse.test)]
mse.optimal[j] <- min(mse.test)
}
mseplot<-ggplot() + geom_line(aes(x=dimen, y=mse.optimal),color = "red") + geom_point(aes(x=dimen, y=mse.optimal)) + geom_line(aes(x=dimen, y=k.optimal), color = "blue") + geom_point(aes(x=dimen, y=k.optimal))
print(mseplot +ggtitle("Minimum MSE (red) and optimal k (blue) at different z dimentionality")+labs(y="Minimum MSE or Optimal k", x="Dimension"))
# put your R code here inside the blocks
set.seed(1234)
population <- rnorm(n=pop.size, mean=pop.mean, sd=pop.sd)  # type ?rnorm for details
plot(population, main='Normal population in question')
sample.mean <- numeric(length=length(num.of.samples))
sample.var  <- numeric(length=length(num.of.samples))
#sampling
for (i in 1:num.of.samples) {
sampletaken <- sample(x=population, size=sample.size, replace=FALSE)
sample.mean [i]<- mean(sampletaken)
sample.var [i] <- var(sampletaken)
}
#evaluating sampled sample sets
mean.samplemean<-mean(sample.mean)
sd.samplemean<-sd(sample.mean)
mean.samplevar<-mean(sample.var)
sd.samplevar<-sd(sample.var)
hist(sample.mean, main = paste ("Histogram of Sample Mean"), probability=TRUE, breaks=30)  # plot sample distribution
curve(dnorm(x, mean=mean.samplemean, sd=sd.samplemean), add=TRUE, col="blue")
hist(sample.var, main = paste ("Histogram of Sample Variance"),probability=TRUE, breaks=30)  # plot sample distribution
curve(dnorm(x, mean=mean.samplevar, sd=sd.samplevar), add=TRUE, col="blue")
library("ggplot2")
#############################
### Generating data
# Simulate a 3rd-order polynomial data
# y = beta0 + beta1 * x + beta2 * x^2 + beta3 * x^3 + epsilon
# epsilon ~ N(0, sigma^2)
#############################
## population parameters
beta0 <- 1
beta1 <- -2
beta2 <- 6
beta3 <- -1
sigma <- 2
set.seed(7890)
## training data
x <- runif(n=100, min=0, max=5)  # 100 points in Uniform(0, 5)
f_x <- beta0 + beta1 * x + beta2 * x^2 + beta3 * x^3
epsilon <- rnorm(n=100, mean=0, sd=sigma)  # 100 noise terms in Normal(0, sigma)
y <- f_x + epsilon
plot.train <- ggplot() + geom_point(aes(x=x, y=y), size=2) + geom_line(aes(x=x, y=f_x)) + theme_bw()
plot.train
x.test <- runif(n=50, min=0, max=5)
f_x.test <- beta0 + beta1 * x.test + beta2 * x.test^2 + beta3 * x.test^3
epsilon.test <- rnorm(n=length(x.test), mean=0, sd=sigma)
y.test <- f_x.test + epsilon.test
plot.test <- ggplot() + geom_point(aes(x=x.test, y=y.test), size=2) + geom_line(aes(x=x.test, y=f_x.test)) + theme_bw()
plot.test
library("FNN")
?knn.reg
train.x <- matrix(x, ncol=1)
test.x <- matrix(x.test, ncol=1)
model15.train <- knn.reg(train=train.x, test=train.x, y=y, k=15)
str(model15.train)
plot.train + geom_line(aes(x=x, y=model15.train$pred), col="blue")
model15.train.mse <- mean((y - model15.train$pred)^2)
model15.test <- knn.reg(train=train.x, test=test.x, y=y, k=15)
str(model15.test)
plot.test + geom_line(aes(x=x.test, y=model15.test$pred), col="blue")
# Test MSE
model15.test.mse <- mean((y.test - model15.test$pred)^2)
## 2. k=1
model1.train <- knn.reg(train=train.x, test=train.x, y=y, k=1)
str(model1.train)
# plot the fit
plot.train + geom_line(aes(x=x, y=model1.train$pred), col="blue")
# Training MSE
model1.train.mse <- mean((y - model1.train$pred)^2)
## 2b. k=1, test MSE
model1.test <- knn.reg(train=train.x, test=test.x, y=y, k=1)
str(model1.test)
# plot the fit
plot.test + geom_line(aes(x=x.test, y=model1.test$pred), col="blue")
# Test MSE
model1.test.mse <- mean((y.test - model1.test$pred)^2)
## 3. k=50
model50.train <- knn.reg(train=train.x, test=train.x, y=y, k=50)
str(model50.train)
# plot
plot.train + geom_line(aes(x=x, y=model50.train$pred), col="blue")
# Training MSE
model50.train.mse <- mean((y - model50.train$pred)^2)
## 4. Training and Test Error plot: to enumerate many many k values and measure MSE
# k's that will be evaluated
ks <- 1:30
# construct empty vectors for keeping the MSE for each k
mse.train <- numeric(length=length(ks))
mse.test  <- numeric(length=length(ks))
# loop over all the k and evaluate MSE in each of them
for (i in seq(along=ks)) {
model.train <- knn.reg(train.x, train.x, y, k=ks[i])
model.test  <- knn.reg(train.x, test.x, y, k=ks[i])
mse.train[i] <- mean((y - model.train$pred)^2)
mse.test[i] <- mean((y.test - model.test$pred)^2)
}
mse.train
mse.test
# optimal k
k.opt <- ks[which.min(mse.test)]
# optimal MSE
mse.opt <- min(mse.test)
# plot MSE on Training and Test
ggplot() + geom_line(aes(x=ks, y=mse.train), color="red") + geom_point(aes(x=ks, y=mse.train)) + geom_line(aes(x=ks, y=mse.test), color="blue") + geom_point(aes(x=ks, y=mse.test)) + scale_x_reverse(lim=c(30, 1)) + geom_hline(yintercept=sigma^2, linetype=2) + theme_bw()
#############################
### Loading data
#############################
# load the dataset "mixture.example" in package "ElemStatLearn")
library("ElemStatLearn")  # run install.packages("ElemStatLearn") if you haven't
?mixture.example
str(mixture.example)
# copy important ones out
x <- mixture.example$x
y <- mixture.example$y
prob <- mixture.example$prob
xnew <- mixture.example$xnew
px1 <- mixture.example$px1
px2 <- mixture.example$px2
summary(x)
table(y)
summary(prob)
# visualize the "prob" matrix
prob.mat <- matrix(prob, nrow=length(px1), ncol=length(px2))
persp(px1, px2, prob.mat, phi = 45, theta = 45, xlab = "x1", ylab = "x2", main = "True Prob from the Data Generating Model")
df.training <- data.frame(x1=x[ , 1], x2=x[ , 2], y=y)
summary(df.training)
df.training$y <- as.factor(df.training$y)
df.grid <- data.frame(x1=xnew[ , 1], x2=xnew[ , 2])
df.grid$prob <- prob
summary(df.grid)
library("ggplot2")
p0 <- ggplot() + geom_point(data=df.training, aes(x=x1, y=x2, color=y), size=4) + scale_color_manual(values=c("green", "red")) + theme_bw()
p0
p.true <- p0 + stat_contour(data=df.grid, aes(x=x1, y=x2, z=prob), breaks=c(0.5))
p.true
library("FNN")
?knn
model15 <- knn(x, xnew, y, k=15, prob=TRUE)
str(model15)
prob15 <- attr(model15, "prob")
prob15 <- ifelse(model15 == "1", prob15, 1 - prob15)
df.grid$prob15 <- prob15
# plot
p15 <- p.true + stat_contour(data=df.grid, aes(x=x1, y=x2, z=prob15), breaks=c(0.5), color="blue", size=1)
p15
y_hat15 <- ifelse(knn(x, x, y, k=15) == "1", 1, 0)
# total errors in training
sum(y_hat15 != y)
# misclassification rate in training
model15.train.err <- sum(y_hat15 != y) / length(y)
model1 <- knn(x, xnew, y, k=1, prob=TRUE)
str(model1)
prob1 <- attr(model1, "prob")
prob1 <- ifelse(model1 == "1", prob1, 1 - prob1)
df.grid$prob1 <- prob1
# plot
p1 <- p.true + stat_contour(data=df.grid, aes(x=x1, y=x2, z=prob1), breaks=c(0.5), color="blue", size=1)
p1
y_hat1 <- ifelse(knn(x, x, y, k=1) == "1", 1, 0)
# total errors in training
sum(y_hat1 != y)
# misclassification rate in training
model1.train.err <- sum(y_hat1 != y) / length(y)
model100 <- knn(x, xnew, y, k=100, prob=TRUE)
str(model100)
prob100 <- attr(model100, "prob")
prob100 <- ifelse(model100 == "1", prob100, 1 - prob100)
df.grid$prob100 <- prob100
p100 <- p.true + stat_contour(data=df.grid, aes(x=x1, y=x2, z=prob100), breaks=c(0.5), color="blue", size=1)
p100
y_hat100 <- ifelse(knn(x, x, y, k=100) == "1", 1, 0)
# total errors in training
sum(y_hat100 != y)
# misclassification rate in training
model100.train.err <- sum(y_hat100 != y) / length(y)
library("mvtnorm")
set.seed(123)
centers <- c(sample(1:10, 5000, replace=TRUE),
sample(11:20, 5000, replace=TRUE))
means <- mixture.example$means
means <- means[centers, ]
x.test <- rmvnorm(10000, c(0, 0), 0.2 * diag(2))
x.test <- x.test + means
y.test <- c(rep(0, 5000), rep(1, 5000))
bayes.error <- sum(mixture.example$marginal * (prob * I(prob < 0.5) + (1-prob) * I(prob >= 0.5)))
ks <- c(1, 3, 5, 7, 9, 11, 15, 17, 23, 25, 35, 45, 55, 75, 99)
misclass.train <- numeric(length=length(ks))
misclass.test  <- numeric(length=length(ks))
for (i in seq(along=ks)) {
model.train <- knn(x, x, y, k=ks[i])
model.test  <- knn(x, x.test, y, k=ks[i])
misclass.train[i] <- sum(model.train != y) / length(y)
misclass.test[i] <- sum(model.test != y.test) / length(y.test)
}
misclass.train
misclass.test
plot.mse <- ggplot() + geom_line(aes(x=ks, y=misclass.train), color="red") + geom_point(aes(x=ks, y=misclass.train))  + geom_line(aes(x=ks, y=misclass.test), color="blue") + geom_point(aes(x=ks, y=misclass.test))  + scale_x_reverse(lim=c(100, 1))  + theme_bw()
plot.mse
plot.mse + geom_hline(aes(yintercept = bayes.error), linetype="dashed")
